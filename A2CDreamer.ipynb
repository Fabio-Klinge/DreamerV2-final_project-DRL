{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class A2C:\n",
    "    def __init__(self, environment, state_size, action_size, discount_factor = 0.99, actor_learning_rate = 0.001, critic_learning_rate= 0.005, value_size = 1, actor_entropy = 0.001):\n",
    "        self.env = environment\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        self.value_size = value_size\n",
    "        self.actor_entropy = actor_entropy\n",
    "\n",
    "        self.actor = Actor(self.state_size, self.action_size, self.actor_learning_rate)\n",
    "        self.actor_network = self.actor.create_actor_network()\n",
    "        self.critic = Critic(self.state_size, self.value_size, self.critic_learning_rate)\n",
    "        self.critic_network = self.critic.create_critic_network()\n",
    "\n",
    "    def training_loop(self, number_of_episodes):\n",
    "        scores, episodes = [], []\n",
    "        for e in range(number_of_episodes):\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state,[1,self.state_size])\n",
    "            while not done:\n",
    "                # Check if we want to render\n",
    "                #if self.agent.render:\n",
    "                #    env.render()\n",
    "                #get action:\n",
    "                policy = self.actor_network.predict(state,batch_size=1).flatten()\n",
    "                action = np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                next_state = np.reshape(next_state,[1,self.state_size])\n",
    "                # Give immediate penalty for an action that terminates the episode immediately, Since we want to maximize the time\n",
    "                # Note that the max for the cartpole is 499 and it will reset, otherwise we keep the current score if it is not yet done, and if it ended we give a -100 reward\n",
    "                reward = reward if not done or score == 499 else -100\n",
    "                # We now train the model based on the results of our action taken\n",
    "                self.train_model(state,action,reward,next_state,done)\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    score = score if score == 500.0 else score +100\n",
    "                    scores.append(score)\n",
    "                    episodes.append(e)\n",
    "                    plt.plot(episodes,scores)\n",
    "                    \n",
    "                    #if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    #    sys.exit()\n",
    "            if e % 10 ==0: #%50\n",
    "                #agent.actor.save_weights(\"./save_model/cartpole_actor.h5\")\n",
    "                #agent.critic.save_weights(\"./save_model/cartpole_critic.h5\")\n",
    "                \n",
    "                #print(\"episode: {} score: {}\".format(e,score))\n",
    "\n",
    "                plt.show()\n",
    "    \n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        target = np.zeros((1,self.value_size)) # Initialize the policy targets matrix\n",
    "        advantages = np.zeros((1,self.action_size)) # Initialize the advantages matrix\n",
    "\n",
    "        optimizer_actor = tf.keras.optimizers.Adam(learning_rate = self.actor_learning_rate)\n",
    "        optimizer_critic = tf.keras.optimizers.Adam(learning_rate = self.critic_learning_rate)\n",
    "       \n",
    "        with tf.GradientTape() as tape_1, tf.GradientTape() as tape_2:\n",
    "            value = self.critic_network(state)[0] # Get value for this state\n",
    "            next_value = self.critic_network(next_state)[0] # Get value for the next state\n",
    "\n",
    "            logits = self.actor_network(state)\n",
    "            \n",
    "            # update the advantages and value tables if done\n",
    "            advantages[0][action] = reward + (1.0 - done) * self.discount_factor*(next_value) - value # If not yet done, then simply update for the current step.\n",
    "            target[0][0] = reward + (1.0 - done) * self.discount_factor*next_value\n",
    "            \n",
    "\n",
    "            mse = tf.keras.losses.MeanSquaredError()\n",
    "            critic_loss = mse(reward, value)\n",
    "            \n",
    "            action = tf.cast([action], tf.int32)\n",
    "            policy_loss = tf.keras.losses.sparse_categorical_crossentropy(action, logits, from_logits=False) #eigentlich True\n",
    "            \n",
    "            \n",
    "            entropy_loss = tf.keras.losses.categorical_crossentropy(logits, logits, from_logits=False)\n",
    "            actor_loss = policy_loss - self.actor_entropy * entropy_loss\n",
    "\n",
    "        actor_gradient = tape_1.gradient(actor_loss, self.actor_network.trainable_variables)\n",
    "        critic_gradient = tape_2.gradient(critic_loss, self.critic_network.trainable_variables)\n",
    "\n",
    "        #print(len(gradient))\n",
    "        #gradients = gradients[1]\n",
    "        optimizer_critic.apply_gradients(zip(critic_gradient, self.critic_network.trainable_variables))\n",
    "\n",
    "        optimizer_actor.apply_gradients(zip(actor_gradient, self.actor_network.trainable_variables))    \n",
    "        \n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        actor_input = tf.keras.Input(shape=self.state_size) # Define our model\n",
    "        x = Dense(24, activation='relu', kernel_initializer='he_uniform')(actor_input)\n",
    "        actor_output = Dense(self.action_size, activation='softmax', kernel_initializer='he_uniform')(x)\n",
    "        #actor_network.summary()\n",
    "        #actor_network.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = self.learning_rate))\n",
    "        actor_network = tf.keras.Model(actor_input, actor_output, name = \"Actor\")\n",
    "        return actor_network\n",
    "\n",
    "        \n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, state_size, value_size, learning_rate):\n",
    "        self.state_size = state_size\n",
    "        self.value_size = value_size\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def create_critic_network(self):\n",
    "        critic_input = tf.keras.Input(shape=self.state_size) # Define our model\n",
    "        x = Dense(24, activation='relu', kernel_initializer='he_uniform')(critic_input)\n",
    "        critic_output = Dense(self.value_size, activation='linear', kernel_initializer='he_uniform')(x)\n",
    "        #critic.compile(loss = 'mse', optimizer= Adam(lr=self.critic_lr))# Loss is MSE since we want to give out a value and not a probability.        critic_network = tf.keras.Model(critic_input, critic_output)\n",
    "        critic_network = tf.keras.Model(critic_input, critic_output, name = \"Critic\")\n",
    "        #critic_network.summary()\n",
    "        return critic_network\n",
    "\n",
    "\n",
    "\n",
    "environment = gym.make('CartPole-v1')\n",
    "environment.reset()\n",
    "state_size = environment.observation_space.shape[0]\n",
    "# print(environment.action_space) = Discrete(2)\n",
    "# funktioniert nicht, also hardcoden\n",
    "#action_size = environment.action_space.shape[0]\n",
    "action_size = 2\n",
    "a2c = A2C(environment,state_size,action_size)\n",
    "\n",
    "a2c.training_loop(1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6284987c06f01b132727663484f18a91539015bcfcfacd046edd7e007a79e84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
