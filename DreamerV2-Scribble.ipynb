{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Further support\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 192, 48), (1, 192, 48), 0, 0.8037606638879525, False]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import highway_env\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"highway-v0\")\n",
    "state = env.reset()\n",
    "\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (192, 48),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "    \n",
    "env.configure(config)\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "   # print(action)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    #env.render()\n",
    "    a = [state.shape, next_state.shape, action, reward, done]\n",
    "    state = next_state\n",
    "\n",
    "print(a)\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterator trajectories:\n",
      "[(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[5, 5]])>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>), (<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[5, 5]])>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>), (<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[1., 2., 3., 4.]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[5, 5]])>, <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[6.]], dtype=float32)>, <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[ True]])>)]\n",
      "[(TensorShape([1, 4]), TensorShape([1, 4]), TensorShape([1, 2]), TensorShape([1, 1]), TensorShape([1, 1])), (TensorShape([1, 4]), TensorShape([1, 4]), TensorShape([1, 2]), TensorShape([1, 1]), TensorShape([1, 1])), (TensorShape([1, 4]), TensorShape([1, 4]), TensorShape([1, 2]), TensorShape([1, 1]), TensorShape([1, 1]))]\n"
     ]
    }
   ],
   "source": [
    "# Has to save (Observation, action, reward, terminal state)\n",
    "from numpy import float32\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=50,\n",
    "        buffer_length=1000, \n",
    "        observation_size=192*48,\n",
    "        action_size=2\n",
    "    ):\n",
    "        '''\n",
    "        Create replay buffer\n",
    "\n",
    "        Buffer size = batch_size * buffer_length\n",
    "\n",
    "        '''\n",
    "        # Save batch size for other functions of buffer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Tell buffer what data & which size to expect\n",
    "        self.data_spec = (\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, observation_size],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Observation\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, observation_size],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Next state\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, action_size],\n",
    "                dtype=tf.dtypes.int32,\n",
    "                name=\"Action\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                # Reward size\n",
    "                shape=[1, 1],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Reward\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, 1],\n",
    "                # Either 0 or 1 \n",
    "                dtype=tf.dtypes.bool,\n",
    "                name=\"Terminal State\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the buffer \n",
    "        self.buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            self.data_spec, batch_size, buffer_length\n",
    "        )\n",
    "\n",
    "    def stupid(self):\n",
    "        return self.data_spec\n",
    "\n",
    "    def add(self, items):\n",
    "        '''\n",
    "        length of items must be equal to batch size\n",
    "\n",
    "        items: list or tuple of batched data from (50, 4)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # Combine all values from \"items\" in tensor\n",
    "        # Not sure wether we need tf.nest.map_structure\n",
    "        self.batched_values = tf.nest.map_structure(\n",
    "            lambda t: tf.stack([t] * self.batch_size),\n",
    "            items\n",
    "        )\n",
    "\n",
    "        # Add to batch\n",
    "        self.buffer.add_batch(self.batched_values)\n",
    "\n",
    "    def sample(self):\n",
    "        return self.buffer.as_dataset(single_deterministic_pass=False)\n",
    "\n",
    "\n",
    "buffer = Buffer(batch_size=1, observation_size=4)\n",
    "# Save sizes of the stupid tensors\n",
    "data_spec = buffer.stupid()\n",
    "\n",
    "for t in range(4):\n",
    "    buffer.add((\n",
    "        tf.constant([1.0,2.0,3.0,4.0], shape=data_spec[0].shape.as_list()),\n",
    "        tf.constant([1.0,2.0,3.0,4.0], shape=data_spec[1].shape.as_list()),\n",
    "        tf.constant([5], shape=data_spec[2].shape.as_list()),\n",
    "        tf.constant([6.0], shape=data_spec[3].shape.as_list()),\n",
    "        tf.constant([True], shape=data_spec[4].shape.as_list())\n",
    "    ))\n",
    "\n",
    "\n",
    "dataset = buffer.sample()\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "  t, _ = next(iterator)\n",
    "  trajectories.append(t)\n",
    "\n",
    "print(trajectories)\n",
    "\n",
    "print(tf.nest.map_structure(lambda t: t.shape, trajectories))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World model & agent training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam inits\n",
    "Agent Data collection in environment + adding data to ERB (+ measure at which reward loop stops?) \\par\n",
    "World model loop on data sampled from ERB \\par\n",
    "Agent training loop with world model feedback\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7b373b46a3ad398ff0fbe06950ea2901f8c9a237be6643544c903d3b2478057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
