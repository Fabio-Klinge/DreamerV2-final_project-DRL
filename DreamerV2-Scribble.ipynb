{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\User1/.netrc\n",
      "c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\IPython\\html.py:12: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  warn(\"The `IPython.html` package has been deprecated since IPython 4.0. \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User1\\Documents\\GitHub\\DreamerV2-final_project-DRL\\wandb\\run-20220810_142250-28lfzvie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cogsci/DreamerV2-final_project-DRL/runs/28lfzvie\" target=\"_blank\">quiet-serenity-26</a></strong> to <a href=\"https://wandb.ai/cogsci/DreamerV2-final_project-DRL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install tensorflow tensorflow_probability tf_agents numpy gym highway-env tqdm wandb\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "# Layer\n",
    "from tensorflow.keras.layers import Dense, Layer, Conv2DTranspose, Conv2D, GlobalAveragePooling2D, Reshape, BatchNormalization, GRUCell, MaxPooling2D, Flatten, RNN\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, KLDivergence\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "\n",
    "# Buffer \n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Further support\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "wandb.init(settings=wandb.Settings(_disable_stats=True))\n",
    "\n",
    "# Environment\n",
    "import gym\n",
    "import highway_env\n",
    "import random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Has to save (Observation, action, reward, terminal state)\n",
    "from numpy import float32\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=1,\n",
    "        buffer_length=1000, \n",
    "        observation_size=(128,32,1),\n",
    "        action_size=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create replay buffer\n",
    "\n",
    "        Buffer size = batch_size * buffer_length\n",
    "\n",
    "        \"\"\"\n",
    "        # Save batch size for other functions of buffer\n",
    "        # NOT the usual batch size in Deep Learning\n",
    "        # Batches in Uniform Replay Buffer describe size of input added to the buffer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Tell buffer what data & which size to expect\n",
    "        self.data_spec = (\n",
    "            tf.TensorSpec(\n",
    "                shape= observation_size,\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Observation\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=observation_size,\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Next state\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[action_size],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Action\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                # Reward size\n",
    "                shape=[1, ],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Reward\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, ],\n",
    "                # Either 0 or 1 \n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Non-Terminal State\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the buffer \n",
    "        self.buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            self.data_spec, batch_size, buffer_length\n",
    "        )\n",
    "\n",
    "    def obtain_buffer_specs(self):\n",
    "        return self.data_spec\n",
    "\n",
    "    def add(self, items):\n",
    "        \"\"\"\n",
    "        length of items must be equal to batch size\n",
    "\n",
    "        items: list or tuple of batched data from (50, 5)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Combine all values from \"items\" in tensor\n",
    "        # Not sure wether we need tf.nest.map_structure\n",
    "        batched_values = tf.nest.map_structure(\n",
    "            lambda t: tf.stack([t] * self.batch_size),\n",
    "            items\n",
    "        )\n",
    "        \n",
    "        # Add to batch\n",
    "        self.buffer.add_batch(batched_values)\n",
    "\n",
    "    def sample(self, batch_size, prefetch_size):\n",
    "        data = self.buffer.as_dataset(single_deterministic_pass=True)\n",
    "\n",
    "                \n",
    "        # data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        # # normalize inputs from 0/255 to -1/1\n",
    "        # data = data.map(lambda img, target: ((img/128.)-1, target))\n",
    "        # # create one-hot vector for targets\n",
    "        # data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "        # normalize inputs from 0/255 to -1/1\n",
    "        data = data.map(lambda buffer_content, _: (((buffer_content[0]/128.)-1, (buffer_content[1]/128.)-1, buffer_content[2], buffer_content[3], buffer_content[4]), _))\n",
    "        data = data.cache()\n",
    "        data = data.batch(batch_size).prefetch(prefetch_size)\n",
    "        #later we want these to be sequences (Do we though)\n",
    "        return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EnvironmentInteractor:\n",
    "\n",
    "  def __init__(self, config, buffer, environment_name = \"highway-fast-v0\"):\n",
    "    self.config = config\n",
    "\n",
    "    self.env = gym.make(environment_name)    \n",
    "    self.env.configure(config)\n",
    "\n",
    "    self.buffer = buffer\n",
    "    # Save sizes of the stupid tensors\n",
    "    self.data_spec = self.buffer.obtain_buffer_specs()\n",
    "  \n",
    "\n",
    "\n",
    "  def create_trajectories(self, iterations):\n",
    "    state = self.env.reset()\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        action = self.env.action_space.sample()\n",
    "\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        self.buffer.add((\n",
    "          tf.cast(tf.constant(state, shape=self.data_spec[0].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(next_state, shape=self.data_spec[1].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(action, shape=self.data_spec[2].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(reward, shape=self.data_spec[3].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(1-done, shape=self.data_spec[4].shape.as_list()), tf.float32)\n",
    "        ))\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "          state = self.env.reset()\n",
    "\n",
    "\n",
    "  def __del__(self):\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image size\n",
    "image_shape = (128,32, 1)\n",
    "\n",
    "# Long term memory of GRU\n",
    "hidden_unit_size = 200\n",
    "\n",
    "# Z in paper\n",
    "stochastic_state_shape = (32,32)\n",
    "stochastic_state_size = stochastic_state_shape[0] * stochastic_state_shape[1]\n",
    "\n",
    "#\n",
    "action_size = 1\n",
    "horizon = 15\n",
    "discount_factor = 0.995\n",
    "\n",
    "#\n",
    "mlp_hidden_layer_size = 100\n",
    "batch_size = 50\n",
    "\n",
    "# TODO different variable names for network inp/outp sizes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RSSMState(NamedTuple):\n",
    "    logits: tf.Tensor = tf.zeros(shape=(stochastic_state_size,))\n",
    "    stochastic_state_z: tf.Tensor = tf.zeros(shape=(stochastic_state_size,))\n",
    "    hidden_rnn_state: tf.Tensor = tf.zeros(shape=(hidden_unit_size,))\n",
    "\n",
    "    @classmethod\n",
    "    def from_list(cls, rssm_states):\n",
    "        logits = tf.stack([rssm_state.logits for rssm_state in rssm_states])\n",
    "        stochastic_state_z = tf.stack([rssm_state.stochastic_state_z for rssm_state in rssm_states])\n",
    "        hidden_rnn_state = tf.stack([rssm_state.hidden_rnn_state for rssm_state in rssm_states])\n",
    "\n",
    "        return cls(logits, stochastic_state_z, hidden_rnn_state)\n",
    "\n",
    "    def get_hidden_state_h_and_stochastic_state_z(self):\n",
    "        hidden_state_h_and_stochastic_state_z = tf.concat([self.stochastic_state_z, self.hidden_rnn_state], axis=-1)\n",
    "\n",
    "        return hidden_state_h_and_stochastic_state_z\n",
    "\n",
    "    @classmethod\n",
    "    def convert_sequences_to_batches(cls, rssm_state):\n",
    "        logits = cls.convert_sequence_to_batch(rssm_state.logits)\n",
    "        stochastic_state_z = cls.convert_sequence_to_batch(rssm_state.stochastic_state_z)\n",
    "        hidden_rnn_state = cls.convert_sequence_to_batch(rssm_state.hidden_rnn_state)\n",
    "\n",
    "        return cls(logits, stochastic_state_z, hidden_rnn_state)\n",
    "\n",
    "    @classmethod\n",
    "    def convert_sequence_to_batch(cls, sequence):\n",
    "        batch = tf.reshape(sequence, (sequence.shape[0] * sequence.shape[1], *sequence.shape[2:]))\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WorldModel:\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = self.create_encoder()\n",
    "        self.decoder = self.create_decoder()\n",
    "        self.reward_model = self.create_reward_predictor()\n",
    "        self.discount_model = self.create_discount_predictor()\n",
    "        self.actor = self.create_actor()\n",
    "        self.critic = self.create_critic()\n",
    "        self.target_critic = tf.keras.models.clone_model(self.critic)\n",
    "\n",
    "        self.rssm = RSSM()\n",
    "\n",
    "        self.models = (self.encoder,\n",
    "        self.decoder,\n",
    "        self.reward_model,\n",
    "        self.discount_model)\n",
    "\n",
    "\n",
    "    def create_encoder(self, input_size=image_shape, output_size=hidden_unit_size):\n",
    "        # Third dimension might be obsolete\n",
    "        encoder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Conv2D(16, (3, 3), activation=\"elu\", padding=\"same\")(encoder_input) # 16 layers of filtered 192x48 features\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 96x24\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(64, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 48x12\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 48x12\n",
    "        x = GlobalAveragePooling2D()(x) # 64\n",
    "        encoder_output = Dense(output_size, activation = \"elu\")(x)\n",
    "\n",
    "        encoder = tf.keras.Model(encoder_input, encoder_output, name=\"Encoder\")\n",
    "\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    # Input size = 1024(z:32x32) + 200(size of hidden state)\n",
    "    # Output size = game frame\n",
    "    def create_decoder(\n",
    "        self, \n",
    "        input_size=stochastic_state_size + hidden_unit_size, \n",
    "        output_size=image_shape\n",
    "    ):\n",
    "        # Third dimension might be obsolete\n",
    "        decoder_input = tf.keras.Input(shape=input_size)\n",
    "        # TODO WIE SCHLIMM IST EIN MLP HIER?\n",
    "        x = Dense(256, activation= \"elu\")(decoder_input)\n",
    "        x = Reshape((32, 8, 1))(x) \n",
    "        # TODO Check whether correct reshape happens\n",
    "        #tf.debugging.assert_equal(x)\n",
    "        x = Conv2DTranspose(16, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"linear\", padding=\"same\")(x)\n",
    "       # x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = Flatten()(x)\n",
    "        # Might needs shape as Tensor  #event_shape=output_size\n",
    "\n",
    "        # decoder_output = tfp.layers.IndependentNormal(event_shape=output_size)(x)\n",
    "\n",
    "\n",
    "        decoder = tf.keras.Model(\n",
    "            decoder_input,\n",
    "            x,\n",
    "            name=\"Decoder\"\n",
    "        )\n",
    "\n",
    "        return decoder\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_reward_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        reward_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(reward_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(output_size)(x)\n",
    "        # Creates indipendent normal distribution\n",
    "        # Hope is that it learns to output variables over reward space [0,1]\n",
    "        #reward_predictor_output = tfp.layers.IndependentNormal()(x)\n",
    "\n",
    "        reward_predictor = tf.keras.Model(\n",
    "            reward_predictor_input,\n",
    "            x,\n",
    "            name=\"create_reward_predictor\"\n",
    "        )\n",
    "\n",
    "        return reward_predictor\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_discount_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        discount_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(discount_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(output_size, activation=\"elu\")(x)\n",
    "        # Create 1 output sampled from bernoulli distribution\n",
    "        #discount_predictor_output = tfp.layers.IndependentBernoulli()(x)\n",
    "\n",
    "        discount_predictor = tf.keras.Model(\n",
    "            discount_predictor_input,\n",
    "            x,\n",
    "            name=\"discount_predictor\"\n",
    "        )\n",
    "\n",
    "        return discount_predictor\n",
    "\n",
    "    def create_actor(\n",
    "        self,\n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=action_size\n",
    "    ):\n",
    "        actor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(actor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(output_size, activation=\"linear\")(x)\n",
    "\n",
    "        actor = tf.keras.Model(\n",
    "            actor_input,\n",
    "            x,\n",
    "            name=\"Actor\"\n",
    "        )\n",
    "\n",
    "        return actor\n",
    "\n",
    "    def create_critic(\n",
    "        self,\n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        critic_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(critic_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(output_size, activation=\"linear\")(x)\n",
    "\n",
    "        actor = tf.keras.Model(\n",
    "            critic_input,\n",
    "            x,\n",
    "            name=\"Critic\"\n",
    "        )\n",
    "\n",
    "        return actor\n",
    "\n",
    "    def compute_actor_critic_loss(self, posterior_rssm_state: RSSMState):\n",
    "\n",
    "        # TODO At the moment we are using only batches and not batches of sequences\n",
    "        batched_posterior_rssm_states = tf.stop_gradient(posterior_rssm_state)\n",
    "\n",
    "        dreamed_rssm_states, dreamed_log_probabilities, dreamed_policy_entropies = self.rssm(horizon, self.actor, batched_posterior_rssm_states)\n",
    "\n",
    "        dreamed_hidden_state_h_and_stochastic_state_z = dreamed_rssm_states.get_hidden_state_h_and_stochastic_state_z()\n",
    "\n",
    "        self.set_trainable_models(self.models + self.rssm.models + self.critic + self.target_critic, False)\n",
    "        ########################################\n",
    "        reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "        reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "        dreamed_reward = reward_distribution.mean()\n",
    "\n",
    "        discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "        discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "        dreamed_discount = discount_factor * discount_distribution * tf.round(discount_distribution.prob(discount_distribution.mean()))\n",
    "\n",
    "        target_value_logits = self.target_critic(hidden_state_h_and_stochastic_state_z)\n",
    "        target_value_distribution = tfp.distributions.Independent(tfp.distributions.Normal(target_value_logits, 1))\n",
    "        dreamed_value = target_value_distribution.mean()\n",
    "        ########################################\n",
    "        self.set_trainable_models(models, True)\n",
    "\n",
    "\n",
    "        actor_loss, discount, lambda_returns = self.actor_loss(dreamed_reward, dreamed_value, dreamed_discount, dreamed_log_probabilities, dreamed_policy_entropies)\n",
    "        critic_loss = self.critic_loss(dreamed_hidden_state_h_and_stochastic_state_z, discount, lambda_returns)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "    def actor_loss(self, dreamed_reward, dreamed_value, dreamed_discount, dreamed_log_probabilities, dreamed_policy_entropies, actor_entropy_scale=0.001, lmbda = 0.95):\n",
    "        lambda_returns = self.compute_return(dreamed_reward[:-1], dreamed_value[:-1], dreamed_discount[:-1], bootstrap=dreamed_value[-1], lmbda=lmbda)\n",
    "\n",
    "        advantage = tf.stop_gradient(lambda_returns - dreamed_value[:-1])\n",
    "        objective = dreamed_log_probabilities[1:] * advantage\n",
    "\n",
    "        discounts = tf.concat([tf.ones_like(dreamed_discount[:1]), dreamed_discount[1:]])\n",
    "        discount = tf.math.cumprod(discounts[:-1], 0)\n",
    "        policy_entropy = dreamed_policy_entropies[1:]\n",
    "        actor_loss = -tf.math.reduce_sum(tf.math.reduce_mean(discount * (objective + actor_entropy_scale * policy_entropy), dim=1))\n",
    "        return actor_loss, discount, lambda_returns\n",
    "\n",
    "\n",
    "    def critic_loss(self, dreamed_hidden_state_h_and_stochastic_state_z, discount, lambda_returns):\n",
    "        # TODO dreamed_hidden_state_h_and_stochastic_state_z[:-1]\n",
    "        critic_logits = self.critic(tf.stop_gradient(dreamed_hidden_state_h_and_stochastic_state_z[:-1]))\n",
    "        critic_distribution = tfp.distributions.Independent(tfp.distributions.Normal(critic_logits, 1))\n",
    "        critic_loss = -tf.reduce_mean(tf.stop_gradient(discount) * critic_distribution.log_prob(tf.stop_gradient(lambda_returns)))\n",
    "\n",
    "        return critic_loss\n",
    "\n",
    "    def compute_return(self, reward,\n",
    "                    value,\n",
    "                    discount,\n",
    "                    bootstrap,\n",
    "                    lmbda):\n",
    "\n",
    "        next_values = tf.concat([value[1:], bootstrap[None]], 0)\n",
    "        target = reward + discount + next_values * (1 - lmbda)\n",
    "        timesteps = list(range(reward.shape[0] - 1, -1, -1))\n",
    "        outputs = []\n",
    "        accumulated_reward = bootstrap\n",
    "        for timestep in timesteps:\n",
    "            inp = target[timestep]\n",
    "            discount_factor = discount[timestep]\n",
    "            accumulated_reward = inp + discount_factor * lmbda * accumulated_reward\n",
    "            outputs.append(accumulated_reward)\n",
    "        returns = tf.reverse(tf.stack(outputs), [0])\n",
    "        return returns\n",
    "\n",
    "\n",
    "\n",
    "    def set_trainable_models(self, models, trainable: bool):\n",
    "        for model in models:\n",
    "            model.trainable = trainable\n",
    "\n",
    "    def compute_log_loss(self, distribution, target):\n",
    "        \"\"\"\n",
    "        Computes loss for:\n",
    "        - Image log loss(Output decoder, frame timestep t)\n",
    "        - Reward log loss(Output reward network, obtained reward timestep t)\n",
    "        - Discount log loss(Output of discount network, terminal state timestep t)\n",
    "        \"\"\"\n",
    "        # TODO check whether distribution.log_prob  (target) matches target size\n",
    "        # histogram von wahrsch. distribution /\n",
    "        return -tf.math.reduce_mean(distribution.log_prob(target))\n",
    "\n",
    "\n",
    "    def compute_kl_loss(self, prior_rssm_states, posterior_rssm_states, alpha=0.8):\n",
    "        \"\"\"\n",
    "        alpha: weigh between training the prior toward the representations & regularizing\n",
    "         the representations towards the prior\n",
    "        prior: Z\n",
    "        posterior: Z^\n",
    "        \"\"\"\n",
    "        prior_distribution = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=prior_rssm_states.logits), 1)\n",
    "        posterior_distribution = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=posterior_rssm_states.logits), 1)\n",
    "\n",
    "        prior_distribution_detached = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=tf.stop_gradient(prior_rssm_states.logits)), 1)\n",
    "        posterior_distribution_detached = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=tf.stop_gradient(posterior_rssm_states.logits)), 1)\n",
    "\n",
    "        # Loss with KL Balancing\n",
    "        # TODO check reihenfolge, reduce_mean hat Gradients?!!?\n",
    "        return alpha * tf.math.reduce_mean(tfp.distributions.kl_divergence(posterior_distribution_detached, prior_distribution)) + (1-alpha) * tf.math.reduce_mean(tfp.distributions.kl_divergence(posterior_distribution, prior_distribution_detached))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RSSM:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_action_embedder = self.create_stochastic_state_action_embedder()\n",
    "        self.rnn = self.create_rnn()\n",
    "        self.prior_model = self.create_prior_stochastic_state_embedder()\n",
    "        self.posterior_model = self.create_posterior_stochastic_state_embedder()\n",
    "\n",
    "        self.models = (self.state_action_embedder,\n",
    "        self.rnn,\n",
    "        self.prior_model,\n",
    "        self.posterior_model)\n",
    "\n",
    "\n",
    "    def create_stochastic_state_action_embedder(\n",
    "        self,\n",
    "        input_size=(stochastic_state_size + action_size,),\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        state_action_input = tf.keras.Input(shape=input_size)\n",
    "        state_action_output = Dense(output_size, activation = \"elu\")(state_action_input)\n",
    "\n",
    "        stochastic_state_action_embedder = tf.keras.Model(\n",
    "            state_action_input,\n",
    "            state_action_output,\n",
    "            name=\"stochastic_state_action_embedder\"\n",
    "        )\n",
    "\n",
    "        return stochastic_state_action_embedder\n",
    "\n",
    "    # Contains GRU cell\n",
    "    def create_rnn(\n",
    "        self,\n",
    "        input_size=(hidden_unit_size, ),\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        return RNN(GRUCell(output_size))\n",
    "\n",
    "        rnn_input = tf.keras.Input(shape=input_size)\n",
    "       # rnn_hidden_state_placeholder = tf.keras.Input(shape=(hidden_unit_size,))\n",
    "        rnn_output = rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(output_size))(rnn_input)\n",
    "\n",
    "\n",
    "        rnn = tf.keras.Model(\n",
    "            rnn_input,\n",
    "            rnn_output,\n",
    "            name=\"rnn\"\n",
    "        )\n",
    "\n",
    "        return rnn\n",
    "\n",
    "    # Z^ in paper\n",
    "    def create_prior_stochastic_state_embedder(\n",
    "        self,\n",
    "        input_size=hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(x)\n",
    "\n",
    "        create_prior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_prior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_prior_stochastic_state_embedder\n",
    "\n",
    "    # Z in paper\n",
    "    # Input size = concatenated output of RNN with output of CNN\n",
    "    def create_posterior_stochastic_state_embedder(\n",
    "        self,\n",
    "        input_size=hidden_unit_size+hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(x)\n",
    "\n",
    "        create_posterior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_posterior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_posterior_stochastic_state_embedder\n",
    "\n",
    "    def sample_stochastic_state(self, logits):\n",
    "        \"\"\"\n",
    "        Gets probabilities for each element of class in each category.\n",
    "        Used to generate embeddings from logits.\n",
    "        \"\"\"\n",
    "\n",
    "        # Logit Outputs from MLP\n",
    "        logits = tf.reshape(logits, shape=(-1, *stochastic_state_shape))\n",
    "        # OneHot distribution over logits\n",
    "        logits_distribution = tfp.distributions.OneHotCategorical(logits)\n",
    "        # Sample from OneHot distribution\n",
    "        sample = tf.cast(logits_distribution.sample(), tf.float32)\n",
    "        # TODO observe logits_distribution.prob(sample) after few iterations\n",
    "        sample += logits_distribution.prob(sample) - tf.stop_gradient(logits_distribution.prob(sample))\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def dream(self, previous_rssm_state: RSSMState, previous_action: tf.Tensor, non_terminal=True):\n",
    "        \"\"\"\n",
    "        Creates Z^\n",
    "        \"\"\"\n",
    "        # TODO invert terminal states (terminal state = 1 if episode ended, needs to be 0)\n",
    "        # Embedding of concatenation prior z and action (t-1)\n",
    "        state_action_embedding = self.state_action_embedder(tf.concat([previous_rssm_state.stochastic_state_z * non_terminal, previous_action], axis=1))\n",
    "        # TODO Remove Squeeze\n",
    "        # Create h from GRU with old h (t-1) and the embedding\n",
    "        state_action_embedding = tf.reshape(state_action_embedding, shape=(-1, 200, 1))\n",
    "\n",
    "        hidden_rnn_state = self.rnn(state_action_embedding, previous_rssm_state.hidden_rnn_state * non_terminal)\n",
    "\n",
    "        # Logits created from h (with MLP) to create Z^\n",
    "        prior_logits = self.prior_model(hidden_rnn_state)\n",
    "        # Create Z^\n",
    "        prior_stochastic_state_z = self.sample_stochastic_state(prior_logits)\n",
    "        # Save logits for Z^, Z^ and h\n",
    "        prior_rssm_state = RSSMState(prior_logits, tf.reshape(prior_stochastic_state_z, (-1, stochastic_state_size)), hidden_rnn_state)\n",
    "\n",
    "        return prior_rssm_state\n",
    "\n",
    "    def dreaming_rollout(self, horizon: int, actor: tf.keras.Model, previous_rssm_state: RSSMState):\n",
    "        \"\"\"\n",
    "        Rollout only Z\n",
    "        \"\"\"\n",
    "        rssm_state = previous_rssm_state\n",
    "\n",
    "        next_rssm_states = []\n",
    "        action_entropies = []\n",
    "        image_log_probabilities = []\n",
    "\n",
    "        for timestep in range(horizon):\n",
    "            action_logits = actor(tf.stop_gradient(rssm_state.get_hidden_state_h_and_stochastic_state_z()))\n",
    "            action_distribution = tfp.distributions.Independent(tfp.distributions.Normal(action_logits, 1))\n",
    "            action = action_distribution.sample()\n",
    "\n",
    "            rssm_state = self.dream(rssm_state, action)\n",
    "\n",
    "            next_rssm_states.append(rssm_state)\n",
    "            action_entropies.append(action_distribution.entropy(action))\n",
    "            image_log_probabilities(action_distribution.log_prob(action))\n",
    "\n",
    "        next_rssm_states = RSSMState.from_list(next_rssm_states)\n",
    "        action_entropies = tf.stack(action_entropies, dim=0)\n",
    "        image_log_probabilities = tf.stack(image_log_probabilities, dim=0)\n",
    "\n",
    "        return next_rssm_states, image_log_probabilities, action_entropies\n",
    "\n",
    "\n",
    "    def observe(self, encoded_state: tf.Tensor, previous_action: tf.Tensor, previous_non_terminal: tf.Tensor, previous_rssm_state: RSSMState):\n",
    "        \"\"\"\n",
    "        Creates Z' and Z\n",
    "        \"\"\"\n",
    "        # Obtain Z^\n",
    "        prior_rssm_state = self.dream(previous_rssm_state, previous_action, previous_non_terminal)\n",
    "\n",
    "        # concatenates h and the output of our CNN (encoded input frame X)\n",
    "        encoded_state_and_hidden_state = tf.concat([prior_rssm_state.hidden_rnn_state, encoded_state], axis=1)\n",
    "\n",
    "        # Logits created from concat of h and encoded frame X (with MLP) to create Z\n",
    "        posterior_logits = self.posterior_model(encoded_state_and_hidden_state)\n",
    "        # Create Z\n",
    "        posterior_stochastic_state_z = self.sample_stochastic_state(posterior_logits)\n",
    "        # Saves logits for Z, Z, and h\n",
    "        posterior_rssm_state = RSSMState(posterior_logits, tf.reshape(posterior_stochastic_state_z, (-1, stochastic_state_size)), prior_rssm_state.hidden_rnn_state)\n",
    "\n",
    "        return prior_rssm_state, posterior_rssm_state\n",
    "\n",
    "    def observing_rollout(self, encoded_states: tf.Tensor, actions: tf.Tensor, non_terminals: tf.Tensor, previous_rssm_state: RSSMState):\n",
    "        prior_rssm_states = []\n",
    "        posterior_rssm_states = []\n",
    "\n",
    "        for encoded_state, action, non_terminal in zip(encoded_states, actions, non_terminals):\n",
    "            # TODO remove islandsolution\n",
    "            encoded_state = tf.expand_dims(encoded_state, axis=0)\n",
    "            action = tf.expand_dims(action, axis=0)\n",
    "            non_terminal = tf.expand_dims(non_terminal, axis=0)\n",
    "            #?? 0 if terminal state is reached\n",
    "            previous_action = action * non_terminal\n",
    "            # Z^, Z\n",
    "            prior_rssm_state, posterior_rssm_state = self.observe(encoded_state, previous_action, non_terminal, previous_rssm_state)\n",
    "\n",
    "            # Save Z^, Z\n",
    "            prior_rssm_states.append(prior_rssm_state)\n",
    "            posterior_rssm_states.append(posterior_rssm_state)\n",
    "\n",
    "            # Z for next iteration\n",
    "            previous_rssm_state = posterior_rssm_state\n",
    "        prior_rssm_states = RSSMState.from_list(prior_rssm_states)\n",
    "        posterior_rssm_states = RSSMState.from_list(posterior_rssm_states)\n",
    "\n",
    "        return prior_rssm_states, posterior_rssm_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\highway_env\\vehicle\\objects.py:33: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.position = np.array(position, dtype=np.float)\n",
      "c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\highway_env\\vehicle\\controller.py:273: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.int(np.clip(np.round(x * (self.target_speeds.size - 1)), 0, self.target_speeds.size - 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling _Independent.__init__ (from tensorflow_probability.python.distributions.independent) with reinterpreted_batch_ndims=None is deprecated and will be removed after 2022-03-01.\n",
      "Instructions for updating:\n",
      "Please pass an integer value for `reinterpreted_batch_ndims`. The current behavior corresponds to `reinterpreted_batch_ndims=tf.size(distribution.batch_shape_tensor()) - 1`.\n",
      "Image Log Loss: 4099.2373046875 Reward Log Loss: 1.2610377073287964 Discount Log Loss 0.7119938731193542 KL Loss 0.0002651293470989913\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Shapes of all inputs must match: values[0].shape = [50,1,1024] != values[2].shape = [50,1,200] [Op:Pack] name: packed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15104/1173264934.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mworld_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_actor_critic_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposterior_rssm_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworld_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mworld_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15104/1477946154.py\u001b[0m in \u001b[0;36mcompute_actor_critic_loss\u001b[1;34m(self, posterior_rssm_state)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;31m# TODO At the moment we are using only batches and not batches of sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[0mbatched_posterior_rssm_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposterior_rssm_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mdreamed_rssm_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdreamed_log_probabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdreamed_policy_entropies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrssm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched_posterior_rssm_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7162\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7164\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Shapes of all inputs must match: values[0].shape = [50,1,1024] != values[2].shape = [50,1,200] [Op:Pack] name: packed"
     ]
    }
   ],
   "source": [
    "# TODO move hyperparams\n",
    "epochs = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002)\n",
    "\n",
    "optimizer_actor = tf.keras.optimizers.Adam(4e-5)\n",
    "optimizer_critic = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "buffer = Buffer(batch_size=1)\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 32),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "\n",
    "environment_interactor = EnvironmentInteractor(config, buffer)\n",
    "\n",
    "world_model = WorldModel()\n",
    "rssm = RSSM()\n",
    "\n",
    "models = (\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "wandb.tensorflow.log(tf.summary)\n",
    "\n",
    "for episode in range(100):\n",
    "    environment_interactor.create_trajectories(1000)\n",
    "    data = buffer.sample(batch_size=50, prefetch_size=70)\n",
    "    # Sample from buffer\n",
    "    for sequence in data:\n",
    "        state, next_state, action, reward, non_terminal = sequence[0]\n",
    "\n",
    "            # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        combined_trainable_variables = models[0].trainable_variables\n",
    "        for i in range(len(models)):\n",
    "            if i+1 >= len(models):\n",
    "                break\n",
    "            combined_trainable_variables += models[i+1].trainable_variables\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoded_state = world_model.encoder(state)\n",
    "            initial_rssm_state = RSSMState()\n",
    "            prior_rssm_states, posterior_rssm_states = rssm.observing_rollout(encoded_state, action, non_terminal, initial_rssm_state)\n",
    "            hidden_state_h_and_stochastic_state_z = tf.concat([posterior_rssm_states.stochastic_state_z, posterior_rssm_states.hidden_rnn_state], axis=-1)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            hidden_state_h_and_stochastic_state_z = tf.reshape(hidden_state_h_and_stochastic_state_z, (-1,stochastic_state_size + hidden_unit_size))\n",
    "\n",
    "            decoder_logits = world_model.decoder(hidden_state_h_and_stochastic_state_z)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            decoder_logits = tf.reshape(decoder_logits, (-1, image_shape[0], image_shape[1], image_shape[2]))\n",
    "\n",
    "            decoder_distribution = tfp.distributions.Independent(tfp.distributions.Normal(decoder_logits, 1))\n",
    "            reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "            reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "            discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "            discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "\n",
    "            image_log_loss = world_model.compute_log_loss(decoder_distribution, state)\n",
    "            reward_log_loss = world_model.compute_log_loss(reward_distribution, reward)\n",
    "            discount_log_loss = world_model.compute_log_loss(discount_distribution, non_terminal)\n",
    "            kl_loss = world_model.compute_kl_loss(prior_rssm_states, posterior_rssm_states)\n",
    "\n",
    "\n",
    "\n",
    "            loss = image_log_loss + reward_log_loss + discount_log_loss + kl_loss\n",
    "            \n",
    "            print(f\"Image Log Loss: {image_log_loss} Reward Log Loss: {reward_log_loss} Discount Log Loss {discount_log_loss} KL Loss {kl_loss}\")\n",
    "            wandb.log({\"Image Log Loss\": image_log_loss, \"Reward Log Loss\": reward_log_loss, \"Discount Log Loss\": discount_log_loss, \"KL Loss\": kl_loss, \"Loss\": loss})\n",
    "           \n",
    "            # TODO maybe in Gradienttape??\n",
    "            gradients = tape.gradient(loss, combined_trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, combined_trainable_variables))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            world_model.compute_actor_critic_loss(posterior_rssm_states)\n",
    "\n",
    "            gradients = tape.gradient(loss, world_model.actor + world_model.critic)\n",
    "\n",
    "        optimizer_actor.apply_gradients(zip(gradients, world_model.actor))\n",
    "        optimizer_critic.apply_gradients(zip(gradients, world_model.critic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from importlib_metadata import distribution\n",
    "\n",
    "# TODO move hyperparams\n",
    "epochs = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002)\n",
    "\n",
    "buffer = Buffer(batch_size=1)\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 32),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "\n",
    "environment_interactor = EnvironmentInteractor(config, buffer)\n",
    "\n",
    "# Sample from buffer\n",
    "\n",
    "world_model = WorldModel()\n",
    "\n",
    "for episode in range(50):\n",
    "    environment_interactor.create_trajectories(1000)\n",
    "\n",
    "    data = buffer.sample(batch_size=50, prefetch_size=5)\n",
    "\n",
    "    for sequence in data:\n",
    "        state, next_state, action, reward, non_terminal = sequence[0]\n",
    "\n",
    "            # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        models = world_model.models + world_model.rssm.models\n",
    "        combined_trainable_variables = models[0].trainable_variables\n",
    "        for i in range(len(models)):\n",
    "            if i+1 >= len(models):\n",
    "                break\n",
    "            combined_trainable_variables += models[i+1].trainable_variables\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoded_state = world_model.encoder(state)\n",
    "            initial_rssm_state = RSSMState()\n",
    "            prior_rssm_states, posterior_rssm_states = world_model.rssm.observing_rollout(encoded_state, action, non_terminal, initial_rssm_state)\n",
    "            hidden_state_h_and_stochastic_state_z = tf.concat([posterior_rssm_states.stochastic_state_z, posterior_rssm_states.hidden_rnn_state], axis=-1)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            hidden_state_h_and_stochastic_state_z = tf.reshape(hidden_state_h_and_stochastic_state_z, (-1,stochastic_state_size + hidden_unit_size))\n",
    "\n",
    "            decoder_logits = world_model.decoder(hidden_state_h_and_stochastic_state_z)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            decoder_logits = tf.reshape(decoder_logits, (-1, image_shape[0], image_shape[1], image_shape[2]))\n",
    "\n",
    "            decoder_distribution = tfp.distributions.Independent(tfp.distributions.Normal(decoder_logits, 1))\n",
    "            reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "            reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "            discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "            discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "\n",
    "            image_log_loss = compute_log_loss(decoder_distribution, state)\n",
    "            reward_log_loss = compute_log_loss(reward_distribution, reward)\n",
    "            discount_log_loss = compute_log_loss(discount_distribution, non_terminal)\n",
    "            kl_loss = compute_kl_loss(prior_rssm_states, posterior_rssm_states)\n",
    "\n",
    "\n",
    "\n",
    "            loss = image_log_loss + reward_log_loss + discount_log_loss + kl_loss\n",
    "            print(f\"Image Log Loss: {image_log_loss} Reward Log Loss: {reward_log_loss} Discount Log Loss {discount_log_loss} KL Loss {kl_loss}\")\n",
    "            wandb.log({\"Image Log Loss\": image_log_loss, \"Reward Log Loss\": reward_log_loss, \"Discount Log Loss\": discount_log_loss, \"KL Loss\": kl_loss, \"Loss\": loss})\n",
    "            # TODO maybe in Gradienttape??\n",
    "            gradients = tape.gradient(loss, combined_trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, combined_trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "  t, _ = next(iterator)\n",
    "  trajectories.append(t)\n",
    "\n",
    "\n",
    "    train_step((\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "    )\n",
    "\n",
    "#print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World ModelTraining Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = tf.keras.layers.GRUCell(32)\n",
    "r = tf.random.uniform((64,32), 0,3)\n",
    "g(r,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World model & agent training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyperparam inits\n",
    "Agent Data collection in environment + adding data to ERB (+ measure at which reward loop stops?) \n",
    "World model loop on data sampled from ERB\n",
    "Agent training loop with world model feedback\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate environment and network objects\n",
    "# Loop:\n",
    "# Pass respective inputs to networks\n",
    "# Collect outputs\n",
    "# Compute individuall losses\n",
    "# Add together to 1 big loss\n",
    "# Propagate with gradient Tape through network\n",
    "\n",
    "\n",
    "# compute the loss of an input for the model and optimize/tweak according the parameters\n",
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# TODO move hyperparams to the rest\n",
    "epochs = 32\n",
    "\n",
    "# define loss-function and optimizer\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "\n",
    "    for world_model_input in tqdm(data):\n",
    "        train_loss = train_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7b373b46a3ad398ff0fbe06950ea2901f8c9a237be6643544c903d3b2478057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
