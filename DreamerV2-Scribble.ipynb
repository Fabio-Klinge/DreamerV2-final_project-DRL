{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpeterkeffer\u001B[0m (\u001B[33mcogsci\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.0"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\Users\\Peter\\PycharmProjects\\DreamerV2-final_project-DRL\\wandb\\run-20220805_163631-a72p4bd4</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/cogsci/DreamerV2-final_project-DRL/runs/a72p4bd4\" target=\"_blank\">ruby-frost-3</a></strong> to <a href=\"https://wandb.ai/cogsci/DreamerV2-final_project-DRL\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install tensorflow tensorflow_probability tf_agents numpy gym highway-env tqdm wandb\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "# Layer\n",
    "from tensorflow.keras.layers import Dense, Layer, Conv2DTranspose, Conv2D, GlobalAveragePooling2D, Reshape, BatchNormalization, GRUCell, MaxPooling2D, Flatten, RNN\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, KLDivergence\n",
    "import tensorflow_probability as tfp \n",
    "\n",
    "\n",
    "\n",
    "# Buffer \n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Further support\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "wandb.init(settings=wandb.Settings(_disable_stats=True))\n",
    "\n",
    "# Environment\n",
    "import gym\n",
    "import highway_env\n",
    "import random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Has to save (Observation, action, reward, terminal state)\n",
    "from numpy import float32\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=1,\n",
    "        buffer_length=1000, \n",
    "        observation_size=(128,32,1),\n",
    "        action_size=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create replay buffer\n",
    "\n",
    "        Buffer size = batch_size * buffer_length\n",
    "\n",
    "        \"\"\"\n",
    "        # Save batch size for other functions of buffer\n",
    "        # NOT the usual batch size in Deep Learning\n",
    "        # Batches in Uniform Replay Buffer describe size of input added to the buffer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Tell buffer what data & which size to expect\n",
    "        self.data_spec = (\n",
    "            tf.TensorSpec(\n",
    "                shape= observation_size,\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Observation\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=observation_size,\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Next state\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[action_size],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Action\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                # Reward size\n",
    "                shape=[1, ],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Reward\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, ],\n",
    "                # Either 0 or 1 \n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Non-Terminal State\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the buffer \n",
    "        self.buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            self.data_spec, batch_size, buffer_length\n",
    "        )\n",
    "\n",
    "    def obtain_buffer_specs(self):\n",
    "        return self.data_spec\n",
    "\n",
    "    def add(self, items):\n",
    "        \"\"\"\n",
    "        length of items must be equal to batch size\n",
    "\n",
    "        items: list or tuple of batched data from (50, 5)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Combine all values from \"items\" in tensor\n",
    "        # Not sure wether we need tf.nest.map_structure\n",
    "        batched_values = tf.nest.map_structure(\n",
    "            lambda t: tf.stack([t] * self.batch_size),\n",
    "            items\n",
    "        )\n",
    "        \n",
    "        # Add to batch\n",
    "        self.buffer.add_batch(batched_values)\n",
    "\n",
    "    def sample(self, batch_size, prefetch_size):\n",
    "        data = self.buffer.as_dataset(single_deterministic_pass=True)\n",
    "\n",
    "                \n",
    "        # data = data.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
    "        # # normalize inputs from 0/255 to -1/1\n",
    "        # data = data.map(lambda img, target: ((img/128.)-1, target))\n",
    "        # # create one-hot vector for targets\n",
    "        # data = data.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
    "\n",
    "        # normalize inputs from 0/255 to -1/1\n",
    "        data = data.map(lambda buffer_content, _: (((buffer_content[0]/128.)-1, (buffer_content[1]/128.)-1, buffer_content[2], buffer_content[3], buffer_content[4]), _))\n",
    "        data = data.cache()\n",
    "        data = data.batch(batch_size).prefetch(prefetch_size)\n",
    "        #later we want these to be sequences (Do we though)\n",
    "        return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EnvironmentInteractor:\n",
    "\n",
    "  def __init__(self, config, buffer, environment_name = \"highway-fast-v0\"):\n",
    "    self.config = config\n",
    "\n",
    "    self.env = gym.make(environment_name)    \n",
    "    self.env.configure(config)\n",
    "\n",
    "    self.buffer = buffer\n",
    "    # Save sizes of the stupid tensors\n",
    "    self.data_spec = self.buffer.obtain_buffer_specs()\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  def create_trajectories(self, iterations):\n",
    "    state = self.env.reset()\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        action = self.env.action_space.sample()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        self.buffer.add((\n",
    "          tf.cast(tf.constant(state, shape=self.data_spec[0].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(next_state, shape=self.data_spec[1].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(action, shape=self.data_spec[2].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(reward, shape=self.data_spec[3].shape.as_list()), tf.float32),\n",
    "          tf.cast(tf.constant(1-done, shape=self.data_spec[4].shape.as_list()), tf.float32)\n",
    "        ))\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "          state = self.env.reset()\n",
    "\n",
    "\n",
    "  def __del__(self):\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image size\n",
    "image_shape = (128,32, 1)\n",
    "\n",
    "# Long term memory of GRU\n",
    "hidden_unit_size = 200\n",
    "\n",
    "# Z in paper\n",
    "stochastic_state_shape = (32,32)\n",
    "stochastic_state_size = stochastic_state_shape[0] * stochastic_state_shape[1]\n",
    "\n",
    "#\n",
    "action_size = 1\n",
    "horizon = 15\n",
    "discount_factor = 0.995\n",
    "\n",
    "#\n",
    "mlp_hidden_layer_size = 100\n",
    "batch_size = 50\n",
    "\n",
    "# TODO different variable names for network inp/outp sizes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RSSMState(NamedTuple):\n",
    "    logits: tf.Tensor = tf.zeros(shape=(stochastic_state_size,))\n",
    "    stochastic_state_z: tf.Tensor = tf.zeros(shape=(stochastic_state_size,))\n",
    "    hidden_rnn_state: tf.Tensor = tf.zeros(shape=(hidden_unit_size,))\n",
    "\n",
    "    @classmethod\n",
    "    def from_list(cls, rssm_states):\n",
    "        logits = tf.stack([rssm_state.logits for rssm_state in rssm_states])\n",
    "        stochastic_state_z = tf.stack([rssm_state.stochastic_state_z for rssm_state in rssm_states])\n",
    "        hidden_rnn_state = tf.stack([rssm_state.hidden_rnn_state for rssm_state in rssm_states])\n",
    "\n",
    "        return cls(logits, stochastic_state_z, hidden_rnn_state)\n",
    "\n",
    "    def get_hidden_state_h_and_stochastic_state_z(self):\n",
    "        hidden_state_h_and_stochastic_state_z = tf.concat([self.stochastic_state_z, self.hidden_rnn_state], axis=-1)\n",
    "\n",
    "        return hidden_state_h_and_stochastic_state_z\n",
    "\n",
    "    @classmethod\n",
    "    def convert_sequences_to_batches(cls, rssm_state):\n",
    "        logits = cls.convert_sequence_to_batch(rssm_state.logits)\n",
    "        stochastic_state_z = cls.convert_sequence_to_batch(rssm_state.stochastic_state_z)\n",
    "        hidden_rnn_state = cls.convert_sequence_to_batch(rssm_state.hidden_rnn_state)\n",
    "\n",
    "        return cls(logits, stochastic_state_z, hidden_rnn_state)\n",
    "\n",
    "    @classmethod\n",
    "    def convert_sequence_to_batch(cls, sequence):\n",
    "        batch = tf.reshape(sequence, (sequence.shape[0] * sequence.shape[1], *sequence.shape[2:]))\n",
    "        return batch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mWorldModel\u001B[39;00m:\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m      5\u001B[0m         \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36mWorldModel\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreward_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_reward_predictor()\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdiscount_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_discount_predictor()\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_encoder\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_size\u001B[38;5;241m=\u001B[39m\u001B[43mimage_shape\u001B[49m, output_size\u001B[38;5;241m=\u001B[39mhidden_unit_size):\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;66;03m# Third dimension might be obsolete\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     encoder_input \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mInput(shape\u001B[38;5;241m=\u001B[39minput_size)\n\u001B[0;32m     16\u001B[0m     x \u001B[38;5;241m=\u001B[39m Conv2D(\u001B[38;5;241m16\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124melu\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m\"\u001B[39m)(encoder_input) \u001B[38;5;66;03m# 16 layers of filtered 192x48 features\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'image_shape' is not defined"
     ]
    }
   ],
   "source": [
    "class WorldModel:\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = self.create_encoder()\n",
    "        self.decoder = self.create_decoder()\n",
    "        self.reward_model = self.create_reward_predictor()\n",
    "        self.discount_model = self.create_discount_predictor()\n",
    "        self.actor = self.create_actor()\n",
    "        self.critic = self.create_critic()\n",
    "        self.target_critic = tf.keras.models.clone_model(self.critic)\n",
    "\n",
    "\n",
    "\n",
    "    def create_encoder(self, input_size=image_shape, output_size=hidden_unit_size):\n",
    "        # Third dimension might be obsolete\n",
    "        encoder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Conv2D(16, (3, 3), activation=\"elu\", padding=\"same\")(encoder_input) # 16 layers of filtered 192x48 features\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 96x24\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(64, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 48x12\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 48x12\n",
    "        x = GlobalAveragePooling2D()(x) # 64\n",
    "        encoder_output = Dense(output_size, activation = \"elu\")(x)\n",
    "\n",
    "        encoder = tf.keras.Model(encoder_input, encoder_output, name=\"Encoder\")\n",
    "\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    # Input size = 1024(z:32x32) + 200(size of hidden state)\n",
    "    # Output size = game frame\n",
    "    def create_decoder(\n",
    "        self, \n",
    "        input_size=stochastic_state_size + hidden_unit_size, \n",
    "        output_size=image_shape\n",
    "    ):\n",
    "        # Third dimension might be obsolete\n",
    "        decoder_input = tf.keras.Input(shape=input_size)\n",
    "        # TODO WIE SCHLIMM IST EIN MLP HIER?\n",
    "        x = Dense(256, activation= \"elu\")(decoder_input)\n",
    "        x = Reshape((32, 8, 1))(x) \n",
    "        # TODO Check whether correct reshape happens\n",
    "        #tf.debugging.assert_equal(x)\n",
    "        x = Conv2DTranspose(16, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"linear\", padding=\"same\")(x)\n",
    "       # x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = Flatten()(x)\n",
    "        # Might needs shape as Tensor  #event_shape=output_size\n",
    "\n",
    "        # decoder_output = tfp.layers.IndependentNormal(event_shape=output_size)(x)\n",
    "\n",
    "\n",
    "        decoder = tf.keras.Model(\n",
    "            decoder_input,\n",
    "            x,\n",
    "            name=\"Decoder\"\n",
    "        )\n",
    "\n",
    "        return decoder\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_reward_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        reward_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(reward_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size)(x)\n",
    "        # Creates indipendent normal distribution\n",
    "        # Hope is that it learns to output variables over reward space [0,1]\n",
    "        #reward_predictor_output = tfp.layers.IndependentNormal()(x)\n",
    "\n",
    "        reward_predictor = tf.keras.Model(\n",
    "            reward_predictor_input,\n",
    "            x,\n",
    "            name=\"create_reward_predictor\"\n",
    "        )\n",
    "\n",
    "        return reward_predictor\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_discount_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        discount_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(discount_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        # Create 1 output sampled from bernoulli distribution\n",
    "        #discount_predictor_output = tfp.layers.IndependentBernoulli()(x)\n",
    "\n",
    "        discount_predictor = tf.keras.Model(\n",
    "            discount_predictor_input,\n",
    "            x,\n",
    "            name=\"create_discount_predictor\"\n",
    "        )\n",
    "\n",
    "        return discount_predictor\n",
    "\n",
    "    def compute_actor_critic_loss(self, posterior_rssm_state: RSSMState):\n",
    "\n",
    "        # TODO At the moment we are using only batches and not batches of sequences\n",
    "        batched_posterior_rssm_states = tf.stop_gradient(posterior_rssm_state)\n",
    "\n",
    "        dreamed_rssm_states, dreamed_log_probabilities, dreamed_policy_entropies = self.rssm(horizon, self.actor, batched_posterior_rssm_states)\n",
    "\n",
    "        dreamed_hidden_state_h_and_stochastic_state_z = dreamed_rssm_states.get_hidden_state_h_and_stochastic_state_z()\n",
    "\n",
    "        # TODO models definieren self.world_list+self.value_list+[self.TargetValueModel]+[self.DiscountModel]\n",
    "        self.set_trainable_models(models, False)\n",
    "\n",
    "        reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "        reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "        dreamed_reward = reward_distribution.mean()\n",
    "\n",
    "        discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "        discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "        dreamed_discount = discount_factor * discount_distribution * tf.round(discount_distribution.prob(discount_distribution.mean()))\n",
    "\n",
    "        target_value_logits = self.target_critic(hidden_state_h_and_stochastic_state_z)\n",
    "        target_value_distribution = tfp.distributions.Independent(tfp.distributions.Normal(target_value_logits, 1))\n",
    "        dreamed_value = target_value_distribution.mean()\n",
    "\n",
    "        self.set_trainable_models(models, True)\n",
    "\n",
    "\n",
    "        actor_loss, discount, lambda_returns = self.actor_loss(dreamed_reward, dreamed_value, dreamed_discount, dreamed_log_probabilities, dreamed_policy_entropies)\n",
    "        critic_loss = self.critic_loss(dreamed_hidden_state_h_and_stochastic_state_z[:-1], discount, lambda_returns)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "\n",
    "    def actor_loss(self, dreamed_reward, dreamed_value, dreamed_discount, dreamed_log_probabilities, dreamed_policy_entropies, actor_entropy_scale=0.001, lmbda = 0.95):\n",
    "        lambda_returns = self.compute_return(dreamed_reward[:-1], dreamed_value[:-1], dreamed_discount[:-1], bootstrap=dreamed_value[-1], lmbda=lmbda)\n",
    "\n",
    "        advantage = tf.stop_gradient(lambda_returns - dreamed_value[:-1])\n",
    "        objective = dreamed_log_probabilities[1:] * advantage\n",
    "\n",
    "        discounts = tf.concat([tf.ones_like(dreamed_discount[:1]), dreamed_discount[1:]])\n",
    "        discount = tf.math.cumprod(discounts[:-1], 0)\n",
    "        policy_entropy = dreamed_policy_entropies[1:]\n",
    "        actor_loss = -tf.math.reduce_sum(tf.math.reduce_mean(discount * (objective + actor_entropy_scale * policy_entropy), dim=1))\n",
    "        return actor_loss, discount, lambda_returns\n",
    "\n",
    "\n",
    "    def critic_loss(self, dreamed_hidden_state_h_and_stochastic_state_z, discount, lambda_returns):\n",
    "\n",
    "        critic_distribution = self.critic(tf.stop_gradient(dreamed_hidden_state_h_and_stochastic_state_z))\n",
    "        critic_loss = -tf.reduce_mean(tf.stop_gradient(discount) * tf.stop_gradient(critic_distribution.log_prob(lambda_returns)))\n",
    "\n",
    "        return critic_loss\n",
    "\n",
    "    def compute_return(self, reward,\n",
    "                    value,\n",
    "                    discount,\n",
    "                    bootstrap,\n",
    "                    lmbda):\n",
    "\n",
    "        next_values = tf.concat([value[1:], bootstrap[None]], 0)\n",
    "        target = reward + discount + next_values * (1 - lmbda)\n",
    "        timesteps = list(range(reward.shape[0] - 1, -1, -1))\n",
    "        outputs = []\n",
    "        accumulated_reward = bootstrap\n",
    "        for timestep in timesteps:\n",
    "            inp = target[timestep]\n",
    "            discount_factor = discount[timestep]\n",
    "            accumulated_reward = inp + discount_factor * lmbda * accumulated_reward\n",
    "            outputs.append(accumulated_reward)\n",
    "        returns = tf.reverse(tf.stack(outputs), [0])\n",
    "        return returns\n",
    "\n",
    "\n",
    "\n",
    "    def set_trainable_models(self, models, trainable: bool):\n",
    "        for model in models:\n",
    "            model.trainable = trainable\n",
    "\n",
    "    def compute_log_loss(self, distribution, target):\n",
    "        \"\"\"\n",
    "        Computes loss for:\n",
    "        - Image log loss(Output decoder, frame timestep t)\n",
    "        - Reward log loss(Output reward network, obtained reward timestep t)\n",
    "        - Discount log loss(Output of discount network, terminal state timestep t)\n",
    "        \"\"\"\n",
    "        # TODO check whether distribution.log_prob  (target) matches target size\n",
    "        # histogram von wahrsch. distribution /\n",
    "        return -tf.math.reduce_mean(distribution.log_prob(target))\n",
    "\n",
    "\n",
    "    def compute_kl_loss(self, prior_rssm_states, posterior_rssm_states, alpha=0.8):\n",
    "        \"\"\"\n",
    "        alpha: weigh between training the prior toward the representations & regularizing\n",
    "         the representations towards the prior\n",
    "        prior: Z\n",
    "        posterior: Z^\n",
    "        \"\"\"\n",
    "        prior_distribution = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=prior_rssm_states.logits), 1)\n",
    "        posterior_distribution = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=posterior_rssm_states.logits), 1)\n",
    "\n",
    "        prior_distribution_detached = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=tf.stop_gradient(prior_rssm_states.logits)), 1)\n",
    "        posterior_distribution_detached = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=tf.stop_gradient(posterior_rssm_states.logits)), 1)\n",
    "\n",
    "        # Loss with KL Balancing\n",
    "        # TODO check reihenfolge, reduce_mean hat Gradients?!!?\n",
    "        return alpha * tf.math.reduce_mean(tfp.distributions.kl_divergence(posterior_distribution_detached, prior_distribution)) + (1-alpha) * tf.math.reduce_mean(tfp.distributions.kl_divergence(posterior_distribution, prior_distribution_detached))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class RSSM:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_action_embedder = self.create_stochastic_state_action_embedder()\n",
    "        self.rnn = self.create_rnn()\n",
    "        self.prior_model = self.create_prior_stochastic_state_embedder()\n",
    "        self.posterior_model = self.create_posterior_stochastic_state_embedder()\n",
    "\n",
    "    def create_stochastic_state_action_embedder(\n",
    "        self,\n",
    "        input_size=(stochastic_state_size + action_size,),\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        state_action_input = tf.keras.Input(shape=input_size)\n",
    "        state_action_output = Dense(output_size, activation = \"elu\")(state_action_input)\n",
    "\n",
    "        stochastic_state_action_embedder = tf.keras.Model(\n",
    "            state_action_input,\n",
    "            state_action_output,\n",
    "            name=\"stochastic_state_action_embedder\"\n",
    "        )\n",
    "\n",
    "        return stochastic_state_action_embedder\n",
    "\n",
    "    # Contains GRU cell\n",
    "    def create_rnn(\n",
    "        self,\n",
    "        input_size=(hidden_unit_size, ),\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        return RNN(GRUCell(output_size))\n",
    "\n",
    "        rnn_input = tf.keras.Input(shape=input_size)\n",
    "       # rnn_hidden_state_placeholder = tf.keras.Input(shape=(hidden_unit_size,))\n",
    "        rnn_output = rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(output_size))(rnn_input)\n",
    "\n",
    "\n",
    "        rnn = tf.keras.Model(\n",
    "            rnn_input,\n",
    "            rnn_output,\n",
    "            name=\"rnn\"\n",
    "        )\n",
    "\n",
    "        return rnn\n",
    "\n",
    "    # Z^ in paper\n",
    "    def create_prior_stochastic_state_embedder(\n",
    "        self,\n",
    "        input_size=hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(x)\n",
    "\n",
    "        create_prior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_prior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_prior_stochastic_state_embedder\n",
    "\n",
    "    # Z in paper\n",
    "    # Input size = concatenated output of RNN with output of CNN\n",
    "    def create_posterior_stochastic_state_embedder(\n",
    "        self,\n",
    "        input_size=hidden_unit_size+hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(x)\n",
    "\n",
    "        create_posterior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_posterior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_posterior_stochastic_state_embedder\n",
    "\n",
    "    def sample_stochastic_state(self, logits):\n",
    "        \"\"\"\n",
    "        Gets probabilities for each element of class in each category.\n",
    "        Used to generate embeddings from logits.\n",
    "        \"\"\"\n",
    "\n",
    "        # Logit Outputs from MLP\n",
    "        logits = tf.reshape(logits, shape=(-1, *stochastic_state_shape))\n",
    "        # OneHot distribution over logits\n",
    "        logits_distribution = tfp.distributions.OneHotCategorical(logits)\n",
    "        # Sample from OneHot distribution\n",
    "        sample = tf.cast(logits_distribution.sample(), tf.float32)\n",
    "        # TODO observe logits_distribution.prob(sample) after few iterations\n",
    "        sample += logits_distribution.prob(sample) - tf.stop_gradient(logits_distribution.prob(sample))\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def dream(self, previous_rssm_state: RSSMState, previous_action: tf.Tensor, non_terminal=True):\n",
    "        \"\"\"\n",
    "        Creates Z^\n",
    "        \"\"\"\n",
    "        # TODO invert terminal states (terminal state = 1 if episode ended, needs to be 0)\n",
    "        # Embedding of concatenation prior z and action (t-1)\n",
    "        state_action_embedding = self.state_action_embedder(tf.concat([previous_rssm_state.stochastic_state_z * non_terminal, previous_action], axis=1))\n",
    "        # TODO Remove Squeeze\n",
    "        # Create h from GRU with old h (t-1) and the embedding\n",
    "        state_action_embedding = tf.reshape(state_action_embedding, shape=(-1, 200, 1))\n",
    "\n",
    "        hidden_rnn_state = self.rnn(state_action_embedding, previous_rssm_state.hidden_rnn_state * non_terminal)\n",
    "\n",
    "        # Logits created from h (with MLP) to create Z^\n",
    "        prior_logits = self.prior_model(hidden_rnn_state)\n",
    "        # Create Z^\n",
    "        prior_stochastic_state_z = self.sample_stochastic_state(prior_logits)\n",
    "        # Save logits for Z^, Z^ and h\n",
    "        prior_rssm_state = RSSMState(prior_logits, tf.reshape(prior_stochastic_state_z, (-1, stochastic_state_size)), hidden_rnn_state)\n",
    "\n",
    "        return prior_rssm_state\n",
    "\n",
    "    def dreaming_rollout(self, horizon: int, actor: tf.keras.Model, previous_rssm_state: RSSMState):\n",
    "        \"\"\"\n",
    "        Rollout only Z\n",
    "        \"\"\"\n",
    "        rssm_state = previous_rssm_state\n",
    "\n",
    "        next_rssm_states = []\n",
    "        action_entropies = []\n",
    "        image_log_probabilities = []\n",
    "\n",
    "        for timestep in range(horizon):\n",
    "            action, action_distribution = actor(tf.stop_gradient(rssm_state.get_hidden_state_h_and_stochastic_state_z()))\n",
    "            rssm_state = self.dream(rssm_state, action)\n",
    "\n",
    "            next_rssm_states.append(rssm_state)\n",
    "            action_entropies.append(action_distribution.entropy(action))\n",
    "            image_log_probabilities(action_distribution.log_prob(action))\n",
    "\n",
    "        next_rssm_states = RSSMState.from_list(next_rssm_states)\n",
    "        action_entropies = tf.stack(action_entropies, dim=0)\n",
    "        image_log_probabilities = tf.stack(image_log_probabilities, dim=0)\n",
    "\n",
    "        return next_rssm_states, image_log_probabilities, action_entropies\n",
    "\n",
    "\n",
    "    def observe(self, encoded_state: tf.Tensor, previous_action: tf.Tensor, previous_non_terminal: tf.Tensor, previous_rssm_state: RSSMState):\n",
    "        \"\"\"\n",
    "        Creates Z' and Z\n",
    "        \"\"\"\n",
    "        # Obtain Z^\n",
    "        prior_rssm_state = self.dream(previous_rssm_state, previous_action, previous_non_terminal)\n",
    "\n",
    "        # concatenates h and the output of our CNN (encoded input frame X)\n",
    "        encoded_state_and_hidden_state = tf.concat([prior_rssm_state.hidden_rnn_state, encoded_state], axis=1)\n",
    "\n",
    "        # Logits created from concat of h and encoded frame X (with MLP) to create Z\n",
    "        posterior_logits = self.posterior_model(encoded_state_and_hidden_state)\n",
    "        # Create Z\n",
    "        posterior_stochastic_state_z = self.sample_stochastic_state(posterior_logits)\n",
    "        # Saves logits for Z, Z, and h\n",
    "        posterior_rssm_state = RSSMState(posterior_logits, tf.reshape(posterior_stochastic_state_z, (-1, stochastic_state_size)), prior_rssm_state.hidden_rnn_state)\n",
    "\n",
    "        return prior_rssm_state, posterior_rssm_state\n",
    "\n",
    "    def observing_rollout(self, encoded_states: tf.Tensor, actions: tf.Tensor, non_terminals: tf.Tensor, previous_rssm_state: RSSMState):\n",
    "        prior_rssm_states = []\n",
    "        posterior_rssm_states = []\n",
    "\n",
    "        for encoded_state, action, non_terminal in zip(encoded_states, actions, non_terminals):\n",
    "            # TODO remove islandsolution\n",
    "            encoded_state = tf.expand_dims(encoded_state, axis=0)\n",
    "            action = tf.expand_dims(action, axis=0)\n",
    "            non_terminal = tf.expand_dims(non_terminal, axis=0)\n",
    "            #?? 0 if terminal state is reached\n",
    "            previous_action = action * non_terminal\n",
    "            # Z^, Z\n",
    "            prior_rssm_state, posterior_rssm_state = self.observe(encoded_state, previous_action, non_terminal, previous_rssm_state)\n",
    "\n",
    "            # Save Z^, Z\n",
    "            prior_rssm_states.append(prior_rssm_state)\n",
    "            posterior_rssm_states.append(posterior_rssm_state)\n",
    "\n",
    "            # Z for next iteration\n",
    "            previous_rssm_state = posterior_rssm_state\n",
    "        prior_rssm_states = RSSMState.from_list(prior_rssm_states)\n",
    "        posterior_rssm_states = RSSMState.from_list(posterior_rssm_states)\n",
    "\n",
    "        return prior_rssm_states, posterior_rssm_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\envs\\common\\observation.py:215: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame.from_records(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Peter\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:342: calling _Independent.__init__ (from tensorflow_probability.python.distributions.independent) with reinterpreted_batch_ndims=None is deprecated and will be removed after 2022-03-01.\n",
      "Instructions for updating:\n",
      "Please pass an integer value for `reinterpreted_batch_ndims`. The current behavior corresponds to `reinterpreted_batch_ndims=tf.size(distribution.batch_shape_tensor()) - 1`.\n",
      "Image Log Loss: 4098.0341796875 Reward Log Loss: 127.16785430908203 Discount Log Loss 69.48236846923828 KL Loss 0.00021589998505078256\n",
      "Image Log Loss: 4099.5986328125 Reward Log Loss: 123.47802734375 Discount Log Loss 69.29757690429688 KL Loss 0.00022834003902971745\n",
      "Image Log Loss: 4099.32666015625 Reward Log Loss: 122.35476684570312 Discount Log Loss 69.08842468261719 KL Loss 0.00024108559591695666\n",
      "Image Log Loss: 4097.93701171875 Reward Log Loss: 124.6875991821289 Discount Log Loss 69.00785827636719 KL Loss 0.0002294034493388608\n",
      "Image Log Loss: 4097.6494140625 Reward Log Loss: 123.93911743164062 Discount Log Loss 68.76273345947266 KL Loss 0.0002383828687015921\n",
      "Image Log Loss: 4096.54833984375 Reward Log Loss: 123.92155456542969 Discount Log Loss 68.9259262084961 KL Loss 0.00023135272203944623\n",
      "Image Log Loss: 4096.98828125 Reward Log Loss: 125.53955078125 Discount Log Loss 68.47079467773438 KL Loss 0.0002297523315064609\n",
      "Image Log Loss: 4096.978515625 Reward Log Loss: 124.32771301269531 Discount Log Loss 68.05847930908203 KL Loss 0.0002466151490807533\n",
      "Image Log Loss: 4096.10302734375 Reward Log Loss: 119.37824249267578 Discount Log Loss 68.34856414794922 KL Loss 0.00026338896714150906\n",
      "Image Log Loss: 4096.4033203125 Reward Log Loss: 118.80339813232422 Discount Log Loss 67.48722839355469 KL Loss 0.0002627981884870678\n",
      "Image Log Loss: 4094.60693359375 Reward Log Loss: 118.92984008789062 Discount Log Loss 67.8989486694336 KL Loss 0.00027263129595667124\n",
      "Image Log Loss: 4096.14111328125 Reward Log Loss: 122.8141860961914 Discount Log Loss 67.33168029785156 KL Loss 0.0002746620448306203\n",
      "Image Log Loss: 4094.443115234375 Reward Log Loss: 118.7076644897461 Discount Log Loss 67.05103302001953 KL Loss 0.00028713230858556926\n",
      "Image Log Loss: 4096.45703125 Reward Log Loss: 120.47103881835938 Discount Log Loss 66.60983276367188 KL Loss 0.0002857930667232722\n",
      "Image Log Loss: 4093.999267578125 Reward Log Loss: 116.94754791259766 Discount Log Loss 66.71084594726562 KL Loss 0.00029373724828474224\n",
      "Image Log Loss: 4094.402587890625 Reward Log Loss: 117.56312561035156 Discount Log Loss 66.70028686523438 KL Loss 0.0002848651201929897\n",
      "Image Log Loss: 4094.63818359375 Reward Log Loss: 117.77323150634766 Discount Log Loss 66.11650085449219 KL Loss 0.00032817854662425816\n",
      "Image Log Loss: 4093.7294921875 Reward Log Loss: 117.14981079101562 Discount Log Loss 66.0877914428711 KL Loss 0.00033294555032625794\n",
      "Image Log Loss: 4094.239990234375 Reward Log Loss: 114.35445404052734 Discount Log Loss 65.4317855834961 KL Loss 0.0003661819500848651\n",
      "Image Log Loss: 4094.500732421875 Reward Log Loss: 120.34780883789062 Discount Log Loss 65.69099426269531 KL Loss 0.0004034198063891381\n",
      "Image Log Loss: 4090.5791015625 Reward Log Loss: 117.60409545898438 Discount Log Loss 64.9441146850586 KL Loss 0.00040503175114281476\n",
      "Image Log Loss: 4092.975830078125 Reward Log Loss: 113.1279296875 Discount Log Loss 64.34688568115234 KL Loss 0.0004078217607457191\n",
      "Image Log Loss: 4094.15185546875 Reward Log Loss: 113.86808776855469 Discount Log Loss 64.07459259033203 KL Loss 0.0004392955161165446\n",
      "Image Log Loss: 4092.3623046875 Reward Log Loss: 113.12899780273438 Discount Log Loss 63.67123031616211 KL Loss 0.0004886736860498786\n",
      "Image Log Loss: 4090.811279296875 Reward Log Loss: 112.35441589355469 Discount Log Loss 63.979591369628906 KL Loss 0.0005078286631032825\n",
      "Image Log Loss: 4090.400634765625 Reward Log Loss: 112.03384399414062 Discount Log Loss 63.80794906616211 KL Loss 0.0005181551678106189\n",
      "Image Log Loss: 4091.231201171875 Reward Log Loss: 107.71337127685547 Discount Log Loss 62.75048828125 KL Loss 0.0005763083463534713\n",
      "Image Log Loss: 4089.585693359375 Reward Log Loss: 109.06474304199219 Discount Log Loss 61.40460968017578 KL Loss 0.0006511531537398696\n",
      "Image Log Loss: 4088.759765625 Reward Log Loss: 108.33698272705078 Discount Log Loss 61.860782623291016 KL Loss 0.0006517560686916113\n",
      "Image Log Loss: 4089.0556640625 Reward Log Loss: 107.2809066772461 Discount Log Loss 61.283817291259766 KL Loss 0.0007281265570782125\n",
      "Image Log Loss: 4091.08056640625 Reward Log Loss: 107.91642761230469 Discount Log Loss 59.808956146240234 KL Loss 0.0008494482026435435\n",
      "Image Log Loss: 4089.113037109375 Reward Log Loss: 109.68716430664062 Discount Log Loss 59.89597702026367 KL Loss 0.000934188487008214\n",
      "Image Log Loss: 4088.382568359375 Reward Log Loss: 106.37606811523438 Discount Log Loss 59.591583251953125 KL Loss 0.001085548079572618\n",
      "Image Log Loss: 4090.1142578125 Reward Log Loss: 107.50810241699219 Discount Log Loss 57.679901123046875 KL Loss 0.001142098568379879\n",
      "Image Log Loss: 4088.8349609375 Reward Log Loss: 103.81805419921875 Discount Log Loss 57.203731536865234 KL Loss 0.0013268104521557689\n",
      "Image Log Loss: 4088.49755859375 Reward Log Loss: 102.83109283447266 Discount Log Loss 56.71784973144531 KL Loss 0.0015381094999611378\n",
      "Image Log Loss: 4090.6259765625 Reward Log Loss: 103.40241241455078 Discount Log Loss 54.60945129394531 KL Loss 0.0016983328387141228\n",
      "Image Log Loss: 4087.346923828125 Reward Log Loss: 101.7091293334961 Discount Log Loss 55.24298858642578 KL Loss 0.0019993765745311975\n",
      "Image Log Loss: 4086.768798828125 Reward Log Loss: 101.34725952148438 Discount Log Loss 50.92338943481445 KL Loss 0.002499778289347887\n",
      "Image Log Loss: 4085.375732421875 Reward Log Loss: 101.35238647460938 Discount Log Loss 50.451080322265625 KL Loss 0.003281006123870611\n",
      "Image Log Loss: 4083.322509765625 Reward Log Loss: 102.9476547241211 Discount Log Loss 46.979209899902344 KL Loss 0.004520208574831486\n",
      "Image Log Loss: 4082.6669921875 Reward Log Loss: 106.89310455322266 Discount Log Loss 45.64791488647461 KL Loss 0.006790990941226482\n",
      "Image Log Loss: 4079.29443359375 Reward Log Loss: 107.4505844116211 Discount Log Loss 45.343955993652344 KL Loss 0.007299702148884535\n",
      "Image Log Loss: 4078.554931640625 Reward Log Loss: 103.85871887207031 Discount Log Loss 46.840789794921875 KL Loss 0.005737737752497196\n",
      "Image Log Loss: 4079.685546875 Reward Log Loss: 101.2164077758789 Discount Log Loss 43.300682067871094 KL Loss 0.0047605326399207115\n",
      "Image Log Loss: 4080.9931640625 Reward Log Loss: 99.89033508300781 Discount Log Loss 41.840087890625 KL Loss 0.004233337473124266\n",
      "Image Log Loss: 4079.051025390625 Reward Log Loss: 98.89138793945312 Discount Log Loss 42.991432189941406 KL Loss 0.003757069818675518\n",
      "Image Log Loss: 4077.79931640625 Reward Log Loss: 98.63578033447266 Discount Log Loss 44.39882278442383 KL Loss 0.0036335070617496967\n",
      "Image Log Loss: 4078.878173828125 Reward Log Loss: 97.73426055908203 Discount Log Loss 42.205909729003906 KL Loss 0.003811498172581196\n",
      "Image Log Loss: 4077.987548828125 Reward Log Loss: 97.75550842285156 Discount Log Loss 36.81904983520508 KL Loss 0.004638458602130413\n",
      "Image Log Loss: 4076.458984375 Reward Log Loss: 99.47137451171875 Discount Log Loss 37.13407516479492 KL Loss 0.00608739722520113\n",
      "Image Log Loss: 4076.495849609375 Reward Log Loss: 101.08381652832031 Discount Log Loss 34.92206573486328 KL Loss 0.008131194859743118\n",
      "Image Log Loss: 4076.715576171875 Reward Log Loss: 100.276611328125 Discount Log Loss 29.4614200592041 KL Loss 0.008261915296316147\n",
      "Image Log Loss: 4071.539306640625 Reward Log Loss: 97.56456756591797 Discount Log Loss 32.60231018066406 KL Loss 0.005983223207294941\n",
      "Image Log Loss: 4073.948486328125 Reward Log Loss: 96.98367309570312 Discount Log Loss 31.700950622558594 KL Loss 0.005552586633712053\n",
      "Image Log Loss: 4071.902587890625 Reward Log Loss: 96.61405944824219 Discount Log Loss 32.74464797973633 KL Loss 0.005708472337573767\n",
      "Image Log Loss: 4067.615234375 Reward Log Loss: 96.42849731445312 Discount Log Loss 28.586503982543945 KL Loss 0.0072393277660012245\n",
      "Image Log Loss: 4070.197509765625 Reward Log Loss: 97.49601745605469 Discount Log Loss 24.936424255371094 KL Loss 0.008657615631818771\n",
      "Image Log Loss: 4067.486328125 Reward Log Loss: 97.00147247314453 Discount Log Loss 41.38621520996094 KL Loss 0.007037774194031954\n",
      "Image Log Loss: 4066.867919921875 Reward Log Loss: 95.24560546875 Discount Log Loss 27.428525924682617 KL Loss 0.005000114440917969\n",
      "Image Log Loss: 4066.476806640625 Reward Log Loss: 94.74263763427734 Discount Log Loss 22.19293975830078 KL Loss 0.004443739540874958\n",
      "Image Log Loss: 4065.278076171875 Reward Log Loss: 95.92709350585938 Discount Log Loss 35.819122314453125 KL Loss 0.0044647264294326305\n",
      "Image Log Loss: 4061.86181640625 Reward Log Loss: 94.9433822631836 Discount Log Loss 24.282129287719727 KL Loss 0.005276345647871494\n",
      "Image Log Loss: 4061.614990234375 Reward Log Loss: 94.85494232177734 Discount Log Loss 29.743181228637695 KL Loss 0.005718959029763937\n",
      "Image Log Loss: 4060.727294921875 Reward Log Loss: 95.25178527832031 Discount Log Loss 23.704936981201172 KL Loss 0.00582974823191762\n",
      "Image Log Loss: 4066.489990234375 Reward Log Loss: 94.36094665527344 Discount Log Loss 17.044775009155273 KL Loss 0.005491701420396566\n",
      "Image Log Loss: 4056.79931640625 Reward Log Loss: 94.94617462158203 Discount Log Loss 27.482528686523438 KL Loss 0.005075246561318636\n",
      "Image Log Loss: 4061.110107421875 Reward Log Loss: 94.9077377319336 Discount Log Loss 24.070899963378906 KL Loss 0.00470283767208457\n",
      "Image Log Loss: 4060.8994140625 Reward Log Loss: 94.68737030029297 Discount Log Loss 23.405502319335938 KL Loss 0.004413936752825975\n",
      "Image Log Loss: 4060.7705078125 Reward Log Loss: 94.33607482910156 Discount Log Loss 19.821531295776367 KL Loss 0.0043560522608459\n",
      "Image Log Loss: 4057.1767578125 Reward Log Loss: 94.732421875 Discount Log Loss 29.623239517211914 KL Loss 0.0041510118171572685\n",
      "Image Log Loss: 4055.325927734375 Reward Log Loss: 94.48876953125 Discount Log Loss 21.67730712890625 KL Loss 0.004437352530658245\n",
      "Image Log Loss: 4054.999267578125 Reward Log Loss: 94.38623046875 Discount Log Loss 18.424551010131836 KL Loss 0.004526810720562935\n",
      "Image Log Loss: 4051.90283203125 Reward Log Loss: 94.0058822631836 Discount Log Loss 23.180335998535156 KL Loss 0.004848217125982046\n",
      "Image Log Loss: 4056.063232421875 Reward Log Loss: 94.38843536376953 Discount Log Loss 22.834909439086914 KL Loss 0.005313625559210777\n",
      "Image Log Loss: 4054.7099609375 Reward Log Loss: 94.13146209716797 Discount Log Loss 17.12030601501465 KL Loss 0.005399561021476984\n",
      "Image Log Loss: 4051.957275390625 Reward Log Loss: 94.17387390136719 Discount Log Loss 14.162985801696777 KL Loss 0.005466293543577194\n",
      "Image Log Loss: 4051.038818359375 Reward Log Loss: 94.56695556640625 Discount Log Loss 16.408329010009766 KL Loss 0.0057660783641040325\n",
      "Image Log Loss: 4051.366455078125 Reward Log Loss: 94.19482421875 Discount Log Loss 18.650718688964844 KL Loss 0.0054876841604709625\n",
      "Image Log Loss: 4050.695556640625 Reward Log Loss: 94.14559936523438 Discount Log Loss 13.015283584594727 KL Loss 0.004833762999624014\n",
      "Image Log Loss: 4048.41064453125 Reward Log Loss: 93.72991943359375 Discount Log Loss 15.413405418395996 KL Loss 0.004509810358285904\n",
      "Image Log Loss: 4045.894287109375 Reward Log Loss: 94.13887786865234 Discount Log Loss 12.567821502685547 KL Loss 0.004042947664856911\n",
      "Image Log Loss: 4050.72119140625 Reward Log Loss: 94.00104522705078 Discount Log Loss 9.926393508911133 KL Loss 0.003372382838279009\n",
      "Image Log Loss: 4046.340087890625 Reward Log Loss: 94.1493911743164 Discount Log Loss 14.414039611816406 KL Loss 0.0035773497074842453\n",
      "Image Log Loss: 4048.940673828125 Reward Log Loss: 93.68914031982422 Discount Log Loss 14.170644760131836 KL Loss 0.0033238553442060947\n",
      "Image Log Loss: 4043.777587890625 Reward Log Loss: 93.83621215820312 Discount Log Loss 13.801955223083496 KL Loss 0.003476985264569521\n",
      "Image Log Loss: 4042.577392578125 Reward Log Loss: 93.91203308105469 Discount Log Loss 18.577930450439453 KL Loss 0.004430411383509636\n",
      "Image Log Loss: 4044.661865234375 Reward Log Loss: 93.75152587890625 Discount Log Loss 11.124682426452637 KL Loss 0.004061957821249962\n",
      "Image Log Loss: 4043.559326171875 Reward Log Loss: 93.67682647705078 Discount Log Loss 13.387500762939453 KL Loss 0.004873668774962425\n",
      "Image Log Loss: 4044.51123046875 Reward Log Loss: 93.69467163085938 Discount Log Loss 15.685978889465332 KL Loss 0.006327934097498655\n",
      "Image Log Loss: 4042.99072265625 Reward Log Loss: 93.99097442626953 Discount Log Loss 18.146303176879883 KL Loss 0.00700623681768775\n",
      "Image Log Loss: 4041.02685546875 Reward Log Loss: 93.43386840820312 Discount Log Loss 17.853429794311523 KL Loss 0.007111688144505024\n",
      "Image Log Loss: 4043.68115234375 Reward Log Loss: 93.8573226928711 Discount Log Loss 15.4203519821167 KL Loss 0.0064576552249491215\n",
      "Image Log Loss: 4040.695556640625 Reward Log Loss: 93.73904418945312 Discount Log Loss 10.275639533996582 KL Loss 0.005960261914879084\n",
      "Image Log Loss: 4042.64599609375 Reward Log Loss: 93.62950134277344 Discount Log Loss 14.842367172241211 KL Loss 0.007288599386811256\n",
      "Image Log Loss: 4040.263671875 Reward Log Loss: 93.53997802734375 Discount Log Loss 7.552849292755127 KL Loss 0.0062935929745435715\n",
      "Image Log Loss: 4040.797607421875 Reward Log Loss: 93.57795715332031 Discount Log Loss 12.215841293334961 KL Loss 0.008111825212836266\n",
      "Image Log Loss: 4035.340087890625 Reward Log Loss: 93.43370819091797 Discount Log Loss 14.197691917419434 KL Loss 0.00995277613401413\n",
      "Image Log Loss: 4040.2060546875 Reward Log Loss: 93.46246337890625 Discount Log Loss 12.092747688293457 KL Loss 0.008302802219986916\n",
      "Image Log Loss: 4039.439697265625 Reward Log Loss: 93.76701354980469 Discount Log Loss 14.331159591674805 KL Loss 0.010016382671892643\n",
      "Image Log Loss: 4036.737548828125 Reward Log Loss: 93.4862289428711 Discount Log Loss 11.602128982543945 KL Loss 0.009499055333435535\n",
      "Image Log Loss: 4035.453125 Reward Log Loss: 93.39944458007812 Discount Log Loss 9.133852005004883 KL Loss 0.008900524117052555\n",
      "Image Log Loss: 4039.07568359375 Reward Log Loss: 93.14079284667969 Discount Log Loss 11.135332107543945 KL Loss 0.01163879781961441\n",
      "Image Log Loss: 4037.753662109375 Reward Log Loss: 93.64891815185547 Discount Log Loss 9.119192123413086 KL Loss 0.010850658640265465\n",
      "Image Log Loss: 4038.170654296875 Reward Log Loss: 93.95970916748047 Discount Log Loss 22.44869613647461 KL Loss 0.02294563688337803\n",
      "Image Log Loss: 4037.801513671875 Reward Log Loss: 93.52103424072266 Discount Log Loss 10.885268211364746 KL Loss 0.011782603338360786\n",
      "Image Log Loss: 4035.0361328125 Reward Log Loss: 93.28952026367188 Discount Log Loss 10.328943252563477 KL Loss 0.011726371943950653\n",
      "Image Log Loss: 4034.9658203125 Reward Log Loss: 93.53284454345703 Discount Log Loss 15.397163391113281 KL Loss 0.017121916636824608\n",
      "Image Log Loss: 4033.989990234375 Reward Log Loss: 93.25970458984375 Discount Log Loss 12.731435775756836 KL Loss 0.014602430164813995\n",
      "Image Log Loss: 4037.416259765625 Reward Log Loss: 93.43968963623047 Discount Log Loss 17.076038360595703 KL Loss 0.021083449944853783\n",
      "Image Log Loss: 4032.943115234375 Reward Log Loss: 93.61080169677734 Discount Log Loss 17.166221618652344 KL Loss 0.01920153573155403\n",
      "Image Log Loss: 4034.599365234375 Reward Log Loss: 93.63658142089844 Discount Log Loss 8.35460376739502 KL Loss 0.010961172170937061\n",
      "Image Log Loss: 4034.8388671875 Reward Log Loss: 92.89098358154297 Discount Log Loss 9.796021461486816 KL Loss 0.009740355424582958\n",
      "Image Log Loss: 4032.023193359375 Reward Log Loss: 93.67913818359375 Discount Log Loss 10.346251487731934 KL Loss 0.010762786492705345\n",
      "Image Log Loss: 4031.055908203125 Reward Log Loss: 92.83163452148438 Discount Log Loss 5.2416205406188965 KL Loss 0.006579915061593056\n",
      "Image Log Loss: 4035.4912109375 Reward Log Loss: 93.61039733886719 Discount Log Loss 14.256691932678223 KL Loss 0.013896631076931953\n",
      "Image Log Loss: 4034.514892578125 Reward Log Loss: 93.46208953857422 Discount Log Loss 9.98367691040039 KL Loss 0.011549865826964378\n",
      "Image Log Loss: 4030.376953125 Reward Log Loss: 93.56078338623047 Discount Log Loss 6.064499378204346 KL Loss 0.009186786599457264\n",
      "Image Log Loss: 4034.660888671875 Reward Log Loss: 93.75060272216797 Discount Log Loss 7.688294887542725 KL Loss 0.011170570738613605\n",
      "Image Log Loss: 4031.130615234375 Reward Log Loss: 93.04388427734375 Discount Log Loss 9.907917976379395 KL Loss 0.012885737232863903\n",
      "Image Log Loss: 4031.689697265625 Reward Log Loss: 93.3948745727539 Discount Log Loss 13.616044044494629 KL Loss 0.015159322880208492\n",
      "Image Log Loss: 4032.060302734375 Reward Log Loss: 93.08645629882812 Discount Log Loss 15.438070297241211 KL Loss 0.017858322709798813\n",
      "Image Log Loss: 4033.544921875 Reward Log Loss: 93.218994140625 Discount Log Loss 7.6801438331604 KL Loss 0.010719475336372852\n",
      "Image Log Loss: 4031.967529296875 Reward Log Loss: 93.6689224243164 Discount Log Loss 9.576252937316895 KL Loss 0.012631000950932503\n",
      "Image Log Loss: 4030.3232421875 Reward Log Loss: 92.90971374511719 Discount Log Loss 10.64416217803955 KL Loss 0.013651350513100624\n",
      "Image Log Loss: 4029.344482421875 Reward Log Loss: 93.39979553222656 Discount Log Loss 9.476546287536621 KL Loss 0.014100008644163609\n",
      "Image Log Loss: 4030.715576171875 Reward Log Loss: 93.35337829589844 Discount Log Loss 15.375635147094727 KL Loss 0.018860775977373123\n",
      "Image Log Loss: 4031.916259765625 Reward Log Loss: 93.27874755859375 Discount Log Loss 11.150422096252441 KL Loss 0.013644814491271973\n",
      "Image Log Loss: 4030.511962890625 Reward Log Loss: 93.39363098144531 Discount Log Loss 13.165912628173828 KL Loss 0.016942694783210754\n",
      "Image Log Loss: 4029.063720703125 Reward Log Loss: 93.45563507080078 Discount Log Loss 7.191830635070801 KL Loss 0.012927009724080563\n",
      "Image Log Loss: 4032.21875 Reward Log Loss: 93.6305923461914 Discount Log Loss 13.152200698852539 KL Loss 0.020010128617286682\n",
      "Image Log Loss: 4029.71875 Reward Log Loss: 93.32482147216797 Discount Log Loss 12.60519027709961 KL Loss 0.019427509978413582\n",
      "Image Log Loss: 4031.842529296875 Reward Log Loss: 93.59884643554688 Discount Log Loss 15.823168754577637 KL Loss 0.02094227448105812\n",
      "Image Log Loss: 4027.869384765625 Reward Log Loss: 93.22478485107422 Discount Log Loss 5.3790602684021 KL Loss 0.011116737499833107\n",
      "Image Log Loss: 4029.916259765625 Reward Log Loss: 93.39710998535156 Discount Log Loss 12.572498321533203 KL Loss 0.015224018134176731\n",
      "Image Log Loss: 4029.316162109375 Reward Log Loss: 93.08540344238281 Discount Log Loss 7.296591281890869 KL Loss 0.009437930770218372\n",
      "Image Log Loss: 4029.36474609375 Reward Log Loss: 93.27328491210938 Discount Log Loss 12.206798553466797 KL Loss 0.014092101715505123\n",
      "Image Log Loss: 4028.51123046875 Reward Log Loss: 92.94812774658203 Discount Log Loss 13.227733612060547 KL Loss 0.013764739036560059\n",
      "Image Log Loss: 4028.69873046875 Reward Log Loss: 92.969970703125 Discount Log Loss 5.935576915740967 KL Loss 0.00825019832700491\n",
      "Image Log Loss: 4032.304931640625 Reward Log Loss: 93.37596893310547 Discount Log Loss 12.268852233886719 KL Loss 0.012707056477665901\n",
      "Image Log Loss: 4027.699951171875 Reward Log Loss: 93.22880554199219 Discount Log Loss 13.762874603271484 KL Loss 0.01247621513903141\n",
      "Image Log Loss: 4030.94873046875 Reward Log Loss: 93.1692123413086 Discount Log Loss 7.065544605255127 KL Loss 0.007297667209059\n",
      "Image Log Loss: 4027.373046875 Reward Log Loss: 93.20718383789062 Discount Log Loss 10.051214218139648 KL Loss 0.009552394971251488\n",
      "Image Log Loss: 4030.965087890625 Reward Log Loss: 93.17723846435547 Discount Log Loss 11.768277168273926 KL Loss 0.01071893610060215\n",
      "Image Log Loss: 4027.235107421875 Reward Log Loss: 93.30060577392578 Discount Log Loss 14.385210990905762 KL Loss 0.01344581600278616\n",
      "Image Log Loss: 4032.169921875 Reward Log Loss: 93.06668853759766 Discount Log Loss 13.737300872802734 KL Loss 0.01258355937898159\n",
      "Image Log Loss: 4026.139892578125 Reward Log Loss: 93.09587097167969 Discount Log Loss 8.329136848449707 KL Loss 0.008808787912130356\n",
      "Image Log Loss: 4028.8935546875 Reward Log Loss: 93.45834350585938 Discount Log Loss 9.649087905883789 KL Loss 0.009155597537755966\n",
      "Image Log Loss: 4025.638671875 Reward Log Loss: 93.09772491455078 Discount Log Loss 7.127015590667725 KL Loss 0.006536541506648064\n",
      "Image Log Loss: 4025.698974609375 Reward Log Loss: 93.48013305664062 Discount Log Loss 8.283266067504883 KL Loss 0.007241717539727688\n",
      "Image Log Loss: 4027.734130859375 Reward Log Loss: 93.16006469726562 Discount Log Loss 5.996294975280762 KL Loss 0.005749514792114496\n",
      "Image Log Loss: 4024.780517578125 Reward Log Loss: 93.2402114868164 Discount Log Loss 6.3544392585754395 KL Loss 0.005612729117274284\n",
      "Image Log Loss: 4028.42431640625 Reward Log Loss: 93.37857055664062 Discount Log Loss 9.51463508605957 KL Loss 0.006863438058644533\n",
      "Image Log Loss: 4028.99853515625 Reward Log Loss: 93.49260711669922 Discount Log Loss 12.436588287353516 KL Loss 0.00847618468105793\n",
      "Image Log Loss: 4024.695556640625 Reward Log Loss: 93.23226928710938 Discount Log Loss 10.805645942687988 KL Loss 0.007980815134942532\n",
      "Image Log Loss: 4026.676025390625 Reward Log Loss: 93.33031463623047 Discount Log Loss 9.544295310974121 KL Loss 0.007074681110680103\n",
      "Image Log Loss: 4028.969970703125 Reward Log Loss: 93.94523620605469 Discount Log Loss 7.313090801239014 KL Loss 0.006096530705690384\n",
      "Image Log Loss: 4025.934326171875 Reward Log Loss: 93.09319305419922 Discount Log Loss 9.511185646057129 KL Loss 0.0071676140651106834\n",
      "Image Log Loss: 4026.6728515625 Reward Log Loss: 93.49401092529297 Discount Log Loss 9.091288566589355 KL Loss 0.006490706000477076\n",
      "Image Log Loss: 4023.17333984375 Reward Log Loss: 93.21134948730469 Discount Log Loss 6.222110748291016 KL Loss 0.004329829942435026\n",
      "Image Log Loss: 4028.716796875 Reward Log Loss: 93.1454086303711 Discount Log Loss 7.67835807800293 KL Loss 0.00541150476783514\n",
      "Image Log Loss: 4023.909423828125 Reward Log Loss: 93.32564544677734 Discount Log Loss 7.226510047912598 KL Loss 0.005492148455232382\n",
      "Image Log Loss: 4023.890380859375 Reward Log Loss: 93.31986999511719 Discount Log Loss 7.527101993560791 KL Loss 0.005544381681829691\n",
      "Image Log Loss: 4025.2607421875 Reward Log Loss: 93.34381866455078 Discount Log Loss 9.050012588500977 KL Loss 0.006726002786308527\n",
      "Image Log Loss: 4024.638671875 Reward Log Loss: 92.94373321533203 Discount Log Loss 7.503716468811035 KL Loss 0.0061097899451851845\n",
      "Image Log Loss: 4025.349365234375 Reward Log Loss: 93.14115142822266 Discount Log Loss 10.030511856079102 KL Loss 0.00852961465716362\n",
      "Image Log Loss: 4022.870361328125 Reward Log Loss: 93.4628677368164 Discount Log Loss 6.072504997253418 KL Loss 0.005966881290078163\n",
      "Image Log Loss: 4023.416015625 Reward Log Loss: 93.80545806884766 Discount Log Loss 6.990585803985596 KL Loss 0.008858943358063698\n",
      "Image Log Loss: 4028.7431640625 Reward Log Loss: 93.32212829589844 Discount Log Loss 9.896842002868652 KL Loss 0.012194154784083366\n",
      "Image Log Loss: 4023.662109375 Reward Log Loss: 93.21771240234375 Discount Log Loss 4.6819167137146 KL Loss 0.008537661284208298\n",
      "Image Log Loss: 4026.999267578125 Reward Log Loss: 93.53633117675781 Discount Log Loss 10.163677215576172 KL Loss 0.01464906893670559\n",
      "Image Log Loss: 4022.66552734375 Reward Log Loss: 93.26990509033203 Discount Log Loss 7.239201545715332 KL Loss 0.012969601899385452\n",
      "Image Log Loss: 4022.734130859375 Reward Log Loss: 93.00105285644531 Discount Log Loss 5.669333457946777 KL Loss 0.011542325839400291\n",
      "Image Log Loss: 4023.207275390625 Reward Log Loss: 93.43238067626953 Discount Log Loss 8.09240436553955 KL Loss 0.015051811002194881\n",
      "Image Log Loss: 4018.4443359375 Reward Log Loss: 92.71282958984375 Discount Log Loss 7.2371697425842285 KL Loss 0.013784250244498253\n",
      "Image Log Loss: 4020.304443359375 Reward Log Loss: 93.65118408203125 Discount Log Loss 6.93824577331543 KL Loss 0.014139954932034016\n",
      "Image Log Loss: 4022.335693359375 Reward Log Loss: 93.35696411132812 Discount Log Loss 4.3412184715271 KL Loss 0.009646125137805939\n",
      "Image Log Loss: 4024.828125 Reward Log Loss: 93.39888763427734 Discount Log Loss 10.00931453704834 KL Loss 0.020047858357429504\n",
      "Image Log Loss: 4028.66162109375 Reward Log Loss: 93.70023345947266 Discount Log Loss 7.841871738433838 KL Loss 0.018033180385828018\n",
      "Image Log Loss: 4026.139404296875 Reward Log Loss: 93.58356475830078 Discount Log Loss 4.396046161651611 KL Loss 0.016140880063176155\n",
      "Image Log Loss: 4022.947265625 Reward Log Loss: 93.28783416748047 Discount Log Loss 9.783586502075195 KL Loss 0.028733713552355766\n",
      "Image Log Loss: 4023.492431640625 Reward Log Loss: 93.19129943847656 Discount Log Loss 7.617389678955078 KL Loss 0.02704467438161373\n",
      "Image Log Loss: 4025.890380859375 Reward Log Loss: 93.29383850097656 Discount Log Loss 6.164455413818359 KL Loss 0.02466869354248047\n",
      "Image Log Loss: 4024.815673828125 Reward Log Loss: 93.39828491210938 Discount Log Loss 7.045644760131836 KL Loss 0.024777524173259735\n",
      "Image Log Loss: 4023.15869140625 Reward Log Loss: 93.2283706665039 Discount Log Loss 4.057825088500977 KL Loss 0.018723422661423683\n",
      "Image Log Loss: 4020.9892578125 Reward Log Loss: 93.11659240722656 Discount Log Loss 5.067533493041992 KL Loss 0.02356337569653988\n",
      "Image Log Loss: 4023.639404296875 Reward Log Loss: 93.45339965820312 Discount Log Loss 3.5631494522094727 KL Loss 0.01875901222229004\n",
      "Image Log Loss: 4024.824951171875 Reward Log Loss: 92.94084167480469 Discount Log Loss 9.559261322021484 KL Loss 0.042467523366212845\n",
      "Image Log Loss: 4022.744384765625 Reward Log Loss: 93.06709289550781 Discount Log Loss 5.17421817779541 KL Loss 0.028865782544016838\n",
      "Image Log Loss: 4022.887451171875 Reward Log Loss: 93.13140869140625 Discount Log Loss 9.27591323852539 KL Loss 0.059329770505428314\n",
      "Image Log Loss: 4023.894287109375 Reward Log Loss: 93.34435272216797 Discount Log Loss 8.894617080688477 KL Loss 0.07731795310974121\n",
      "Image Log Loss: 4019.67822265625 Reward Log Loss: 93.76793670654297 Discount Log Loss 5.121429443359375 KL Loss 0.06786405295133591\n",
      "Image Log Loss: 4022.568115234375 Reward Log Loss: 92.69853210449219 Discount Log Loss 6.652035713195801 KL Loss 0.07299475371837616\n",
      "Image Log Loss: 4023.784423828125 Reward Log Loss: 93.33389282226562 Discount Log Loss 6.175478935241699 KL Loss 0.09785547852516174\n",
      "Image Log Loss: 4026.192138671875 Reward Log Loss: 93.135986328125 Discount Log Loss 8.837246894836426 KL Loss 0.19202663004398346\n",
      "Image Log Loss: 4022.769287109375 Reward Log Loss: 93.056884765625 Discount Log Loss 9.982824325561523 KL Loss 0.2998020350933075\n",
      "Image Log Loss: 4024.866455078125 Reward Log Loss: 92.93576049804688 Discount Log Loss 8.575873374938965 KL Loss 0.2937525808811188\n",
      "Image Log Loss: 4017.53125 Reward Log Loss: 92.84886932373047 Discount Log Loss 6.802582263946533 KL Loss 0.3394152522087097\n",
      "Image Log Loss: 4017.840576171875 Reward Log Loss: 92.95153045654297 Discount Log Loss 3.0811712741851807 KL Loss 0.19377735257148743\n",
      "Image Log Loss: 4018.298828125 Reward Log Loss: 93.34203338623047 Discount Log Loss 5.485722064971924 KL Loss 0.36313652992248535\n",
      "Image Log Loss: 4020.0712890625 Reward Log Loss: 93.1353988647461 Discount Log Loss 9.345650672912598 KL Loss 0.7460483312606812\n",
      "Image Log Loss: 4022.9638671875 Reward Log Loss: 92.90487670898438 Discount Log Loss 6.478157997131348 KL Loss 0.5046690106391907\n",
      "Image Log Loss: 4020.841552734375 Reward Log Loss: 92.7944107055664 Discount Log Loss 5.0667548179626465 KL Loss 0.43910717964172363\n",
      "Image Log Loss: 4021.8408203125 Reward Log Loss: 93.45291137695312 Discount Log Loss 4.780425071716309 KL Loss 0.4363235533237457\n",
      "Image Log Loss: 4020.41943359375 Reward Log Loss: 93.31484985351562 Discount Log Loss 7.499941825866699 KL Loss 0.6897839307785034\n",
      "Image Log Loss: 4021.87060546875 Reward Log Loss: 93.01068115234375 Discount Log Loss 7.577746391296387 KL Loss 0.7135652899742126\n",
      "Image Log Loss: 4019.64697265625 Reward Log Loss: 93.08168029785156 Discount Log Loss 5.147174835205078 KL Loss 0.5465622544288635\n",
      "Image Log Loss: 4020.64501953125 Reward Log Loss: 93.0536880493164 Discount Log Loss 4.937769889831543 KL Loss 0.497253954410553\n",
      "Image Log Loss: 4021.094482421875 Reward Log Loss: 92.5604476928711 Discount Log Loss 5.970372200012207 KL Loss 0.6228499412536621\n",
      "Image Log Loss: 4018.675048828125 Reward Log Loss: 92.8812484741211 Discount Log Loss 8.28901481628418 KL Loss 0.879584789276123\n",
      "Image Log Loss: 4017.72802734375 Reward Log Loss: 92.585693359375 Discount Log Loss 5.363473892211914 KL Loss 0.5948858857154846\n",
      "Image Log Loss: 4018.639404296875 Reward Log Loss: 93.03013610839844 Discount Log Loss 5.964508533477783 KL Loss 0.6280516982078552\n",
      "Image Log Loss: 4019.356201171875 Reward Log Loss: 92.84870910644531 Discount Log Loss 6.369981288909912 KL Loss 0.6457745432853699\n",
      "Image Log Loss: 4021.110107421875 Reward Log Loss: 92.86543273925781 Discount Log Loss 6.826899528503418 KL Loss 0.7689407467842102\n",
      "Image Log Loss: 4018.408203125 Reward Log Loss: 92.56710815429688 Discount Log Loss 5.679646015167236 KL Loss 0.6392362713813782\n",
      "Image Log Loss: 4019.011962890625 Reward Log Loss: 92.66249084472656 Discount Log Loss 7.779889106750488 KL Loss 0.9057115912437439\n",
      "Image Log Loss: 4016.82568359375 Reward Log Loss: 92.2931137084961 Discount Log Loss 4.439692497253418 KL Loss 0.515001654624939\n",
      "Image Log Loss: 4017.820556640625 Reward Log Loss: 92.60078430175781 Discount Log Loss 5.415104866027832 KL Loss 0.6493996977806091\n",
      "Image Log Loss: 4018.511962890625 Reward Log Loss: 92.59679412841797 Discount Log Loss 7.4034504890441895 KL Loss 0.9057843089103699\n",
      "Image Log Loss: 4015.750732421875 Reward Log Loss: 92.38079833984375 Discount Log Loss 2.1842241287231445 KL Loss 0.26157906651496887\n",
      "Image Log Loss: 4015.58056640625 Reward Log Loss: 93.04761505126953 Discount Log Loss 6.019863128662109 KL Loss 0.6568232774734497\n",
      "Image Log Loss: 4022.24072265625 Reward Log Loss: 93.05928039550781 Discount Log Loss 6.962164878845215 KL Loss 0.7805188894271851\n",
      "Image Log Loss: 4015.460693359375 Reward Log Loss: 92.590576171875 Discount Log Loss 4.021013259887695 KL Loss 0.5238894820213318\n",
      "Image Log Loss: 4015.422119140625 Reward Log Loss: 92.54349517822266 Discount Log Loss 4.9175519943237305 KL Loss 0.6555988788604736\n",
      "Image Log Loss: 4010.021240234375 Reward Log Loss: 92.46277618408203 Discount Log Loss 2.0270416736602783 KL Loss 0.26685404777526855\n",
      "Image Log Loss: 4020.7705078125 Reward Log Loss: 92.68804931640625 Discount Log Loss 3.8537468910217285 KL Loss 0.5268049836158752\n",
      "Image Log Loss: 4016.341552734375 Reward Log Loss: 92.56727600097656 Discount Log Loss 4.719114303588867 KL Loss 0.660187840461731\n",
      "Image Log Loss: 4017.348388671875 Reward Log Loss: 92.42800903320312 Discount Log Loss 2.8545238971710205 KL Loss 0.3997883200645447\n",
      "Image Log Loss: 4014.969970703125 Reward Log Loss: 92.91105651855469 Discount Log Loss 6.655788421630859 KL Loss 0.7846179008483887\n",
      "Image Log Loss: 4023.50927734375 Reward Log Loss: 93.45218658447266 Discount Log Loss 5.742986679077148 KL Loss 0.6499632596969604\n",
      "Image Log Loss: 4018.693359375 Reward Log Loss: 92.34785461425781 Discount Log Loss 6.276198863983154 KL Loss 0.9126914143562317\n",
      "Image Log Loss: 4015.520263671875 Reward Log Loss: 92.45999908447266 Discount Log Loss 4.486551284790039 KL Loss 0.6512255072593689\n",
      "Image Log Loss: 4014.6025390625 Reward Log Loss: 92.90753936767578 Discount Log Loss 4.40561056137085 KL Loss 0.49549418687820435\n",
      "Image Log Loss: 4016.539306640625 Reward Log Loss: 92.69239044189453 Discount Log Loss 7.797607898712158 KL Loss 0.9993380904197693\n",
      "Image Log Loss: 4014.6494140625 Reward Log Loss: 92.69475555419922 Discount Log Loss 3.334932804107666 KL Loss 0.3511792719364166\n",
      "Image Log Loss: 4016.638427734375 Reward Log Loss: 93.40679931640625 Discount Log Loss 5.718808174133301 KL Loss 0.6120151281356812\n",
      "Image Log Loss: 4018.326171875 Reward Log Loss: 92.48102569580078 Discount Log Loss 2.873274803161621 KL Loss 0.35291945934295654\n",
      "Image Log Loss: 4018.671875 Reward Log Loss: 93.1176528930664 Discount Log Loss 11.495848655700684 KL Loss 0.36030715703964233\n",
      "Image Log Loss: 4017.836181640625 Reward Log Loss: 92.98062133789062 Discount Log Loss 5.739682674407959 KL Loss 0.5858076214790344\n",
      "Image Log Loss: 4016.9130859375 Reward Log Loss: 92.4154281616211 Discount Log Loss 4.489852428436279 KL Loss 0.5916826725006104\n",
      "Image Log Loss: 4014.652099609375 Reward Log Loss: 93.25236511230469 Discount Log Loss 2.751150131225586 KL Loss 0.3519792854785919\n",
      "Image Log Loss: 4017.96875 Reward Log Loss: 93.06428527832031 Discount Log Loss 6.423526763916016 KL Loss 0.6881371140480042\n",
      "Image Log Loss: 4017.011962890625 Reward Log Loss: 93.14502716064453 Discount Log Loss 3.691879987716675 KL Loss 0.33076781034469604\n",
      "Image Log Loss: 4020.578857421875 Reward Log Loss: 92.60620880126953 Discount Log Loss 4.545607089996338 KL Loss 0.5338590741157532\n",
      "Image Log Loss: 4016.47412109375 Reward Log Loss: 93.13314819335938 Discount Log Loss 6.085482120513916 KL Loss 0.66294264793396\n",
      "Image Log Loss: 4017.31494140625 Reward Log Loss: 92.73124694824219 Discount Log Loss 6.177232265472412 KL Loss 0.6661823987960815\n",
      "Image Log Loss: 4021.737548828125 Reward Log Loss: 92.76634979248047 Discount Log Loss 3.6448357105255127 KL Loss 0.46233469247817993\n",
      "Image Log Loss: 4019.881591796875 Reward Log Loss: 92.8816146850586 Discount Log Loss 4.149888038635254 KL Loss 0.3561781346797943\n",
      "Image Log Loss: 4016.116943359375 Reward Log Loss: 92.71939086914062 Discount Log Loss 6.005711078643799 KL Loss 0.5931020379066467\n",
      "Image Log Loss: 4018.8193359375 Reward Log Loss: 92.96205139160156 Discount Log Loss 5.902101039886475 KL Loss 0.6072633862495422\n",
      "Image Log Loss: 4017.016845703125 Reward Log Loss: 93.21183776855469 Discount Log Loss 5.398075103759766 KL Loss 0.5004990100860596\n",
      "Image Log Loss: 4014.916259765625 Reward Log Loss: 92.97988891601562 Discount Log Loss 7.3275251388549805 KL Loss 0.7527554035186768\n",
      "Image Log Loss: 4015.34375 Reward Log Loss: 92.73727416992188 Discount Log Loss 4.915571212768555 KL Loss 0.44747573137283325\n",
      "Image Log Loss: 4017.533447265625 Reward Log Loss: 93.07066345214844 Discount Log Loss 8.886375427246094 KL Loss 0.9545577764511108\n",
      "Image Log Loss: 4015.894287109375 Reward Log Loss: 92.78229522705078 Discount Log Loss 6.653777599334717 KL Loss 0.6657480597496033\n",
      "Image Log Loss: 4015.235107421875 Reward Log Loss: 92.82086181640625 Discount Log Loss 8.080101013183594 KL Loss 0.8742716908454895\n",
      "Image Log Loss: 4013.582763671875 Reward Log Loss: 93.41211700439453 Discount Log Loss 5.034822463989258 KL Loss 0.45143067836761475\n",
      "Image Log Loss: 4014.29248046875 Reward Log Loss: 93.10430908203125 Discount Log Loss 4.6053948402404785 KL Loss 0.42587193846702576\n",
      "Image Log Loss: 4012.835693359375 Reward Log Loss: 93.34772491455078 Discount Log Loss 4.741550445556641 KL Loss 0.4287448227405548\n",
      "Image Log Loss: 4016.793701171875 Reward Log Loss: 92.77037048339844 Discount Log Loss 3.583195447921753 KL Loss 0.4433499276638031\n",
      "Image Log Loss: 4013.25 Reward Log Loss: 92.69474792480469 Discount Log Loss 4.544081687927246 KL Loss 0.417061448097229\n",
      "Image Log Loss: 4014.1494140625 Reward Log Loss: 93.0873031616211 Discount Log Loss 4.899506568908691 KL Loss 0.4340941309928894\n",
      "Image Log Loss: 4015.298095703125 Reward Log Loss: 93.11068725585938 Discount Log Loss 7.0947465896606445 KL Loss 0.7462828755378723\n",
      "Image Log Loss: 4012.60498046875 Reward Log Loss: 92.73699188232422 Discount Log Loss 5.599000930786133 KL Loss 0.5376282930374146\n",
      "Image Log Loss: 4012.27783203125 Reward Log Loss: 92.82783508300781 Discount Log Loss 4.072702884674072 KL Loss 0.3279320001602173\n",
      "Image Log Loss: 4012.521484375 Reward Log Loss: 92.79032897949219 Discount Log Loss 5.2883524894714355 KL Loss 0.5206637382507324\n",
      "Image Log Loss: 4010.661865234375 Reward Log Loss: 92.83615112304688 Discount Log Loss 7.847079277038574 KL Loss 0.8500543236732483\n",
      "Image Log Loss: 4013.639404296875 Reward Log Loss: 93.27499389648438 Discount Log Loss 7.0481276512146 KL Loss 0.7411033511161804\n",
      "Image Log Loss: 4014.522705078125 Reward Log Loss: 92.6149673461914 Discount Log Loss 5.263033866882324 KL Loss 0.5165398120880127\n",
      "Image Log Loss: 4012.8857421875 Reward Log Loss: 92.54788208007812 Discount Log Loss 4.455366134643555 KL Loss 0.40784958004951477\n",
      "Image Log Loss: 4018.919921875 Reward Log Loss: 93.25770568847656 Discount Log Loss 5.690822601318359 KL Loss 0.541962206363678\n",
      "Image Log Loss: 4011.720703125 Reward Log Loss: 93.04642486572266 Discount Log Loss 7.271397113800049 KL Loss 0.752926766872406\n",
      "Image Log Loss: 4016.6923828125 Reward Log Loss: 93.33942413330078 Discount Log Loss 4.8788042068481445 KL Loss 0.4320927858352661\n",
      "Image Log Loss: 4015.207275390625 Reward Log Loss: 92.95294189453125 Discount Log Loss 3.623032331466675 KL Loss 0.2960691750049591\n",
      "Image Log Loss: 4014.2109375 Reward Log Loss: 92.64896392822266 Discount Log Loss 4.411486625671387 KL Loss 0.4010770320892334\n",
      "Image Log Loss: 4013.17529296875 Reward Log Loss: 92.597900390625 Discount Log Loss 5.983906269073486 KL Loss 0.6149817705154419\n",
      "Image Log Loss: 4010.155517578125 Reward Log Loss: 93.0703125 Discount Log Loss 5.651771068572998 KL Loss 0.5354680418968201\n",
      "Image Log Loss: 4012.060546875 Reward Log Loss: 92.71749877929688 Discount Log Loss 5.64553689956665 KL Loss 0.532957136631012\n",
      "Image Log Loss: 4012.899169921875 Reward Log Loss: 93.17389678955078 Discount Log Loss 5.358163833618164 KL Loss 0.5115785598754883\n",
      "Image Log Loss: 4015.529296875 Reward Log Loss: 92.6481704711914 Discount Log Loss 5.785250186920166 KL Loss 0.7413280606269836\n",
      "Image Log Loss: 4013.81884765625 Reward Log Loss: 92.78450012207031 Discount Log Loss 6.357796669006348 KL Loss 0.6294882893562317\n",
      "Image Log Loss: 4011.1533203125 Reward Log Loss: 93.01477813720703 Discount Log Loss 5.624410629272461 KL Loss 0.5294181704521179\n",
      "Image Log Loss: 4010.844482421875 Reward Log Loss: 93.04430389404297 Discount Log Loss 8.621088981628418 KL Loss 0.9357926845550537\n",
      "Image Log Loss: 4012.5224609375 Reward Log Loss: 93.20791625976562 Discount Log Loss 4.758474349975586 KL Loss 0.4165107011795044\n",
      "Image Log Loss: 4012.097412109375 Reward Log Loss: 92.82544708251953 Discount Log Loss 8.599112510681152 KL Loss 0.9309214949607849\n",
      "Image Log Loss: 4013.343017578125 Reward Log Loss: 93.1210708618164 Discount Log Loss 5.445560455322266 KL Loss 0.5115044116973877\n",
      "Image Log Loss: 4011.20947265625 Reward Log Loss: 93.1542739868164 Discount Log Loss 5.437131881713867 KL Loss 0.5112554430961609\n",
      "Image Log Loss: 4012.5078125 Reward Log Loss: 92.89167022705078 Discount Log Loss 5.430431365966797 KL Loss 0.510852038860321\n",
      "Image Log Loss: 4013.2060546875 Reward Log Loss: 92.89774322509766 Discount Log Loss 7.089137554168701 KL Loss 0.722653329372406\n",
      "Image Log Loss: 4016.189453125 Reward Log Loss: 92.74633026123047 Discount Log Loss 6.3004469871521 KL Loss 0.6189215779304504\n",
      "Image Log Loss: 4014.63916015625 Reward Log Loss: 92.7533950805664 Discount Log Loss 4.795072555541992 KL Loss 0.4153430163860321\n",
      "Image Log Loss: 4010.1611328125 Reward Log Loss: 92.87088012695312 Discount Log Loss 8.542726516723633 KL Loss 0.9192947149276733\n",
      "Image Log Loss: 4011.945556640625 Reward Log Loss: 92.62921905517578 Discount Log Loss 4.718321323394775 KL Loss 0.4088563919067383\n",
      "Image Log Loss: 4016.49365234375 Reward Log Loss: 93.09515380859375 Discount Log Loss 6.2762770652771 KL Loss 0.6142771244049072\n",
      "Image Log Loss: 4015.631591796875 Reward Log Loss: 93.10417175292969 Discount Log Loss 3.7069296836853027 KL Loss 0.2910030782222748\n",
      "Image Log Loss: 4012.7548828125 Reward Log Loss: 92.73147583007812 Discount Log Loss 5.056936264038086 KL Loss 0.4873959422111511\n",
      "Image Log Loss: 4010.7109375 Reward Log Loss: 92.8257827758789 Discount Log Loss 5.487646102905273 KL Loss 0.5091773867607117\n",
      "Image Log Loss: 4012.799072265625 Reward Log Loss: 93.01238250732422 Discount Log Loss 6.320981979370117 KL Loss 0.6169746518135071\n",
      "Image Log Loss: 4008.875732421875 Reward Log Loss: 92.5506820678711 Discount Log Loss 5.039953231811523 KL Loss 0.48565196990966797\n",
      "Image Log Loss: 4009.828857421875 Reward Log Loss: 92.42278289794922 Discount Log Loss 7.1598944664001465 KL Loss 0.9222004413604736\n",
      "Image Log Loss: 4010.88427734375 Reward Log Loss: 93.11384582519531 Discount Log Loss 3.8239290714263916 KL Loss 0.2960754334926605\n",
      "Image Log Loss: 4012.706298828125 Reward Log Loss: 92.800537109375 Discount Log Loss 8.118402481079102 KL Loss 0.8918201923370361\n",
      "Image Log Loss: 4010.108642578125 Reward Log Loss: 93.02043914794922 Discount Log Loss 4.453393459320068 KL Loss 0.38940203189849854\n",
      "Image Log Loss: 4014.2919921875 Reward Log Loss: 93.00991821289062 Discount Log Loss 4.589001655578613 KL Loss 0.3959939181804657\n",
      "Image Log Loss: 4011.0537109375 Reward Log Loss: 93.10511779785156 Discount Log Loss 5.986335277557373 KL Loss 0.5918490886688232\n",
      "Image Log Loss: 4009.304443359375 Reward Log Loss: 92.65837097167969 Discount Log Loss 4.988819599151611 KL Loss 0.4818410873413086\n",
      "Image Log Loss: 4022.6806640625 Reward Log Loss: 92.79409790039062 Discount Log Loss 4.743188858032227 KL Loss 0.4075731635093689\n",
      "Image Log Loss: 4019.583740234375 Reward Log Loss: 92.91710662841797 Discount Log Loss 4.428102970123291 KL Loss 0.3871947228908539\n",
      "Image Log Loss: 4012.491943359375 Reward Log Loss: 92.64968872070312 Discount Log Loss 6.210740566253662 KL Loss 0.603428065776825\n",
      "Image Log Loss: 4010.6767578125 Reward Log Loss: 92.44282531738281 Discount Log Loss 5.536673069000244 KL Loss 0.7105082869529724\n",
      "Image Log Loss: 4009.215576171875 Reward Log Loss: 92.72614288330078 Discount Log Loss 7.809965133666992 KL Loss 0.8112350702285767\n",
      "Image Log Loss: 4010.7861328125 Reward Log Loss: 92.68511962890625 Discount Log Loss 4.947478771209717 KL Loss 0.478778600692749\n",
      "Image Log Loss: 4010.696044921875 Reward Log Loss: 93.01144409179688 Discount Log Loss 6.9638776779174805 KL Loss 0.7028034925460815\n",
      "Image Log Loss: 4017.04443359375 Reward Log Loss: 92.9731216430664 Discount Log Loss 5.7054853439331055 KL Loss 0.5784192085266113\n",
      "Image Log Loss: 4015.26123046875 Reward Log Loss: 92.70327758789062 Discount Log Loss 5.41938591003418 KL Loss 0.49945351481437683\n",
      "Image Log Loss: 4010.3056640625 Reward Log Loss: 93.10930633544922 Discount Log Loss 6.253877639770508 KL Loss 0.6057884097099304\n",
      "Image Log Loss: 4010.39501953125 Reward Log Loss: 92.75057983398438 Discount Log Loss 5.162985801696777 KL Loss 0.48466262221336365\n",
      "Image Log Loss: 4013.5009765625 Reward Log Loss: 92.85163116455078 Discount Log Loss 6.180045127868652 KL Loss 0.5983263254165649\n",
      "Image Log Loss: 4009.001220703125 Reward Log Loss: 93.35208129882812 Discount Log Loss 7.010426044464111 KL Loss 0.7038772106170654\n",
      "Image Log Loss: 4013.90771484375 Reward Log Loss: 92.62528228759766 Discount Log Loss 5.156172275543213 KL Loss 0.48306596279144287\n",
      "Image Log Loss: 4010.7744140625 Reward Log Loss: 92.79300689697266 Discount Log Loss 6.16800594329834 KL Loss 0.5963857769966125\n",
      "Image Log Loss: 4015.159912109375 Reward Log Loss: 93.04115295410156 Discount Log Loss 5.912468910217285 KL Loss 0.5830176472663879\n",
      "Image Log Loss: 4007.94873046875 Reward Log Loss: 93.02906036376953 Discount Log Loss 6.231121063232422 KL Loss 0.6015774607658386\n",
      "Image Log Loss: 4013.24462890625 Reward Log Loss: 93.08042907714844 Discount Log Loss 5.395223617553711 KL Loss 0.49477386474609375\n",
      "Image Log Loss: 4010.096923828125 Reward Log Loss: 92.7886734008789 Discount Log Loss 5.29306173324585 KL Loss 0.48853516578674316\n",
      "Image Log Loss: 4012.412841796875 Reward Log Loss: 92.9139633178711 Discount Log Loss 3.6102139949798584 KL Loss 0.2814563810825348\n",
      "Image Log Loss: 4009.3212890625 Reward Log Loss: 92.8213119506836 Discount Log Loss 6.9772844314575195 KL Loss 0.6984418630599976\n",
      "Image Log Loss: 4010.738525390625 Reward Log Loss: 92.7013168334961 Discount Log Loss 5.1232500076293945 KL Loss 0.4798271059989929\n",
      "Image Log Loss: 4011.5048828125 Reward Log Loss: 92.4999771118164 Discount Log Loss 3.908133029937744 KL Loss 0.49867358803749084\n",
      "Image Log Loss: 4011.8173828125 Reward Log Loss: 92.67733764648438 Discount Log Loss 3.3359534740448 KL Loss 0.2723158001899719\n",
      "Image Log Loss: 4012.87939453125 Reward Log Loss: 92.73175811767578 Discount Log Loss 5.37922477722168 KL Loss 0.49228137731552124\n",
      "Image Log Loss: 4014.242431640625 Reward Log Loss: 92.88725280761719 Discount Log Loss 4.687380790710449 KL Loss 0.39841610193252563\n",
      "Image Log Loss: 4012.297607421875 Reward Log Loss: 92.80150604248047 Discount Log Loss 5.104752063751221 KL Loss 0.47734010219573975\n",
      "Image Log Loss: 4013.40283203125 Reward Log Loss: 93.19683837890625 Discount Log Loss 5.101091384887695 KL Loss 0.4767172336578369\n",
      "Image Log Loss: 4010.987548828125 Reward Log Loss: 92.68234252929688 Discount Log Loss 6.778643608093262 KL Loss 0.6813642382621765\n",
      "Image Log Loss: 4009.4912109375 Reward Log Loss: 93.1439437866211 Discount Log Loss 6.878857612609863 KL Loss 0.6871999502182007\n",
      "Image Log Loss: 4007.261474609375 Reward Log Loss: 92.76716613769531 Discount Log Loss 6.012363433837891 KL Loss 0.5816452503204346\n",
      "Image Log Loss: 4008.16259765625 Reward Log Loss: 92.87032318115234 Discount Log Loss 6.946331977844238 KL Loss 0.6916948556900024\n",
      "Image Log Loss: 4008.115234375 Reward Log Loss: 92.874267578125 Discount Log Loss 5.253786563873291 KL Loss 0.48193293809890747\n",
      "Image Log Loss: 4008.972900390625 Reward Log Loss: 92.70716857910156 Discount Log Loss 5.551188945770264 KL Loss 0.5639981031417847\n",
      "Image Log Loss: 4010.72998046875 Reward Log Loss: 92.83240509033203 Discount Log Loss 6.1128716468811035 KL Loss 0.5858271718025208\n",
      "Image Log Loss: 4010.33251953125 Reward Log Loss: 92.77950286865234 Discount Log Loss 5.355566024780273 KL Loss 0.48703309893608093\n",
      "Image Log Loss: 4009.503662109375 Reward Log Loss: 92.85631561279297 Discount Log Loss 7.504704475402832 KL Loss 0.7745077610015869\n",
      "Image Log Loss: 4010.844482421875 Reward Log Loss: 92.79558563232422 Discount Log Loss 7.500467300415039 KL Loss 0.7736159563064575\n",
      "Image Log Loss: 4011.04833984375 Reward Log Loss: 93.0468521118164 Discount Log Loss 7.321615219116211 KL Loss 0.7651268839836121\n",
      "Image Log Loss: 4012.26904296875 Reward Log Loss: 92.78166961669922 Discount Log Loss 5.988973617553711 KL Loss 0.5757904648780823\n",
      "Image Log Loss: 4014.17431640625 Reward Log Loss: 92.71356201171875 Discount Log Loss 6.094733715057373 KL Loss 0.5816996097564697\n",
      "Image Log Loss: 4009.388671875 Reward Log Loss: 92.96631622314453 Discount Log Loss 5.2336812019348145 KL Loss 0.4779340624809265\n",
      "Image Log Loss: 4013.9912109375 Reward Log Loss: 92.9032211303711 Discount Log Loss 4.480996608734131 KL Loss 0.37985795736312866\n",
      "Image Log Loss: 4011.233642578125 Reward Log Loss: 92.60380554199219 Discount Log Loss 3.995973825454712 KL Loss 0.36338144540786743\n",
      "Image Log Loss: 4013.7470703125 Reward Log Loss: 92.65360260009766 Discount Log Loss 6.544316291809082 KL Loss 0.6621898412704468\n",
      "Image Log Loss: 4008.98681640625 Reward Log Loss: 92.60295104980469 Discount Log Loss 6.9816436767578125 KL Loss 0.750241219997406\n",
      "Image Log Loss: 4010.8701171875 Reward Log Loss: 92.98165893554688 Discount Log Loss 5.332942008972168 KL Loss 0.48210930824279785\n",
      "Image Log Loss: 4011.132568359375 Reward Log Loss: 92.6416244506836 Discount Log Loss 4.584770679473877 KL Loss 0.38461172580718994\n",
      "Image Log Loss: 4011.091552734375 Reward Log Loss: 93.10448455810547 Discount Log Loss 5.029603481292725 KL Loss 0.467056006193161\n",
      "Image Log Loss: 4009.108642578125 Reward Log Loss: 92.70626831054688 Discount Log Loss 6.151545524597168 KL Loss 0.5829484462738037\n",
      "Image Log Loss: 4012.030029296875 Reward Log Loss: 92.63394165039062 Discount Log Loss 5.447977066040039 KL Loss 0.5532965064048767\n",
      "Image Log Loss: 4011.340576171875 Reward Log Loss: 92.9705581665039 Discount Log Loss 4.269679069519043 KL Loss 0.368958443403244\n",
      "Image Log Loss: 4011.07470703125 Reward Log Loss: 92.85376739501953 Discount Log Loss 5.01295280456543 KL Loss 0.46465134620666504\n",
      "Image Log Loss: 4010.556884765625 Reward Log Loss: 92.71263885498047 Discount Log Loss 5.008843421936035 KL Loss 0.4644545614719391\n",
      "Image Log Loss: 4008.711181640625 Reward Log Loss: 92.97948455810547 Discount Log Loss 7.245187759399414 KL Loss 0.7518723011016846\n",
      "Image Log Loss: 4009.0830078125 Reward Log Loss: 93.09150695800781 Discount Log Loss 6.816248893737793 KL Loss 0.6697009801864624\n",
      "Image Log Loss: 4012.578125 Reward Log Loss: 92.79423522949219 Discount Log Loss 6.8128981590271 KL Loss 0.6692826151847839\n",
      "Image Log Loss: 4008.613037109375 Reward Log Loss: 92.78023529052734 Discount Log Loss 7.558250904083252 KL Loss 0.7643746137619019\n",
      "Image Log Loss: 4010.170654296875 Reward Log Loss: 92.9126968383789 Discount Log Loss 3.4915597438812256 KL Loss 0.2695392370223999\n",
      "Image Log Loss: 4011.66552734375 Reward Log Loss: 92.6875 Discount Log Loss 5.36570930480957 KL Loss 0.5473159551620483\n",
      "Image Log Loss: 4009.748779296875 Reward Log Loss: 92.72747802734375 Discount Log Loss 5.932878494262695 KL Loss 0.5631531476974487\n",
      "Image Log Loss: 4008.561767578125 Reward Log Loss: 93.00907135009766 Discount Log Loss 2.9455904960632324 KL Loss 0.1808658093214035\n",
      "Image Log Loss: 4010.80712890625 Reward Log Loss: 93.03802490234375 Discount Log Loss 6.05496072769165 KL Loss 0.569271445274353\n",
      "Image Log Loss: 4014.3525390625 Reward Log Loss: 92.98670959472656 Discount Log Loss 4.559724807739258 KL Loss 0.37799662351608276\n",
      "Image Log Loss: 4009.767822265625 Reward Log Loss: 93.17762756347656 Discount Log Loss 6.668900012969971 KL Loss 0.6564772725105286\n",
      "Image Log Loss: 4010.99755859375 Reward Log Loss: 92.6155014038086 Discount Log Loss 6.794991970062256 KL Loss 0.6620427966117859\n",
      "Image Log Loss: 4011.7060546875 Reward Log Loss: 92.66654205322266 Discount Log Loss 3.806999444961548 KL Loss 0.3515664339065552\n",
      "Image Log Loss: 4011.786865234375 Reward Log Loss: 92.66144561767578 Discount Log Loss 5.286482334136963 KL Loss 0.5409746766090393\n",
      "Image Log Loss: 4007.496826171875 Reward Log Loss: 92.63150024414062 Discount Log Loss 6.0293288230896 KL Loss 0.761829674243927\n",
      "Image Log Loss: 4011.726318359375 Reward Log Loss: 92.65718841552734 Discount Log Loss 5.379766464233398 KL Loss 0.4760720729827881\n",
      "Image Log Loss: 4010.67333984375 Reward Log Loss: 92.75948333740234 Discount Log Loss 4.1778340339660645 KL Loss 0.360218346118927\n",
      "Image Log Loss: 4015.24365234375 Reward Log Loss: 93.14328002929688 Discount Log Loss 5.157139778137207 KL Loss 0.46250998973846436\n",
      "Image Log Loss: 4009.13427734375 Reward Log Loss: 92.85773468017578 Discount Log Loss 6.782869815826416 KL Loss 0.6580823659896851\n",
      "Image Log Loss: 4011.4853515625 Reward Log Loss: 92.78958129882812 Discount Log Loss 5.291018009185791 KL Loss 0.46742546558380127\n",
      "Image Log Loss: 4009.29345703125 Reward Log Loss: 92.84680938720703 Discount Log Loss 7.132954120635986 KL Loss 0.735751748085022\n",
      "Image Log Loss: 4010.06396484375 Reward Log Loss: 92.99771118164062 Discount Log Loss 4.448328495025635 KL Loss 0.44118040800094604\n",
      "Image Log Loss: 4010.17431640625 Reward Log Loss: 92.9406967163086 Discount Log Loss 6.029173374176025 KL Loss 0.5605112314224243\n",
      "Image Log Loss: 4011.703857421875 Reward Log Loss: 92.70640563964844 Discount Log Loss 5.283383846282959 KL Loss 0.4654805660247803\n",
      "Image Log Loss: 4012.91064453125 Reward Log Loss: 92.73469543457031 Discount Log Loss 2.922645330429077 KL Loss 0.25168097019195557\n",
      "Image Log Loss: 4009.343017578125 Reward Log Loss: 92.98157501220703 Discount Log Loss 5.367559909820557 KL Loss 0.4716600179672241\n",
      "Image Log Loss: 4014.04443359375 Reward Log Loss: 92.8825454711914 Discount Log Loss 3.02567195892334 KL Loss 0.37699636816978455\n",
      "Image Log Loss: 4006.373046875 Reward Log Loss: 92.76814270019531 Discount Log Loss 7.506430149078369 KL Loss 0.7447224259376526\n",
      "Image Log Loss: 4009.6396484375 Reward Log Loss: 92.79190063476562 Discount Log Loss 5.843226909637451 KL Loss 0.6238192319869995\n",
      "Image Log Loss: 4009.0947265625 Reward Log Loss: 92.9370346069336 Discount Log Loss 6.848839282989502 KL Loss 0.6573807597160339\n",
      "Image Log Loss: 4010.498779296875 Reward Log Loss: 93.05862426757812 Discount Log Loss 4.531442165374756 KL Loss 0.36884021759033203\n",
      "Image Log Loss: 4011.318359375 Reward Log Loss: 92.73137664794922 Discount Log Loss 5.801509857177734 KL Loss 0.6214889883995056\n",
      "Image Log Loss: 4005.0576171875 Reward Log Loss: 92.927490234375 Discount Log Loss 4.371129989624023 KL Loss 0.3607925772666931\n",
      "Image Log Loss: 4008.173828125 Reward Log Loss: 92.8467788696289 Discount Log Loss 7.334211349487305 KL Loss 0.7320534586906433\n",
      "Image Log Loss: 4008.573486328125 Reward Log Loss: 92.65785217285156 Discount Log Loss 5.848475933074951 KL Loss 0.5458625555038452\n",
      "Image Log Loss: 4011.1455078125 Reward Log Loss: 93.11752319335938 Discount Log Loss 4.361915588378906 KL Loss 0.35983648896217346\n",
      "Image Log Loss: 4009.6884765625 Reward Log Loss: 92.78289031982422 Discount Log Loss 5.263799667358398 KL Loss 0.4590839743614197\n",
      "Image Log Loss: 4007.277587890625 Reward Log Loss: 92.6832046508789 Discount Log Loss 6.003608226776123 KL Loss 0.5511171221733093\n",
      "Image Log Loss: 4011.122802734375 Reward Log Loss: 92.58892822265625 Discount Log Loss 4.228540420532227 KL Loss 0.4297002851963043\n",
      "Image Log Loss: 4006.488037109375 Reward Log Loss: 92.3729019165039 Discount Log Loss 2.266756534576416 KL Loss 0.2795637249946594\n",
      "Image Log Loss: 4006.480712890625 Reward Log Loss: 93.0583724975586 Discount Log Loss 4.614039421081543 KL Loss 0.3714058995246887\n",
      "Image Log Loss: 4008.703369140625 Reward Log Loss: 93.71019744873047 Discount Log Loss 6.408787727355957 KL Loss 0.7053912281990051\n",
      "Image Log Loss: 4013.081298828125 Reward Log Loss: 92.84314727783203 Discount Log Loss 4.344183444976807 KL Loss 0.35572704672813416\n",
      "Image Log Loss: 4008.74072265625 Reward Log Loss: 92.55054473876953 Discount Log Loss 4.5144195556640625 KL Loss 0.3631289303302765\n",
      "Image Log Loss: 4007.675537109375 Reward Log Loss: 92.86229705810547 Discount Log Loss 5.816185474395752 KL Loss 0.5395998358726501\n",
      "Image Log Loss: 4012.140869140625 Reward Log Loss: 93.08814239501953 Discount Log Loss 7.692005634307861 KL Loss 0.8030521869659424\n",
      "Image Log Loss: 4006.62744140625 Reward Log Loss: 92.79802703857422 Discount Log Loss 5.809473991394043 KL Loss 0.537651002407074\n",
      "Image Log Loss: 4009.716796875 Reward Log Loss: 93.51520538330078 Discount Log Loss 4.326982498168945 KL Loss 0.3539367914199829\n",
      "Image Log Loss: 4014.18994140625 Reward Log Loss: 93.43579864501953 Discount Log Loss 2.846328020095825 KL Loss 0.1703827679157257\n",
      "Image Log Loss: 4009.48876953125 Reward Log Loss: 92.78414916992188 Discount Log Loss 3.9596445560455322 KL Loss 0.34327763319015503\n",
      "Image Log Loss: 4006.97802734375 Reward Log Loss: 93.03244018554688 Discount Log Loss 6.826047897338867 KL Loss 0.6421899199485779\n",
      "Image Log Loss: 4008.1142578125 Reward Log Loss: 92.75384521484375 Discount Log Loss 4.0293474197387695 KL Loss 0.4213518500328064\n",
      "Image Log Loss: 4009.697509765625 Reward Log Loss: 92.80105590820312 Discount Log Loss 4.669215679168701 KL Loss 0.4326239824295044\n",
      "Image Log Loss: 4007.096923828125 Reward Log Loss: 92.77763366699219 Discount Log Loss 5.240467548370361 KL Loss 0.4499010443687439\n",
      "Image Log Loss: 4006.824462890625 Reward Log Loss: 93.20263671875 Discount Log Loss 4.499122619628906 KL Loss 0.3589724004268646\n",
      "Image Log Loss: 4005.52490234375 Reward Log Loss: 92.98843383789062 Discount Log Loss 4.634570121765137 KL Loss 0.4298457205295563\n",
      "Image Log Loss: 4012.833740234375 Reward Log Loss: 92.48765563964844 Discount Log Loss 2.9993255138397217 KL Loss 0.36505115032196045\n",
      "Image Log Loss: 4006.686767578125 Reward Log Loss: 92.90058898925781 Discount Log Loss 5.344945907592773 KL Loss 0.4544515609741211\n",
      "Image Log Loss: 4006.851806640625 Reward Log Loss: 92.85586547851562 Discount Log Loss 6.505849838256836 KL Loss 0.6208845376968384\n",
      "Image Log Loss: 4007.638671875 Reward Log Loss: 93.0169906616211 Discount Log Loss 7.566977500915527 KL Loss 0.7253369092941284\n",
      "Image Log Loss: 4016.164306640625 Reward Log Loss: 93.05534362792969 Discount Log Loss 4.583672046661377 KL Loss 0.4274460971355438\n",
      "Image Log Loss: 4011.818115234375 Reward Log Loss: 92.96131896972656 Discount Log Loss 5.0137763023376465 KL Loss 0.4369163513183594\n",
      "Image Log Loss: 4004.101318359375 Reward Log Loss: 92.94096374511719 Discount Log Loss 3.862089157104492 KL Loss 0.2722497582435608\n",
      "Image Log Loss: 4008.750244140625 Reward Log Loss: 92.98961639404297 Discount Log Loss 4.26645565032959 KL Loss 0.345833957195282\n",
      "Image Log Loss: 4012.52685546875 Reward Log Loss: 92.9473648071289 Discount Log Loss 5.226803779602051 KL Loss 0.4436814486980438\n",
      "Image Log Loss: 4008.472412109375 Reward Log Loss: 93.12954711914062 Discount Log Loss 6.083204746246338 KL Loss 0.540915846824646\n",
      "Image Log Loss: 4014.66943359375 Reward Log Loss: 92.31730651855469 Discount Log Loss 1.5134142637252808 KL Loss 0.18143488466739655\n",
      "Image Log Loss: 4011.259765625 Reward Log Loss: 93.17943572998047 Discount Log Loss 5.3415327072143555 KL Loss 0.4497767984867096\n",
      "Image Log Loss: 4008.450927734375 Reward Log Loss: 92.8984375 Discount Log Loss 3.787766695022583 KL Loss 0.4081425368785858\n",
      "Image Log Loss: 4009.6826171875 Reward Log Loss: 92.49995422363281 Discount Log Loss 4.514712810516357 KL Loss 0.49814826250076294\n",
      "Image Log Loss: 4007.478759765625 Reward Log Loss: 92.80046844482422 Discount Log Loss 3.50551700592041 KL Loss 0.25379669666290283\n",
      "Image Log Loss: 4009.806884765625 Reward Log Loss: 92.84343719482422 Discount Log Loss 5.338910102844238 KL Loss 0.4481622278690338\n",
      "Image Log Loss: 4008.879150390625 Reward Log Loss: 92.58482360839844 Discount Log Loss 3.728415012359619 KL Loss 0.40550026297569275\n",
      "Image Log Loss: 4007.761962890625 Reward Log Loss: 92.77767944335938 Discount Log Loss 6.078631401062012 KL Loss 0.5357252359390259\n",
      "Image Log Loss: 4010.048095703125 Reward Log Loss: 92.3178482055664 Discount Log Loss 4.43691873550415 KL Loss 0.4927334487438202\n",
      "Image Log Loss: 4013.74072265625 Reward Log Loss: 92.40886688232422 Discount Log Loss 2.9380266666412354 KL Loss 0.3140298128128052\n",
      "Image Log Loss: 4012.4697265625 Reward Log Loss: 93.10066223144531 Discount Log Loss 4.471385955810547 KL Loss 0.3487597405910492\n",
      "Image Log Loss: 4015.246337890625 Reward Log Loss: 92.63308715820312 Discount Log Loss 5.949018478393555 KL Loss 0.5251820087432861\n",
      "Image Log Loss: 4006.4619140625 Reward Log Loss: 92.7099838256836 Discount Log Loss 4.596015453338623 KL Loss 0.3555372357368469\n",
      "Image Log Loss: 4005.1279296875 Reward Log Loss: 92.45419311523438 Discount Log Loss 4.942534923553467 KL Loss 0.42699572443962097\n",
      "Image Log Loss: 4013.530029296875 Reward Log Loss: 93.4393539428711 Discount Log Loss 3.6206536293029785 KL Loss 0.32601383328437805\n",
      "Image Log Loss: 4009.2587890625 Reward Log Loss: 93.14349365234375 Discount Log Loss 7.5531158447265625 KL Loss 0.7070639729499817\n",
      "Image Log Loss: 4012.247802734375 Reward Log Loss: 92.61404418945312 Discount Log Loss 5.0366411209106445 KL Loss 0.5739529132843018\n",
      "Image Log Loss: 4008.40087890625 Reward Log Loss: 92.87359619140625 Discount Log Loss 5.050670623779297 KL Loss 0.4993749260902405\n",
      "Image Log Loss: 4006.501220703125 Reward Log Loss: 93.34070587158203 Discount Log Loss 2.981036901473999 KL Loss 0.1696966588497162\n",
      "Image Log Loss: 4008.94873046875 Reward Log Loss: 92.78270721435547 Discount Log Loss 9.028666496276855 KL Loss 0.8789933919906616\n",
      "Image Log Loss: 4007.84814453125 Reward Log Loss: 93.01021575927734 Discount Log Loss 6.380493640899658 KL Loss 0.5972015857696533\n",
      "Image Log Loss: 4006.066162109375 Reward Log Loss: 92.76183319091797 Discount Log Loss 6.808845043182373 KL Loss 0.6146116852760315\n",
      "Image Log Loss: 4008.450927734375 Reward Log Loss: 92.75344848632812 Discount Log Loss 5.330137729644775 KL Loss 0.4394265413284302\n",
      "Image Log Loss: 4007.463134765625 Reward Log Loss: 92.60739135742188 Discount Log Loss 6.363518238067627 KL Loss 0.5949282050132751\n",
      "Image Log Loss: 4007.27978515625 Reward Log Loss: 92.7551498413086 Discount Log Loss 7.095868110656738 KL Loss 0.6806209087371826\n",
      "Image Log Loss: 4007.75244140625 Reward Log Loss: 92.68091583251953 Discount Log Loss 5.6148505210876465 KL Loss 0.5057891607284546\n",
      "Image Log Loss: 4014.322265625 Reward Log Loss: 92.60365295410156 Discount Log Loss 1.9242119789123535 KL Loss 0.2137422412633896\n",
      "Image Log Loss: 4009.818359375 Reward Log Loss: 92.50331115722656 Discount Log Loss 2.970327377319336 KL Loss 0.3485412895679474\n",
      "Image Log Loss: 4009.453125 Reward Log Loss: 92.55117797851562 Discount Log Loss 4.10520076751709 KL Loss 0.47283685207366943\n",
      "Image Log Loss: 4014.6083984375 Reward Log Loss: 93.32935333251953 Discount Log Loss 3.386294364929199 KL Loss 0.31534773111343384\n",
      "Image Log Loss: 4009.9013671875 Reward Log Loss: 92.74549102783203 Discount Log Loss 1.8630731105804443 KL Loss 0.2112594097852707\n",
      "Image Log Loss: 4011.95556640625 Reward Log Loss: 92.9452133178711 Discount Log Loss 4.104420185089111 KL Loss 0.3280711770057678\n",
      "Image Log Loss: 4009.701904296875 Reward Log Loss: 92.34439086914062 Discount Log Loss 4.033607482910156 KL Loss 0.46680372953414917\n",
      "Image Log Loss: 4005.844970703125 Reward Log Loss: 92.72930908203125 Discount Log Loss 6.057565689086914 KL Loss 0.5172202587127686\n",
      "Image Log Loss: 4008.01123046875 Reward Log Loss: 92.84600830078125 Discount Log Loss 5.549803256988525 KL Loss 0.49807921051979065\n",
      "Image Log Loss: 4004.25927734375 Reward Log Loss: 92.91869354248047 Discount Log Loss 5.459435939788818 KL Loss 0.5696240067481995\n",
      "Image Log Loss: 4012.2529296875 Reward Log Loss: 92.69033813476562 Discount Log Loss 5.898841381072998 KL Loss 0.5069395899772644\n",
      "Image Log Loss: 4006.56494140625 Reward Log Loss: 92.6543197631836 Discount Log Loss 3.946823835372925 KL Loss 0.4600679576396942\n",
      "Image Log Loss: 4004.558837890625 Reward Log Loss: 93.3231430053711 Discount Log Loss 4.044231414794922 KL Loss 0.3231978714466095\n",
      "Image Log Loss: 4009.239990234375 Reward Log Loss: 92.41635131835938 Discount Log Loss 3.1802120208740234 KL Loss 0.3719022870063782\n",
      "Image Log Loss: 4008.83251953125 Reward Log Loss: 93.32424926757812 Discount Log Loss 5.888781547546387 KL Loss 0.5037633776664734\n",
      "Image Log Loss: 4009.440673828125 Reward Log Loss: 92.75100708007812 Discount Log Loss 3.113607168197632 KL Loss 0.306052029132843\n",
      "Image Log Loss: 4004.982177734375 Reward Log Loss: 92.8565444946289 Discount Log Loss 6.205162525177002 KL Loss 0.5757747888565063\n",
      "Image Log Loss: 4004.603515625 Reward Log Loss: 92.45281219482422 Discount Log Loss 0.9202225208282471 KL Loss 0.11097131669521332\n",
      "Image Log Loss: 4008.333740234375 Reward Log Loss: 92.8297348022461 Discount Log Loss 3.6841180324554443 KL Loss 0.4257529675960541\n",
      "Image Log Loss: 4010.4951171875 Reward Log Loss: 92.5556869506836 Discount Log Loss 2.3618149757385254 KL Loss 0.2781482934951782\n",
      "Image Log Loss: 4007.251220703125 Reward Log Loss: 93.09843444824219 Discount Log Loss 5.190279483795166 KL Loss 0.5546364188194275\n",
      "Image Log Loss: 4011.32373046875 Reward Log Loss: 93.6195297241211 Discount Log Loss 6.044629096984863 KL Loss 0.5077062845230103\n",
      "Image Log Loss: 4006.039306640625 Reward Log Loss: 92.6674575805664 Discount Log Loss 5.311193466186523 KL Loss 0.42350059747695923\n",
      "Image Log Loss: 4012.248046875 Reward Log Loss: 92.63494110107422 Discount Log Loss 6.041783332824707 KL Loss 0.5077667236328125\n",
      "Image Log Loss: 4002.875732421875 Reward Log Loss: 92.4404067993164 Discount Log Loss 4.4987263679504395 KL Loss 0.5243404507637024\n",
      "Image Log Loss: 4005.822509765625 Reward Log Loss: 92.77904510498047 Discount Log Loss 5.307724475860596 KL Loss 0.4209313988685608\n",
      "Image Log Loss: 4009.271240234375 Reward Log Loss: 92.95557403564453 Discount Log Loss 5.856613636016846 KL Loss 0.49403876066207886\n",
      "Image Log Loss: 4008.769287109375 Reward Log Loss: 92.69035339355469 Discount Log Loss 6.7634501457214355 KL Loss 0.588076114654541\n",
      "Image Log Loss: 4007.64306640625 Reward Log Loss: 92.81829833984375 Discount Log Loss 6.805720806121826 KL Loss 0.6476026773452759\n",
      "Image Log Loss: 4007.9873046875 Reward Log Loss: 92.87870788574219 Discount Log Loss 3.8466362953186035 KL Loss 0.2512434720993042\n",
      "Image Log Loss: 4010.0673828125 Reward Log Loss: 92.79544830322266 Discount Log Loss 6.029065132141113 KL Loss 0.5004749298095703\n",
      "Image Log Loss: 4004.8486328125 Reward Log Loss: 92.75007629394531 Discount Log Loss 2.790032386779785 KL Loss 0.2936311960220337\n",
      "Image Log Loss: 4010.27490234375 Reward Log Loss: 93.02448272705078 Discount Log Loss 3.499668598175049 KL Loss 0.37414905428886414\n",
      "Image Log Loss: 4004.35009765625 Reward Log Loss: 92.6593246459961 Discount Log Loss 5.298405170440674 KL Loss 0.41533440351486206\n",
      "Image Log Loss: 4008.304931640625 Reward Log Loss: 92.80626678466797 Discount Log Loss 6.020898818969727 KL Loss 0.5574976801872253\n",
      "Image Log Loss: 4013.4560546875 Reward Log Loss: 92.5890884399414 Discount Log Loss 2.95638370513916 KL Loss 0.342891663312912\n",
      "Image Log Loss: 4008.01123046875 Reward Log Loss: 92.84379577636719 Discount Log Loss 5.295770168304443 KL Loss 0.41395294666290283\n",
      "Image Log Loss: 4008.6201171875 Reward Log Loss: 92.69259643554688 Discount Log Loss 4.129489898681641 KL Loss 0.4529283046722412\n",
      "Image Log Loss: 4011.529296875 Reward Log Loss: 92.97391510009766 Discount Log Loss 1.492404818534851 KL Loss 0.17516495287418365\n",
      "Image Log Loss: 4005.75439453125 Reward Log Loss: 92.58203887939453 Discount Log Loss 5.2929253578186035 KL Loss 0.411144495010376\n",
      "Image Log Loss: 4003.768798828125 Reward Log Loss: 92.78128814697266 Discount Log Loss 6.01515007019043 KL Loss 0.49249595403671265\n",
      "Image Log Loss: 4007.00439453125 Reward Log Loss: 93.01378631591797 Discount Log Loss 5.099714756011963 KL Loss 0.3993445336818695\n",
      "Image Log Loss: 4011.163818359375 Reward Log Loss: 93.2143325805664 Discount Log Loss 5.290560722351074 KL Loss 0.40912002325057983\n",
      "Image Log Loss: 4007.014404296875 Reward Log Loss: 93.0696792602539 Discount Log Loss 6.732731819152832 KL Loss 0.5721172094345093\n",
      "Image Log Loss: 4008.5498046875 Reward Log Loss: 92.42615509033203 Discount Log Loss 4.374425888061523 KL Loss 0.3163589835166931\n",
      "Image Log Loss: 4006.43408203125 Reward Log Loss: 92.66442108154297 Discount Log Loss 4.013810634613037 KL Loss 0.44407370686531067\n",
      "Image Log Loss: 4009.85009765625 Reward Log Loss: 92.92340850830078 Discount Log Loss 5.092808723449707 KL Loss 0.39605575799942017\n",
      "Image Log Loss: 4013.960693359375 Reward Log Loss: 93.01378631591797 Discount Log Loss 3.733548641204834 KL Loss 0.30092406272888184\n",
      "Image Log Loss: 4003.786865234375 Reward Log Loss: 92.8255844116211 Discount Log Loss 5.089210033416748 KL Loss 0.3945755660533905\n",
      "Image Log Loss: 4004.597900390625 Reward Log Loss: 92.48783111572266 Discount Log Loss 5.147939682006836 KL Loss 0.46115025877952576\n",
      "Image Log Loss: 4007.5 Reward Log Loss: 92.81587982177734 Discount Log Loss 5.804478645324707 KL Loss 0.4724433422088623\n",
      "Image Log Loss: 4008.190673828125 Reward Log Loss: 92.56470489501953 Discount Log Loss 2.898899555206299 KL Loss 0.3276946544647217\n",
      "Image Log Loss: 4007.703857421875 Reward Log Loss: 92.5324935913086 Discount Log Loss 3.616070032119751 KL Loss 0.40515363216400146\n",
      "Image Log Loss: 4006.143798828125 Reward Log Loss: 92.83374786376953 Discount Log Loss 5.336413383483887 KL Loss 0.5926378965377808\n",
      "Image Log Loss: 4007.164306640625 Reward Log Loss: 92.46986389160156 Discount Log Loss 4.321926116943359 KL Loss 0.48041924834251404\n",
      "Image Log Loss: 4012.236572265625 Reward Log Loss: 92.34368896484375 Discount Log Loss 1.4531205892562866 KL Loss 0.1634557545185089\n",
      "Image Log Loss: 4008.956298828125 Reward Log Loss: 92.93930053710938 Discount Log Loss 5.774621486663818 KL Loss 0.5324410200119019\n",
      "Image Log Loss: 4009.89501953125 Reward Log Loss: 92.51283264160156 Discount Log Loss 2.406187057495117 KL Loss 0.2712441384792328\n",
      "Image Log Loss: 4006.02490234375 Reward Log Loss: 92.42134094238281 Discount Log Loss 4.539645195007324 KL Loss 0.5054588913917542\n",
      "Image Log Loss: 4008.92822265625 Reward Log Loss: 93.00375366210938 Discount Log Loss 2.8778269290924072 KL Loss 0.31871211528778076\n",
      "Image Log Loss: 4009.79931640625 Reward Log Loss: 92.53591918945312 Discount Log Loss 4.9889349937438965 KL Loss 0.44930291175842285\n",
      "Image Log Loss: 4010.984130859375 Reward Log Loss: 92.69544982910156 Discount Log Loss 4.252742290496826 KL Loss 0.3690003752708435\n",
      "Image Log Loss: 4009.152587890625 Reward Log Loss: 92.88276672363281 Discount Log Loss 4.942277431488037 KL Loss 0.4472472667694092\n",
      "Image Log Loss: 4008.99853515625 Reward Log Loss: 92.71011352539062 Discount Log Loss 3.737388849258423 KL Loss 0.4189705550670624\n",
      "Image Log Loss: 4009.675537109375 Reward Log Loss: 93.26248931884766 Discount Log Loss 2.2912933826446533 KL Loss 0.26037949323654175\n",
      "Image Log Loss: 4014.045654296875 Reward Log Loss: 92.86639404296875 Discount Log Loss 3.6194546222686768 KL Loss 0.22472204267978668\n",
      "Image Log Loss: 4011.855712890625 Reward Log Loss: 92.64875030517578 Discount Log Loss 3.6902337074279785 KL Loss 0.4131341576576233\n",
      "Image Log Loss: 4009.50439453125 Reward Log Loss: 92.48716735839844 Discount Log Loss 3.6766552925109863 KL Loss 0.41144514083862305\n",
      "Image Log Loss: 4010.880615234375 Reward Log Loss: 92.82040405273438 Discount Log Loss 5.475501537322998 KL Loss 0.5167162418365479\n",
      "Image Log Loss: 4007.746337890625 Reward Log Loss: 93.03811645507812 Discount Log Loss 6.866877555847168 KL Loss 0.6715031862258911\n",
      "Image Log Loss: 4005.0625 Reward Log Loss: 92.5819320678711 Discount Log Loss 3.6372368335723877 KL Loss 0.4048472046852112\n",
      "Image Log Loss: 4005.266845703125 Reward Log Loss: 92.74118041992188 Discount Log Loss 5.362408638000488 KL Loss 0.5133917331695557\n",
      "Image Log Loss: 4004.497802734375 Reward Log Loss: 92.6103286743164 Discount Log Loss 4.32651424407959 KL Loss 0.4784168601036072\n",
      "Image Log Loss: 4007.570556640625 Reward Log Loss: 92.5369644165039 Discount Log Loss 1.432710886001587 KL Loss 0.15583696961402893\n",
      "Image Log Loss: 4011.33056640625 Reward Log Loss: 93.20150756835938 Discount Log Loss 2.1436049938201904 KL Loss 0.23198193311691284\n",
      "Image Log Loss: 4006.105224609375 Reward Log Loss: 92.6678695678711 Discount Log Loss 4.275472640991211 KL Loss 0.4607229232788086\n",
      "Image Log Loss: 4010.773193359375 Reward Log Loss: 92.57756805419922 Discount Log Loss 2.871246576309204 KL Loss 0.3156581223011017\n",
      "Image Log Loss: 4015.524658203125 Reward Log Loss: 92.94605255126953 Discount Log Loss 5.2654500007629395 KL Loss 0.3832213282585144\n",
      "Image Log Loss: 4008.691162109375 Reward Log Loss: 92.99118041992188 Discount Log Loss 1.4422422647476196 KL Loss 0.16124384105205536\n",
      "Image Log Loss: 4012.672607421875 Reward Log Loss: 92.67571258544922 Discount Log Loss 3.5526366233825684 KL Loss 0.3819049000740051\n",
      "Image Log Loss: 4008.9873046875 Reward Log Loss: 92.54987335205078 Discount Log Loss 2.8412482738494873 KL Loss 0.3046424090862274\n",
      "Image Log Loss: 4007.808837890625 Reward Log Loss: 93.10171508789062 Discount Log Loss 5.968731880187988 KL Loss 0.455652117729187\n",
      "Image Log Loss: 4007.0830078125 Reward Log Loss: 93.10687255859375 Discount Log Loss 5.750651836395264 KL Loss 0.5733932256698608\n",
      "Image Log Loss: 4010.2255859375 Reward Log Loss: 92.22245788574219 Discount Log Loss 2.8331384658813477 KL Loss 0.3022187352180481\n",
      "Image Log Loss: 4006.638427734375 Reward Log Loss: 92.19148254394531 Discount Log Loss 2.12471079826355 KL Loss 0.22671839594841003\n",
      "Image Log Loss: 4012.836669921875 Reward Log Loss: 92.5273666381836 Discount Log Loss 4.270500659942627 KL Loss 0.4189099371433258\n",
      "Image Log Loss: 4008.019287109375 Reward Log Loss: 92.81403350830078 Discount Log Loss 5.959871292114258 KL Loss 0.449065625667572\n",
      "Image Log Loss: 4008.0126953125 Reward Log Loss: 92.44409942626953 Discount Log Loss 3.516258955001831 KL Loss 0.34234166145324707\n",
      "Image Log Loss: 4008.88134765625 Reward Log Loss: 92.4106674194336 Discount Log Loss 1.4141583442687988 KL Loss 0.15068693459033966\n",
      "Image Log Loss: 4014.061767578125 Reward Log Loss: 92.55703735351562 Discount Log Loss 4.967775344848633 KL Loss 0.3599548637866974\n",
      "Image Log Loss: 4013.241455078125 Reward Log Loss: 92.92076873779297 Discount Log Loss 3.435450553894043 KL Loss 0.3382401168346405\n",
      "Image Log Loss: 4006.199951171875 Reward Log Loss: 92.54402160644531 Discount Log Loss 4.107395172119141 KL Loss 0.4121396541595459\n",
      "Image Log Loss: 4004.1337890625 Reward Log Loss: 92.43701934814453 Discount Log Loss 2.6686582565307617 KL Loss 0.26290038228034973\n",
      "Image Log Loss: 4012.576171875 Reward Log Loss: 92.51119232177734 Discount Log Loss 3.5155560970306396 KL Loss 0.36865678429603577\n",
      "Image Log Loss: 4005.422607421875 Reward Log Loss: 93.45736694335938 Discount Log Loss 4.242351531982422 KL Loss 0.28279805183410645\n",
      "Image Log Loss: 4007.297607421875 Reward Log Loss: 92.65080261230469 Discount Log Loss 3.2649567127227783 KL Loss 0.33167415857315063\n",
      "Image Log Loss: 4012.447509765625 Reward Log Loss: 92.80070495605469 Discount Log Loss 5.248637676239014 KL Loss 0.3660537004470825\n",
      "Image Log Loss: 4010.7001953125 Reward Log Loss: 92.98116302490234 Discount Log Loss 2.109046220779419 KL Loss 0.2205163538455963\n",
      "Image Log Loss: 4009.11181640625 Reward Log Loss: 93.09455108642578 Discount Log Loss 3.5090155601501465 KL Loss 0.36550289392471313\n",
      "Image Log Loss: 4005.503173828125 Reward Log Loss: 92.50537872314453 Discount Log Loss 4.207056522369385 KL Loss 0.43699753284454346\n",
      "Image Log Loss: 4005.276611328125 Reward Log Loss: 92.85269165039062 Discount Log Loss 6.645608901977539 KL Loss 0.5083521008491516\n",
      "Image Log Loss: 4006.836669921875 Reward Log Loss: 92.6207504272461 Discount Log Loss 6.64389181137085 KL Loss 0.5071726441383362\n",
      "Image Log Loss: 4006.185546875 Reward Log Loss: 92.37779998779297 Discount Log Loss 1.4035961627960205 KL Loss 0.14650803804397583\n",
      "Image Log Loss: 4005.594482421875 Reward Log Loss: 92.45071411132812 Discount Log Loss 2.799060106277466 KL Loss 0.28890544176101685\n",
      "Image Log Loss: 4009.0771484375 Reward Log Loss: 92.49718475341797 Discount Log Loss 2.7970688343048096 KL Loss 0.28752484917640686\n",
      "Image Log Loss: 4003.6279296875 Reward Log Loss: 92.56391906738281 Discount Log Loss 2.097536563873291 KL Loss 0.21667605638504028\n",
      "Image Log Loss: 4008.332763671875 Reward Log Loss: 92.97550964355469 Discount Log Loss 5.241030216217041 KL Loss 0.35722848773002625\n",
      "Image Log Loss: 4007.183349609375 Reward Log Loss: 92.7825698852539 Discount Log Loss 3.727621555328369 KL Loss 0.3886502981185913\n",
      "Image Log Loss: 4006.45068359375 Reward Log Loss: 92.91169738769531 Discount Log Loss 3.017777919769287 KL Loss 0.3181042969226837\n",
      "Image Log Loss: 4015.201171875 Reward Log Loss: 92.8393325805664 Discount Log Loss 3.504643440246582 KL Loss 0.20095689594745636\n",
      "Image Log Loss: 4008.333740234375 Reward Log Loss: 92.98994445800781 Discount Log Loss 2.089647054672241 KL Loss 0.21317927539348602\n",
      "Image Log Loss: 4008.362548828125 Reward Log Loss: 92.85884857177734 Discount Log Loss 5.931033134460449 KL Loss 0.42335614562034607\n",
      "Image Log Loss: 4006.663818359375 Reward Log Loss: 92.5213851928711 Discount Log Loss 3.47464656829834 KL Loss 0.3519308567047119\n",
      "Image Log Loss: 4006.06787109375 Reward Log Loss: 92.61219787597656 Discount Log Loss 2.948333978652954 KL Loss 0.31167614459991455\n",
      "Image Log Loss: 4007.509765625 Reward Log Loss: 92.35313415527344 Discount Log Loss 4.162815570831299 KL Loss 0.42033851146698\n",
      "Image Log Loss: 4010.792236328125 Reward Log Loss: 92.73625183105469 Discount Log Loss 5.926001071929932 KL Loss 0.416860431432724\n",
      "Image Log Loss: 4010.0908203125 Reward Log Loss: 93.030029296875 Discount Log Loss 6.61636209487915 KL Loss 0.4867447018623352\n",
      "Image Log Loss: 4010.38623046875 Reward Log Loss: 93.17640686035156 Discount Log Loss 1.3881534337997437 KL Loss 0.1400378942489624\n",
      "Image Log Loss: 4010.66943359375 Reward Log Loss: 92.4248046875 Discount Log Loss 2.077756881713867 KL Loss 0.20777973532676697\n",
      "Image Log Loss: 4007.51123046875 Reward Log Loss: 93.0501937866211 Discount Log Loss 4.540024757385254 KL Loss 0.27624964714050293\n",
      "Image Log Loss: 4010.97509765625 Reward Log Loss: 92.5494613647461 Discount Log Loss 2.7645821571350098 KL Loss 0.27538347244262695\n",
      "Image Log Loss: 4009.5712890625 Reward Log Loss: 92.95365142822266 Discount Log Loss 3.5654027462005615 KL Loss 0.3706991374492645\n",
      "Image Log Loss: 4008.690673828125 Reward Log Loss: 92.39128112792969 Discount Log Loss 2.071800947189331 KL Loss 0.20563489198684692\n",
      "Image Log Loss: 4006.46630859375 Reward Log Loss: 92.78857421875 Discount Log Loss 4.871782302856445 KL Loss 0.32893288135528564\n",
      "Image Log Loss: 4009.161865234375 Reward Log Loss: 92.47085571289062 Discount Log Loss 1.3811335563659668 KL Loss 0.13782545924186707\n",
      "Image Log Loss: 4010.695556640625 Reward Log Loss: 93.03063201904297 Discount Log Loss 3.4941625595092773 KL Loss 0.1922038048505783\n",
      "Image Log Loss: 4006.060546875 Reward Log Loss: 92.44711303710938 Discount Log Loss 2.754373550415039 KL Loss 0.27171790599823\n",
      "Image Log Loss: 4005.62060546875 Reward Log Loss: 92.49981689453125 Discount Log Loss 2.8370699882507324 KL Loss 0.29564082622528076\n",
      "Image Log Loss: 4009.13037109375 Reward Log Loss: 92.58624267578125 Discount Log Loss 4.863633632659912 KL Loss 0.3249036967754364\n",
      "Image Log Loss: 4008.10400390625 Reward Log Loss: 92.68799591064453 Discount Log Loss 6.596835136413574 KL Loss 0.46967172622680664\n",
      "Image Log Loss: 4006.773193359375 Reward Log Loss: 92.43331909179688 Discount Log Loss 4.808413505554199 KL Loss 0.46919140219688416\n",
      "Image Log Loss: 4009.9345703125 Reward Log Loss: 92.57276153564453 Discount Log Loss 4.8570876121521 KL Loss 0.3218093812465668\n",
      "Image Log Loss: 4004.611328125 Reward Log Loss: 92.36921691894531 Discount Log Loss 2.808619976043701 KL Loss 0.2891930639743805\n",
      "Image Log Loss: 4006.4072265625 Reward Log Loss: 92.81775665283203 Discount Log Loss 4.16677713394165 KL Loss 0.2537897825241089\n",
      "Image Log Loss: 4010.7080078125 Reward Log Loss: 92.90142822265625 Discount Log Loss 4.115927219390869 KL Loss 0.39720630645751953\n",
      "Image Log Loss: 4001.430419921875 Reward Log Loss: 92.44161987304688 Discount Log Loss 2.793278217315674 KL Loss 0.2854440212249756\n",
      "Image Log Loss: 4003.589599609375 Reward Log Loss: 92.38337707519531 Discount Log Loss 2.058950424194336 KL Loss 0.19836655259132385\n",
      "Image Log Loss: 4007.07177734375 Reward Log Loss: 92.28942108154297 Discount Log Loss 2.058000087738037 KL Loss 0.1978054940700531\n",
      "Image Log Loss: 4009.0166015625 Reward Log Loss: 92.70843505859375 Discount Log Loss 5.219926357269287 KL Loss 0.3270682394504547\n",
      "Image Log Loss: 4010.7861328125 Reward Log Loss: 93.26238250732422 Discount Log Loss 5.219479560852051 KL Loss 0.32580339908599854\n",
      "Image Log Loss: 4002.9296875 Reward Log Loss: 92.41199493408203 Discount Log Loss 4.105841159820557 KL Loss 0.3902040123939514\n",
      "Image Log Loss: 4005.33251953125 Reward Log Loss: 92.8053207397461 Discount Log Loss 4.137218952178955 KL Loss 0.4071216285228729\n",
      "Image Log Loss: 4008.07177734375 Reward Log Loss: 92.90084075927734 Discount Log Loss 2.0842151641845703 KL Loss 0.2122671902179718\n",
      "Image Log Loss: 4008.8662109375 Reward Log Loss: 92.55697631835938 Discount Log Loss 2.7342662811279297 KL Loss 0.2582205533981323\n",
      "Image Log Loss: 4012.3427734375 Reward Log Loss: 92.67909240722656 Discount Log Loss 4.831563472747803 KL Loss 0.309187114238739\n",
      "Image Log Loss: 4000.251953125 Reward Log Loss: 92.8018569946289 Discount Log Loss 3.171166181564331 KL Loss 0.12944617867469788\n",
      "Image Log Loss: 4008.528076171875 Reward Log Loss: 93.12008666992188 Discount Log Loss 6.578452110290527 KL Loss 0.44566985964775085\n",
      "Image Log Loss: 4001.86083984375 Reward Log Loss: 92.55126953125 Discount Log Loss 3.463831901550293 KL Loss 0.18032079935073853\n",
      "Image Log Loss: 4004.56884765625 Reward Log Loss: 92.6948013305664 Discount Log Loss 4.1425886154174805 KL Loss 0.2418951541185379\n",
      "Image Log Loss: 4006.851806640625 Reward Log Loss: 92.31993103027344 Discount Log Loss 4.771482944488525 KL Loss 0.44231879711151123\n",
      "Image Log Loss: 4004.625 Reward Log Loss: 92.7161636352539 Discount Log Loss 2.7280123233795166 KL Loss 0.25278711318969727\n",
      "Image Log Loss: 4007.63623046875 Reward Log Loss: 92.79349517822266 Discount Log Loss 6.176224231719971 KL Loss 0.42672497034072876\n",
      "Image Log Loss: 4006.1982421875 Reward Log Loss: 92.65097045898438 Discount Log Loss 5.894594669342041 KL Loss 0.37428703904151917\n",
      "Image Log Loss: 4004.417236328125 Reward Log Loss: 92.68438720703125 Discount Log Loss 4.086732387542725 KL Loss 0.3732222318649292\n",
      "Image Log Loss: 4005.281982421875 Reward Log Loss: 92.69371032714844 Discount Log Loss 6.573824405670166 KL Loss 0.4329782724380493\n",
      "Image Log Loss: 4004.6572265625 Reward Log Loss: 92.4614028930664 Discount Log Loss 4.084115028381348 KL Loss 0.3707689046859741\n",
      "Image Log Loss: 4003.309326171875 Reward Log Loss: 92.43914031982422 Discount Log Loss 2.0434377193450928 KL Loss 0.18587616086006165\n",
      "Image Log Loss: 4006.831787109375 Reward Log Loss: 93.05155181884766 Discount Log Loss 5.891456127166748 KL Loss 0.36709603667259216\n",
      "Image Log Loss: 4000.86376953125 Reward Log Loss: 92.45508575439453 Discount Log Loss 2.7207024097442627 KL Loss 0.24537599086761475\n",
      "Image Log Loss: 4007.8173828125 Reward Log Loss: 93.29987335205078 Discount Log Loss 3.853731155395508 KL Loss 0.18403901159763336\n",
      "Image Log Loss: 4005.437255859375 Reward Log Loss: 92.6396484375 Discount Log Loss 3.405891180038452 KL Loss 0.31516268849372864\n",
      "Image Log Loss: 4005.55224609375 Reward Log Loss: 92.82195281982422 Discount Log Loss 2.7171390056610107 KL Loss 0.2425566017627716\n",
      "Image Log Loss: 4007.110595703125 Reward Log Loss: 92.369873046875 Discount Log Loss 2.7159104347229004 KL Loss 0.24088995158672333\n",
      "Image Log Loss: 4004.54248046875 Reward Log Loss: 92.938232421875 Discount Log Loss 5.886776924133301 KL Loss 0.35949286818504333\n",
      "Image Log Loss: 4005.902587890625 Reward Log Loss: 93.11698150634766 Discount Log Loss 2.713214159011841 KL Loss 0.23979826271533966\n",
      "Image Log Loss: 4007.335205078125 Reward Log Loss: 92.49574279785156 Discount Log Loss 2.7118537425994873 KL Loss 0.23926964402198792\n",
      "Image Log Loss: 4006.128173828125 Reward Log Loss: 92.54393768310547 Discount Log Loss 3.3942980766296387 KL Loss 0.3068424165248871\n",
      "Image Log Loss: 4013.265380859375 Reward Log Loss: 92.35546875 Discount Log Loss 2.040018081665039 KL Loss 0.18833822011947632\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 39>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m episode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m50\u001B[39m):\n\u001B[1;32m---> 40\u001B[0m     \u001B[43menvironment_interactor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_trajectories\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     data \u001B[38;5;241m=\u001B[39m buffer\u001B[38;5;241m.\u001B[39msample(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, prefetch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m70\u001B[39m)\n",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36mEnvironmentInteractor.create_trajectories\u001B[1;34m(self, iterations)\u001B[0m\n\u001B[0;32m     20\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m---> 21\u001B[0m next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuffer\u001B[38;5;241m.\u001B[39madd((\n\u001B[0;32m     24\u001B[0m   tf\u001B[38;5;241m.\u001B[39mcast(tf\u001B[38;5;241m.\u001B[39mconstant(state, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_spec[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mas_list()), tf\u001B[38;5;241m.\u001B[39mfloat32),\n\u001B[0;32m     25\u001B[0m   tf\u001B[38;5;241m.\u001B[39mcast(tf\u001B[38;5;241m.\u001B[39mconstant(next_state, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_spec[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mas_list()), tf\u001B[38;5;241m.\u001B[39mfloat32),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     28\u001B[0m   tf\u001B[38;5;241m.\u001B[39mcast(tf\u001B[38;5;241m.\u001B[39mconstant(\u001B[38;5;241m1\u001B[39m\u001B[38;5;241m-\u001B[39mdone, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_spec[\u001B[38;5;241m4\u001B[39m]\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mas_list()), tf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     29\u001B[0m ))\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:13\u001B[0m, in \u001B[0;36mOrderEnforcing.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot call env.step() before calling reset()\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 13\u001B[0m observation, reward, done, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m observation, reward, done, info\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\envs\\common\\abstract.py:216\u001B[0m, in \u001B[0;36mAbstractEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_simulate\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m obs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_type\u001B[38;5;241m.\u001B[39mobserve()\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\envs\\common\\abstract.py:235\u001B[0m, in \u001B[0;36mAbstractEnv._simulate\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_type\u001B[38;5;241m.\u001B[39mact(action)\n\u001B[1;32m--> 235\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroad\u001B[38;5;241m.\u001B[39mstep(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimulation_frequency\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\road\\road.py:324\u001B[0m, in \u001B[0;36mRoad.act\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m vehicle \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvehicles:\n\u001B[1;32m--> 324\u001B[0m     \u001B[43mvehicle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\vehicle\\behavior.py:100\u001B[0m, in \u001B[0;36mIDMVehicle.act\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;66;03m# Longitudinal: IDM\u001B[39;00m\n\u001B[1;32m--> 100\u001B[0m front_vehicle, rear_vehicle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mneighbour_vehicles\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlane_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    101\u001B[0m action[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124macceleration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39macceleration(ego_vehicle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    102\u001B[0m                                            front_vehicle\u001B[38;5;241m=\u001B[39mfront_vehicle,\n\u001B[0;32m    103\u001B[0m                                            rear_vehicle\u001B[38;5;241m=\u001B[39mrear_vehicle)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\road\\road.py:361\u001B[0m, in \u001B[0;36mRoad.neighbour_vehicles\u001B[1;34m(self, vehicle, lane_index)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vehicle \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(v, Landmark):  \u001B[38;5;66;03m# self.network.is_connected_road(v.lane_index,\u001B[39;00m\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;66;03m# lane_index, same_lane=True):\u001B[39;00m\n\u001B[1;32m--> 361\u001B[0m     s_v, lat_v \u001B[38;5;241m=\u001B[39m \u001B[43mlane\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_coordinates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mposition\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lane\u001B[38;5;241m.\u001B[39mon_lane(v\u001B[38;5;241m.\u001B[39mposition, s_v, lat_v, margin\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\highway_env\\road\\lane.py:185\u001B[0m, in \u001B[0;36mStraightLane.local_coordinates\u001B[1;34m(self, position)\u001B[0m\n\u001B[0;32m    184\u001B[0m delta \u001B[38;5;241m=\u001B[39m position \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart\n\u001B[1;32m--> 185\u001B[0m longitudinal \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdirection\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m lateral \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdot(delta, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdirection_lateral)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\executing\\executing.py:312\u001B[0m, in \u001B[0;36mSource.executing\u001B[1;34m(cls, frame_or_tb)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 312\u001B[0m     args \u001B[38;5;241m=\u001B[39m \u001B[43mexecuting_cache\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "\u001B[1;31mKeyError\u001B[0m: (<code object run_code at 0x0000022A08D74870, file \"C:\\Users\\Peter\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3362>, 2379560208496, 74)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1993\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   1992\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1993\u001B[0m         stb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mInteractiveTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1994\u001B[0m \u001B[43m            \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_offset\u001B[49m\n\u001B[0;32m   1995\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1997\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1118\u001B[0m, in \u001B[0;36mAutoFormattedTB.structured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1117\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtb \u001B[38;5;241m=\u001B[39m tb\n\u001B[1;32m-> 1118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mFormattedTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1119\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1012\u001B[0m, in \u001B[0;36mFormattedTB.structured_traceback\u001B[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m   1010\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose_modes:\n\u001B[0;32m   1011\u001B[0m     \u001B[38;5;66;03m# Verbose modes need a full traceback\u001B[39;00m\n\u001B[1;32m-> 1012\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVerboseTB\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstructured_traceback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\n\u001B[0;32m   1014\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMinimal\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:865\u001B[0m, in \u001B[0;36mVerboseTB.structured_traceback\u001B[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001B[0m\n\u001B[0;32m    864\u001B[0m \u001B[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 865\u001B[0m formatted_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_exception_as_a_whole\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m                                                       \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    868\u001B[0m colors \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mColors  \u001B[38;5;66;03m# just a shorthand + quicker name lookup\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:799\u001B[0m, in \u001B[0;36mVerboseTB.format_exception_as_a_whole\u001B[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m    797\u001B[0m head \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_header(etype, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlong_header)\n\u001B[0;32m    798\u001B[0m records \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 799\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_records\u001B[49m\u001B[43m(\u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnumber_of_lines_of_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_offset\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m etb \u001B[38;5;28;01melse\u001B[39;00m []\n\u001B[0;32m    800\u001B[0m )\n\u001B[0;32m    802\u001B[0m frames \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\ultratb.py:854\u001B[0m, in \u001B[0;36mVerboseTB.get_records\u001B[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001B[0m\n\u001B[0;32m    849\u001B[0m options \u001B[38;5;241m=\u001B[39m stack_data\u001B[38;5;241m.\u001B[39mOptions(\n\u001B[0;32m    850\u001B[0m     before\u001B[38;5;241m=\u001B[39mbefore,\n\u001B[0;32m    851\u001B[0m     after\u001B[38;5;241m=\u001B[39mafter,\n\u001B[0;32m    852\u001B[0m     pygments_formatter\u001B[38;5;241m=\u001B[39mformatter,\n\u001B[0;32m    853\u001B[0m )\n\u001B[1;32m--> 854\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstack_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFrameInfo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43metb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m[tb_offset:]\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\stack_data\\core.py:565\u001B[0m, in \u001B[0;36mFrameInfo.stack_data\u001B[1;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001B[0m\n\u001B[0;32m    563\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m frame\u001B[38;5;241m.\u001B[39mf_code, lineno\n\u001B[1;32m--> 565\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m collapse_repeated(\n\u001B[0;32m    566\u001B[0m     stack,\n\u001B[0;32m    567\u001B[0m     mapper\u001B[38;5;241m=\u001B[39mmapper,\n\u001B[0;32m    568\u001B[0m     collapser\u001B[38;5;241m=\u001B[39mRepeatedFrames,\n\u001B[0;32m    569\u001B[0m     key\u001B[38;5;241m=\u001B[39m_frame_key,\n\u001B[0;32m    570\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\stack_data\\utils.py:84\u001B[0m, in \u001B[0;36mcollapse_repeated\u001B[1;34m(lst, collapser, mapper, key)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_highlighted:\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mmap\u001B[39m(mapper, original_group)\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\stack_data\\core.py:555\u001B[0m, in \u001B[0;36mFrameInfo.stack_data.<locals>.mapper\u001B[1;34m(f)\u001B[0m\n\u001B[0;32m    554\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmapper\u001B[39m(f):\n\u001B[1;32m--> 555\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\stack_data\\core.py:520\u001B[0m, in \u001B[0;36mFrameInfo.__init__\u001B[1;34m(self, frame_or_tb, options)\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    516\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    517\u001B[0m         frame_or_tb: Union[FrameType, TracebackType],\n\u001B[0;32m    518\u001B[0m         options: Optional[Options] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    519\u001B[0m ):\n\u001B[1;32m--> 520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecuting \u001B[38;5;241m=\u001B[39m \u001B[43mSource\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecuting\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe_or_tb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    521\u001B[0m     frame, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlineno \u001B[38;5;241m=\u001B[39m frame_and_lineno(frame_or_tb)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\executing\\executing.py:364\u001B[0m, in \u001B[0;36mSource.executing\u001B[1;34m(cls, frame_or_tb)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m source, node, stmts, decorator\n\u001B[1;32m--> 364\u001B[0m args \u001B[38;5;241m=\u001B[39m find(source\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfor_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m, retry_cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    365\u001B[0m executing_cache[key] \u001B[38;5;241m=\u001B[39m args\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\executing\\executing.py:247\u001B[0m, in \u001B[0;36mSource.for_frame\u001B[1;34m(cls, frame, use_cache)\u001B[0m\n\u001B[0;32m    244\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    245\u001B[0m \u001B[38;5;124;03mReturns the `Source` object corresponding to the file the frame is executing in.\u001B[39;00m\n\u001B[0;32m    246\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 247\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfor_filename\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_code\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mco_filename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf_globals\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\executing\\executing.py:265\u001B[0m, in \u001B[0;36mSource.for_filename\u001B[1;34m(cls, filename, module_globals, use_cache)\u001B[0m\n\u001B[0;32m    264\u001B[0m lines \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(linecache\u001B[38;5;241m.\u001B[39mgetlines(filename, module_globals))\n\u001B[1;32m--> 265\u001B[0m result \u001B[38;5;241m=\u001B[39m source_cache[filename] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_for_filename_and_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlines\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\executing\\executing.py:276\u001B[0m, in \u001B[0;36mSource._for_filename_and_lines\u001B[1;34m(cls, filename, lines)\u001B[0m\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m--> 276\u001B[0m result \u001B[38;5;241m=\u001B[39m source_cache[(filename, lines)] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlines\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\stack_data\\core.py:81\u001B[0m, in \u001B[0;36mSource.__init__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree:\n\u001B[1;32m---> 81\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masttokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\executing\\executing.py:408\u001B[0m, in \u001B[0;36mSource.asttokens\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01masttokens\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ASTTokens  \u001B[38;5;66;03m# must be installed separately\u001B[39;00m\n\u001B[1;32m--> 408\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mASTTokens\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    409\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtree\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\asttokens\\asttokens.py:59\u001B[0m, in \u001B[0;36mASTTokens.__init__\u001B[1;34m(self, source_text, parse, tree, filename)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Tokenize the code.\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource_text\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Extract the start positions of all tokens, so that we can quickly map positions to tokens.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\asttokens\\asttokens.py:85\u001B[0m, in \u001B[0;36mASTTokens._generate_tokens\u001B[1;34m(self, text)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;66;03m# This is technically an undocumented API for Python3, but allows us to use the same API as for\u001B[39;00m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;66;03m# Python2. See http://stackoverflow.com/a/4952291/328565.\u001B[39;00m\n\u001B[1;32m---> 85\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, tok \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(tokenize\u001B[38;5;241m.\u001B[39mgenerate_tokens(io\u001B[38;5;241m.\u001B[39mStringIO(text)\u001B[38;5;241m.\u001B[39mreadline)):\n\u001B[0;32m     86\u001B[0m   tok_type, tok_str, start, end, line \u001B[38;5;241m=\u001B[39m tok\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tokenize.py:590\u001B[0m, in \u001B[0;36m_tokenize\u001B[1;34m(readline, encoding)\u001B[0m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m initial\u001B[38;5;241m.\u001B[39misidentifier():               \u001B[38;5;66;03m# ordinary name\u001B[39;00m\n\u001B[1;32m--> 590\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[43mTokenInfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNAME\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m initial \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124m'\u001B[39m:                      \u001B[38;5;66;03m# continued stmt\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "    \u001B[1;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2014\u001B[0m, in \u001B[0;36mInteractiveShell.showtraceback\u001B[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001B[0m\n\u001B[0;32m   2011\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_showtraceback(etype, value, stb)\n\u001B[0;32m   2013\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m-> 2014\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_exception_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mstderr)\n",
      "File \u001B[1;32m~\\PycharmProjects\\DreamerV2-final_project-DRL\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1950\u001B[0m, in \u001B[0;36mInteractiveShell.get_exception_only\u001B[1;34m(self, exc_tuple)\u001B[0m\n\u001B[0;32m   1945\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1946\u001B[0m \u001B[38;5;124;03mReturn as a string (ending with a newline) the exception that\u001B[39;00m\n\u001B[0;32m   1947\u001B[0m \u001B[38;5;124;03mjust occurred, without any traceback.\u001B[39;00m\n\u001B[0;32m   1948\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1949\u001B[0m etype, value, tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_exc_info(exc_tuple)\n\u001B[1;32m-> 1950\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[43mtraceback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_exception_only\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1951\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(msg)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\traceback.py:140\u001B[0m, in \u001B[0;36mformat_exception_only\u001B[1;34m(etype, value)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mformat_exception_only\u001B[39m(etype, value):\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;124;03m\"\"\"Format the exception part of a traceback.\u001B[39;00m\n\u001B[0;32m    126\u001B[0m \n\u001B[0;32m    127\u001B[0m \u001B[38;5;124;03m    The arguments are the exception type and value such as given by\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    138\u001B[0m \n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[43mTracebackException\u001B[49m\u001B[43m(\u001B[49m\u001B[43metype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mformat_exception_only())\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\traceback.py:524\u001B[0m, in \u001B[0;36mTracebackException.__init__\u001B[1;34m(self, exc_type, exc_value, exc_traceback, limit, lookup_lines, capture_locals, _seen)\u001B[0m\n\u001B[0;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmsg \u001B[38;5;241m=\u001B[39m exc_value\u001B[38;5;241m.\u001B[39mmsg\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lookup_lines:\n\u001B[1;32m--> 524\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\traceback.py:536\u001B[0m, in \u001B[0;36mTracebackException._load_lines\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    534\u001B[0m     frame\u001B[38;5;241m.\u001B[39mline\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__context__:\n\u001B[1;32m--> 536\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__context__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cause__:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cause__\u001B[38;5;241m.\u001B[39m_load_lines()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\traceback.py:536\u001B[0m, in \u001B[0;36mTracebackException._load_lines\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    534\u001B[0m     frame\u001B[38;5;241m.\u001B[39mline\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__context__:\n\u001B[1;32m--> 536\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__context__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cause__:\n\u001B[0;32m    538\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cause__\u001B[38;5;241m.\u001B[39m_load_lines()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\traceback.py:534\u001B[0m, in \u001B[0;36mTracebackException._load_lines\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;124;03m\"\"\"Private API. force all lines in the stack to be loaded.\"\"\"\u001B[39;00m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack:\n\u001B[1;32m--> 534\u001B[0m     \u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mline\u001B[49m\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__context__:\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__context__\u001B[38;5;241m.\u001B[39m_load_lines()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\traceback.py:288\u001B[0m, in \u001B[0;36mFrameSummary.line\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mline\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    287\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_line \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 288\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_line \u001B[38;5;241m=\u001B[39m \u001B[43mlinecache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetline\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlineno\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[0;32m    289\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_line\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\linecache.py:30\u001B[0m, in \u001B[0;36mgetline\u001B[1;34m(filename, lineno, module_globals)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgetline\u001B[39m(filename, lineno, module_globals\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;124;03m\"\"\"Get a line for a Python source file from the cache.\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;124;03m    Update the cache if it doesn't contain an entry for this file already.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m     lines \u001B[38;5;241m=\u001B[39m \u001B[43mgetlines\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_globals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m lineno \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(lines):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m lines[lineno \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\linecache.py:46\u001B[0m, in \u001B[0;36mgetlines\u001B[1;34m(filename, module_globals)\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m cache[filename][\u001B[38;5;241m2\u001B[39m]\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mupdatecache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule_globals\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mMemoryError\u001B[39;00m:\n\u001B[0;32m     48\u001B[0m     clearcache()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\linecache.py:136\u001B[0m, in \u001B[0;36mupdatecache\u001B[1;34m(filename, module_globals)\u001B[0m\n\u001B[0;32m    134\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[0;32m    135\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 136\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtokenize\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfullname\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[0;32m    137\u001B[0m         lines \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tokenize.py:394\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(filename)\u001B[0m\n\u001B[0;32m    392\u001B[0m buffer \u001B[38;5;241m=\u001B[39m _builtin_open(filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    393\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 394\u001B[0m     encoding, lines \u001B[38;5;241m=\u001B[39m \u001B[43mdetect_encoding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    395\u001B[0m     buffer\u001B[38;5;241m.\u001B[39mseek(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m    396\u001B[0m     text \u001B[38;5;241m=\u001B[39m TextIOWrapper(buffer, encoding, line_buffering\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tokenize.py:363\u001B[0m, in \u001B[0;36mdetect_encoding\u001B[1;34m(readline)\u001B[0m\n\u001B[0;32m    360\u001B[0m         encoding \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-sig\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    361\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m encoding\n\u001B[1;32m--> 363\u001B[0m first \u001B[38;5;241m=\u001B[39m \u001B[43mread_or_stop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    364\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first\u001B[38;5;241m.\u001B[39mstartswith(BOM_UTF8):\n\u001B[0;32m    365\u001B[0m     bom_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\tokenize.py:321\u001B[0m, in \u001B[0;36mdetect_encoding.<locals>.read_or_stop\u001B[1;34m()\u001B[0m\n\u001B[0;32m    319\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_or_stop\u001B[39m():\n\u001B[0;32m    320\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 321\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    322\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m    323\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from importlib_metadata import distribution\n",
    "\n",
    "# TODO move hyperparams\n",
    "epochs = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002)\n",
    "\n",
    "buffer = Buffer(batch_size=1)\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 32),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "\n",
    "environment_interactor = EnvironmentInteractor(config, buffer)\n",
    "\n",
    "# Sample from buffer\n",
    "\n",
    "world_model = WorldModel()\n",
    "rssm = RSSM()\n",
    "\n",
    "models = (\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "\n",
    "for episode in range(50):\n",
    "    environment_interactor.create_trajectories(1000)\n",
    "\n",
    "    data = buffer.sample(batch_size=50, prefetch_size=5)\n",
    "\n",
    "    for sequence in data:\n",
    "        state, next_state, action, reward, non_terminal = sequence[0]\n",
    "\n",
    "            # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        combined_trainable_variables = models[0].trainable_variables\n",
    "        for i in range(len(models)):\n",
    "            if i+1 >= len(models):\n",
    "                break\n",
    "            combined_trainable_variables += models[i+1].trainable_variables\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoded_state = world_model.encoder(state)\n",
    "            initial_rssm_state = RSSMState()\n",
    "            prior_rssm_states, posterior_rssm_states = rssm.observing_rollout(encoded_state, action, non_terminal, initial_rssm_state)\n",
    "            hidden_state_h_and_stochastic_state_z = tf.concat([posterior_rssm_states.stochastic_state_z, posterior_rssm_states.hidden_rnn_state], axis=-1)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            hidden_state_h_and_stochastic_state_z = tf.reshape(hidden_state_h_and_stochastic_state_z, (-1,stochastic_state_size + hidden_unit_size))\n",
    "\n",
    "            decoder_logits = world_model.decoder(hidden_state_h_and_stochastic_state_z)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            decoder_logits = tf.reshape(decoder_logits, (-1, image_shape[0], image_shape[1], image_shape[2]))\n",
    "\n",
    "            decoder_distribution = tfp.distributions.Independent(tfp.distributions.Normal(decoder_logits, 1))\n",
    "            reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "            reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "            discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "            discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "\n",
    "            image_log_loss = compute_log_loss(decoder_distribution, state)\n",
    "            reward_log_loss = compute_log_loss(reward_distribution, reward)\n",
    "            discount_log_loss = compute_log_loss(discount_distribution, non_terminal)\n",
    "            kl_loss = compute_kl_loss(prior_rssm_states, posterior_rssm_states)\n",
    "\n",
    "\n",
    "\n",
    "            loss = image_log_loss + reward_log_loss + discount_log_loss + kl_loss\n",
    "            print(f\"Image Log Loss: {image_log_loss} Reward Log Loss: {reward_log_loss} Discount Log Loss {discount_log_loss} KL Loss {kl_loss}\")\n",
    "            wandb.log({\"Image Log Loss\": image_log_loss, \"Reward Log Loss\": reward_log_loss, \"Discount Log Loss\": discount_log_loss, \"KL Loss\": kl_loss, \"Loss\": loss})\n",
    "            # TODO maybe in Gradienttape??\n",
    "            gradients = tape.gradient(loss, combined_trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, combined_trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "  t, _ = next(iterator)\n",
    "  trajectories.append(t)\n",
    "\n",
    "\n",
    "    train_step((\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "    )\n",
    "\n",
    "#print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World ModelTraining Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g = tf.keras.layers.GRUCell(32)\n",
    "r = tf.random.uniform((64,32), 0,3)\n",
    "g(r,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World model & agent training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyperparam inits\n",
    "Agent Data collection in environment + adding data to ERB (+ measure at which reward loop stops?) \n",
    "World model loop on data sampled from ERB\n",
    "Agent training loop with world model feedback\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate environment and network objects\n",
    "# Loop:\n",
    "# Pass respective inputs to networks\n",
    "# Collect outputs\n",
    "# Compute individuall losses\n",
    "# Add together to 1 big loss\n",
    "# Propagate with gradient Tape through network\n",
    "\n",
    "\n",
    "# compute the loss of an input for the model and optimize/tweak according the parameters\n",
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# TODO move hyperparams to the rest\n",
    "epochs = 32\n",
    "\n",
    "# define loss-function and optimizer\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "\n",
    "    for world_model_input in tqdm(data):\n",
    "        train_loss = train_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76ff493d4a64cba1f151b79ed1bae57e7e5cd0f3c80d023b172c30c4e877d0b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}