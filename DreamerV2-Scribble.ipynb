{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Imports and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "# Layer\n",
    "from tensorflow.keras.layers import Dense, Layer, Conv2DTranspose, Conv2D, GlobalAveragePooling2D, Reshape, BatchNormalization, GRUCell, MaxPooling2D, Flatten, RNN\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy, KLDivergence\n",
    "import tensorflow_probability as tfp \n",
    "\n",
    "\n",
    "\n",
    "# Buffer \n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "# Further support\n",
    "import numpy as np\n",
    "from typing import NamedTuple\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# Environment\n",
    "import gym\n",
    "import highway_env\n",
    "import random\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeterkeffer\u001b[0m (\u001b[33mcogsci\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/peterkeffer/DreamerV2-final_project-DRL/wandb/run-20220803_161651-14afolop</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/cogsci/DreamerV2/runs/14afolop\" target=\"_blank\">atomic-monkey-5</a></strong> to <a href=\"https://wandb.ai/cogsci/DreamerV2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/cogsci/DreamerV2/runs/14afolop?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2a7fc5090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\"Debugging\", project=\"DreamerV2\", sync_tensorboard=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Has to save (Observation, action, reward, terminal state)\n",
    "from numpy import float32\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=1,\n",
    "        buffer_length=1000, \n",
    "        observation_size=(128,32,1),\n",
    "        action_size=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create replay buffer\n",
    "\n",
    "        Buffer size = batch_size * buffer_length\n",
    "\n",
    "        \"\"\"\n",
    "        # Save batch size for other functions of buffer\n",
    "        # NOT the usual batch size in Deep Learning\n",
    "        # Batches in Uniform Replay Buffer describe size of input added to the buffer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Tell buffer what data & which size to expect\n",
    "        self.data_spec = (\n",
    "            tf.TensorSpec(\n",
    "                shape= observation_size,\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Observation\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=observation_size,\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Next state\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[action_size],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Action\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                # Reward size\n",
    "                shape=[1, ],\n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Reward\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, ],\n",
    "                # Either 0 or 1 \n",
    "                dtype=tf.dtypes.float32,\n",
    "                name=\"Terminal State\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the buffer \n",
    "        self.buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            self.data_spec, batch_size, buffer_length\n",
    "        )\n",
    "\n",
    "    def obtain_buffer_specs(self):\n",
    "        return self.data_spec\n",
    "\n",
    "    def add(self, items):\n",
    "        \"\"\"\n",
    "        length of items must be equal to batch size\n",
    "\n",
    "        items: list or tuple of batched data from (50, 5)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Combine all values from \"items\" in tensor\n",
    "        # Not sure wether we need tf.nest.map_structure\n",
    "        batched_values = tf.nest.map_structure(\n",
    "            lambda t: tf.stack([t] * self.batch_size),\n",
    "            items\n",
    "        )\n",
    "        \n",
    "        # Add to batch\n",
    "        self.buffer.add_batch(batched_values)\n",
    "\n",
    "    def sample(self, batch_size, prefetch_size):\n",
    "        data = self.buffer.as_dataset(single_deterministic_pass=True)\n",
    "        data = data.batch(batch_size).prefetch(prefetch_size)\n",
    "        #later we want these to sequences\n",
    "        return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EnvironmentInteractor:\n",
    "\n",
    "  def __init__(self, config, buffer, environment_name = \"highway-fast-v0\"):\n",
    "    self.config = config\n",
    "\n",
    "    self.env = gym.make(environment_name)    \n",
    "    self.env.configure(config)\n",
    "\n",
    "    self.buffer = buffer\n",
    "    # Save sizes of the stupid tensors\n",
    "    self.data_spec = self.buffer.obtain_buffer_specs()\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  def create_trajectories(self, iterations):\n",
    "    state = self.env.reset()\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        action = self.env.action_space.sample()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        \n",
    "        self.buffer.add((\n",
    "             tf.cast(tf.constant(state, shape=self.data_spec[0].shape.as_list()), tf.float32),\n",
    "             tf.cast(tf.constant(next_state, shape=self.data_spec[1].shape.as_list()), tf.float32),\n",
    "            tf.cast(tf.constant(action, shape=self.data_spec[2].shape.as_list()), tf.float32),\n",
    "             tf.cast(tf.constant(reward, shape=self.data_spec[3].shape.as_list()), tf.float32),\n",
    "            tf.cast(tf.constant(1-done, shape=self.data_spec[4].shape.as_list()), tf.float32)\n",
    "        ))\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "          state = self.env.reset()\n",
    "\n",
    "\n",
    "  def __del__(self):\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Image size\n",
    "image_shape = (128,32, 1)\n",
    "\n",
    "# Long term memory of GRU\n",
    "hidden_unit_size = 200\n",
    "\n",
    "# Z in paper\n",
    "stochastic_state_shape = (32,32)\n",
    "stochastic_state_size = stochastic_state_shape[0] * stochastic_state_shape[1]\n",
    "\n",
    "#\n",
    "action_size = 1\n",
    "\n",
    "#\n",
    "mlp_hidden_layer_size = 100\n",
    "batch_size = 50\n",
    "\n",
    "# TODO different variable names for network inp/outp sizes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class WorldModel:\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = self.create_encoder()\n",
    "        self.decoder = self.create_decoder()\n",
    "        self.reward_model = self.create_reward_predictor()\n",
    "        self.discount_model = self.create_discount_predictor()\n",
    "\n",
    "\n",
    "    def create_encoder(self, input_size=image_shape, output_size=hidden_unit_size):\n",
    "        # Third dimension might be obsolete\n",
    "        encoder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Conv2D(16, (3, 3), activation=\"elu\", padding=\"same\")(encoder_input) # 16 layers of filtered 192x48 features\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 96x24\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(64, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 48x12\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 48x12\n",
    "        x = GlobalAveragePooling2D()(x) # 64\n",
    "        encoder_output = Dense(output_size, activation = \"elu\")(x)\n",
    "\n",
    "        encoder = tf.keras.Model(encoder_input, encoder_output, name=\"Encoder\")\n",
    "\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    # Input size = 1024(z:32x32) + 200(size of hidden state)\n",
    "    # Output size = game frame\n",
    "    def create_decoder(\n",
    "        self, \n",
    "        input_size=stochastic_state_size + hidden_unit_size, \n",
    "        output_size=image_shape\n",
    "    ):\n",
    "        # Third dimension might be obsolete\n",
    "        decoder_input = tf.keras.Input(shape=input_size)\n",
    "        # TODO WIE SCHLIMM IST EIN MLP HIER?\n",
    "        x = Dense(256, activation= \"elu\")(decoder_input)\n",
    "        x = Reshape((32, 8, 1))(x) \n",
    "        # TODO Check whether correct reshape happens\n",
    "        #tf.debugging.assert_equal(x)\n",
    "        x = Conv2DTranspose(16, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "       # x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = Flatten()(x)\n",
    "        # Might needs shape as Tensor  #event_shape=output_size\n",
    "\n",
    "        # decoder_output = tfp.layers.IndependentNormal(event_shape=output_size)(x)\n",
    "\n",
    "\n",
    "        decoder = tf.keras.Model(\n",
    "            decoder_input,\n",
    "            x,\n",
    "            name=\"Decoder\"\n",
    "        )\n",
    "\n",
    "        return decoder\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_reward_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        reward_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(reward_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size)(x)\n",
    "        # Creates indipendent normal distribution\n",
    "        # Hope is that it learns to output variables over reward space [0,1]\n",
    "        #reward_predictor_output = tfp.layers.IndependentNormal()(x)\n",
    "\n",
    "        reward_predictor = tf.keras.Model(\n",
    "            reward_predictor_input,\n",
    "            x,\n",
    "            name=\"create_reward_predictor\"\n",
    "        )\n",
    "\n",
    "        return reward_predictor\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_discount_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        discount_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(discount_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        # Create 1 output sampled from bernoulli distribution\n",
    "        #discount_predictor_output = tfp.layers.IndependentBernoulli()(x)\n",
    "\n",
    "        discount_predictor = tf.keras.Model(\n",
    "            discount_predictor_input,\n",
    "            x,\n",
    "            name=\"create_discount_predictor\"\n",
    "        )\n",
    "\n",
    "        return discount_predictor\n",
    "\n",
    "\n",
    "\n",
    "class RSSMState(NamedTuple):\n",
    "    logits: tf.Tensor = tf.zeros(shape=(stochastic_state_size,))\n",
    "    stochastic_state_z: tf.Tensor = tf.zeros(shape=(stochastic_state_size,))\n",
    "    hidden_rnn_state: tf.Tensor = tf.zeros(shape=(hidden_unit_size,))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_list(cls, rssm_states):\n",
    "        logits = tf.stack([rssm_state.logits for rssm_state in rssm_states])\n",
    "        stochastic_state_z = tf.stack([rssm_state.stochastic_state_z for rssm_state in rssm_states])\n",
    "        hidden_rnn_state = tf.stack([rssm_state.hidden_rnn_state for rssm_state in rssm_states])\n",
    "\n",
    "        return cls(logits, stochastic_state_z, hidden_rnn_state)\n",
    "\n",
    "\n",
    "\n",
    "class RSSM:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_action_embedder = self.create_stochastic_state_action_embedder()\n",
    "        self.rnn = self.create_rnn()\n",
    "        self.prior_model = self.create_prior_stochastic_state_embedder()\n",
    "        self.posterior_model = self.create_posterior_stochastic_state_embedder()\n",
    "\n",
    "    def create_stochastic_state_action_embedder(\n",
    "        self, \n",
    "        input_size=(stochastic_state_size + action_size,),\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        state_action_input = tf.keras.Input(shape=input_size)\n",
    "        state_action_output = Dense(output_size, activation = \"elu\")(state_action_input)\n",
    "\n",
    "        stochastic_state_action_embedder = tf.keras.Model(\n",
    "            state_action_input,\n",
    "            state_action_output,\n",
    "            name=\"stochastic_state_action_embedder\"\n",
    "        )\n",
    "\n",
    "        return stochastic_state_action_embedder\n",
    "\n",
    "    # Contains GRU cell\n",
    "    def create_rnn(\n",
    "        self, \n",
    "        input_size=(hidden_unit_size, ),\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        return RNN(GRUCell(output_size))\n",
    "\n",
    "        rnn_input = tf.keras.Input(shape=input_size)\n",
    "       # rnn_hidden_state_placeholder = tf.keras.Input(shape=(hidden_unit_size,))\n",
    "        rnn_output = rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(output_size))(rnn_input)\n",
    "\n",
    "\n",
    "        rnn = tf.keras.Model(\n",
    "            rnn_input,\n",
    "            rnn_output,\n",
    "            name=\"rnn\"\n",
    "        )\n",
    "\n",
    "        return rnn\n",
    "\n",
    "    # Z^ in paper\n",
    "    def create_prior_stochastic_state_embedder(\n",
    "        self, \n",
    "        input_size=hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(x)\n",
    "\n",
    "        create_prior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_prior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_prior_stochastic_state_embedder\n",
    "\n",
    "    # Z in paper\n",
    "    # Input size = concatenated output of RNN with output of CNN\n",
    "    def create_posterior_stochastic_state_embedder(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(x)\n",
    "\n",
    "        create_posterior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_posterior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_posterior_stochastic_state_embedder\n",
    "\n",
    "    def sample_stochastic_state(self, logits):\n",
    "        \"\"\"\n",
    "        Gets probabilities for each element of class in each category.\n",
    "        Used to generate embeddings from logits.\n",
    "        \"\"\"\n",
    "\n",
    "        # Logit Outputs from MLP\n",
    "        logits = tf.reshape(logits, shape=(-1, *stochastic_state_shape))\n",
    "        # OneHot distribution over logits\n",
    "        logits_distribution = tfp.distributions.OneHotCategorical(logits)\n",
    "        # Sample from OneHot distribution\n",
    "        sample = tf.cast(logits_distribution.sample(), tf.float32)\n",
    "        # TODO observe logits_distribution.prob(sample) after few iterations\n",
    "        sample += logits_distribution.prob(sample) - tf.stop_gradient(logits_distribution.prob(sample))\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def dream(self, previous_rssm_state: RSSMState, previous_action: tf.Tensor, non_terminal):\n",
    "        \"\"\"\n",
    "        Creates Z^\n",
    "        \"\"\"\n",
    "        # TODO invert terminal states (terminal state = 1 if episode ended, needs to be 0)\n",
    "        # Embedding of concatenation prior z and action (t-1)\n",
    "        state_action_embedding = self.state_action_embedder(tf.concat([previous_rssm_state.stochastic_state_z * non_terminal, previous_action], axis=1))\n",
    "        # TODO Remove Squeeze\n",
    "        # Create h from GRU with old h (t-1) and the embedding\n",
    "        state_action_embedding = tf.reshape(state_action_embedding, shape=(-1, 200, 1))\n",
    "        \n",
    "        hidden_rnn_state = self.rnn(state_action_embedding, previous_rssm_state.hidden_rnn_state * non_terminal)\n",
    "\n",
    "        # Logits created from h (with MLP) to create Z^\n",
    "        prior_logits = self.prior_model(hidden_rnn_state)\n",
    "        # Create Z^\n",
    "        prior_stochastic_state_z = self.sample_stochastic_state(prior_logits)\n",
    "        # Save logits for Z^, Z^ and h\n",
    "        prior_rssm_state = RSSMState(prior_logits, tf.reshape(prior_stochastic_state_z, (-1, stochastic_state_size)), hidden_rnn_state)\n",
    "\n",
    "        return prior_rssm_state\n",
    "\n",
    "    def dreaming_rollout(self, horizon: int, actor: tf.keras.Model, previous_rssm_state: RSSMState):\n",
    "        \"\"\"\n",
    "        Rollout only Z\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def observe(self, encoded_state: tf.Tensor, previous_action: tf.Tensor, previous_non_terminal: tf.Tensor, previous_rssm_state: RSSMState):\n",
    "        \"\"\"\n",
    "        Creates Z' and Z\n",
    "        \"\"\"\n",
    "        # Obtain Z^\n",
    "        prior_rssm_state = self.dream(previous_rssm_state, previous_action, previous_non_terminal)\n",
    "\n",
    "        # concatenates h and the output of our CNN (encoded input frame X)\n",
    "        encoded_state_and_hidden_state = tf.concat([prior_rssm_state.hidden_rnn_state, encoded_state], axis=1)\n",
    "        \n",
    "        # Logits created from concat of h and encoded frame X (with MLP) to create Z\n",
    "        posterior_logits = self.posterior_model(encoded_state_and_hidden_state)\n",
    "        # Create Z\n",
    "        posterior_stochastic_state_z = self.sample_stochastic_state(posterior_logits)\n",
    "        # Saves logits for Z, Z, and h\n",
    "        posterior_rssm_state = RSSMState(posterior_logits, tf.reshape(posterior_stochastic_state_z, (-1, stochastic_state_size)), prior_rssm_state.hidden_rnn_state)\n",
    "\n",
    "        return prior_rssm_state, posterior_rssm_state\n",
    "\n",
    "    def observing_rollout(self, encoded_states: tf.Tensor, actions: tf.Tensor, non_terminals: tf.Tensor, previous_rssm_state: RSSMState):\n",
    "        prior_rssm_states = []\n",
    "        posterior_rssm_states = []\n",
    "\n",
    "        for encoded_state, action, non_terminal in zip(encoded_states, actions, non_terminals):\n",
    "            # TODO remove islandsolution\n",
    "            encoded_state = tf.expand_dims(encoded_state, axis=0)\n",
    "            action = tf.expand_dims(action, axis=0)\n",
    "            non_terminal = tf.expand_dims(non_terminal, axis=0)\n",
    "            #?? 0 if terminal state is reached\n",
    "            previous_action = action * non_terminal\n",
    "            # Z^, Z\n",
    "            prior_rssm_state, posterior_rssm_state = self.observe(encoded_state, previous_action, non_terminal, previous_rssm_state)\n",
    "            \n",
    "            # Save Z^, Z\n",
    "            prior_rssm_states.append(prior_rssm_state)\n",
    "            posterior_rssm_states.append(posterior_rssm_state)\n",
    "\n",
    "            # Z for next iteration    \n",
    "            previous_rssm_state = posterior_rssm_state\n",
    "        prior_rssm_states = RSSMState.from_list(prior_rssm_states)\n",
    "        posterior_rssm_states = RSSMState.from_list(posterior_rssm_states)\n",
    "\n",
    "        return prior_rssm_states, posterior_rssm_states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_log_loss(distribution, target):\n",
    "    \"\"\"\n",
    "    Computes loss for:\n",
    "    - Image log loss(Output decoder, frame timestep t)\n",
    "    - Reward log loss(Output reward network, obtained reward timestep t)\n",
    "    - Discount log loss(Output of discount network, terminal state timestep t)\n",
    "    \"\"\"\n",
    "    return -tf.math.reduce_mean(distribution.log_prob(target))\n",
    "      \n",
    "\n",
    "def compute_kl_loss(prior_rssm_states, posterior_rssm_states, alpha=0.8,):\n",
    "    \"\"\"\n",
    "    alpha: weigh between training the prior toward the representations & regularizing\n",
    "     the representations towards the prior\n",
    "    prior: Z\n",
    "    posterior: Z^\n",
    "    \"\"\"\n",
    "    prior_distribution = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=prior_rssm_states.logits), 1)\n",
    "    posterior_distribution = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=posterior_rssm_states.logits), 1)\n",
    "    \n",
    "    prior_distribution_detached = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=tf.stop_gradient(prior_rssm_states.logits)), 1)\n",
    "    posterior_distribution_detached = tfp.distributions.Independent(tfp.distributions.OneHotCategorical(logits=tf.stop_gradient(posterior_rssm_states.logits)), 1)\n",
    "\n",
    "    # Loss with KL Balancing\n",
    "    return alpha * tf.math.reduce_mean(tfp.distributions.kl_divergence(posterior_distribution_detached, prior_distribution)) + (1-alpha) * tf.math.reduce_mean(tfp.distributions.kl_divergence(posterior_distribution, prior_distribution_detached))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/highway_env/envs/common/observation.py:215: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame.from_records(\n"
     ]
    }
   ],
   "source": [
    "# TODO move hyperparams\n",
    "epochs = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002)\n",
    "\n",
    "buffer = Buffer(batch_size=1)\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 32),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "\n",
    "environment_interactor = EnvironmentInteractor(config, buffer)\n",
    "\n",
    "world_model = WorldModel()\n",
    "rssm = RSSM()\n",
    "\n",
    "models = (\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "wandb.tensorflow.log(tf.summary)\n",
    "\n",
    "for episode in range(100):\n",
    "    environment_interactor.create_trajectories(1000)\n",
    "    data = buffer.sample(batch_size=50, prefetch_size=70)\n",
    "    # Sample from buffer\n",
    "    for sequence in data:\n",
    "        state, next_state, action, reward, non_terminal = sequence[0]\n",
    "\n",
    "            # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        combined_trainable_variables = models[0].trainable_variables\n",
    "        for i in range(len(models)):\n",
    "            if i+1 >= len(models):\n",
    "                break\n",
    "            combined_trainable_variables += models[i+1].trainable_variables\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoded_state = world_model.encoder(state)\n",
    "            initial_rssm_state = RSSMState()\n",
    "            prior_rssm_states, posterior_rssm_states = rssm.observing_rollout(encoded_state, action, non_terminal, initial_rssm_state)\n",
    "            hidden_state_h_and_stochastic_state_z = tf.concat([posterior_rssm_states.stochastic_state_z, posterior_rssm_states.hidden_rnn_state], axis=-1)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            hidden_state_h_and_stochastic_state_z = tf.reshape(hidden_state_h_and_stochastic_state_z, (-1,stochastic_state_size + hidden_unit_size))\n",
    "\n",
    "            decoder_logits = world_model.decoder(hidden_state_h_and_stochastic_state_z)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            decoder_logits = tf.reshape(decoder_logits, (-1, image_shape[0], image_shape[1], image_shape[2]))\n",
    "\n",
    "            decoder_distribution = tfp.distributions.Independent(tfp.distributions.Normal(decoder_logits, 1))\n",
    "            reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "            reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "            discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "            discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "\n",
    "            image_log_loss = compute_log_loss(decoder_distribution, state)\n",
    "            reward_log_loss = compute_log_loss(reward_distribution, reward)\n",
    "            discount_log_loss = compute_log_loss(discount_distribution, non_terminal)\n",
    "            kl_loss = compute_kl_loss(prior_rssm_states, posterior_rssm_states)\n",
    "\n",
    "\n",
    "\n",
    "            loss = image_log_loss + reward_log_loss + discount_log_loss + kl_loss\n",
    "            \n",
    "            print(f\"Image Log Loss: {image_log_loss} Reward Log Loss: {reward_log_loss} Discount Log Loss {discount_log_loss} KL Loss {kl_loss}\")\n",
    "            wandb.log({\"Image Log Loss\": image_log_loss, \"Reward Log Loss\": reward_log_loss, \"Discount Log Loss\": discount_log_loss, \"KL Loss\": kl_loss, \"Loss\": loss})\n",
    "           \n",
    "            # TODO maybe in Gradienttape??\n",
    "            gradients = tape.gradient(loss, combined_trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, combined_trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/highway_env/envs/common/observation.py:215: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(pd.DataFrame.from_records(\n",
      "2022-08-03 14:26:03.160121: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling _Independent.__init__ (from tensorflow_probability.python.distributions.independent) with reinterpreted_batch_ndims=None is deprecated and will be removed after 2022-03-01.\n",
      "Instructions for updating:\n",
      "Please pass an integer value for `reinterpreted_batch_ndims`. The current behavior corresponds to `reinterpreted_batch_ndims=tf.size(distribution.batch_shape_tensor()) - 1`.\n",
      "Image Log Loss: 33639716.0 Reward Log Loss: 121.02690124511719 Discount Log Loss 69.5711898803711 KL Loss 1.9140706062316895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error resolved after 0:00:07.855914, resuming normal operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Log Loss: 33607540.0 Reward Log Loss: 122.5135726928711 Discount Log Loss 69.46884155273438 KL Loss 1.8781226873397827\n",
      "Image Log Loss: 33624976.0 Reward Log Loss: 127.441650390625 Discount Log Loss 69.17655181884766 KL Loss 2.0242600440979004\n",
      "Image Log Loss: 33649492.0 Reward Log Loss: 120.8692855834961 Discount Log Loss 69.0105209350586 KL Loss 2.4363396167755127\n",
      "Image Log Loss: 33595752.0 Reward Log Loss: 122.66472625732422 Discount Log Loss 68.3469009399414 KL Loss 3.085357904434204\n",
      "Image Log Loss: 33605040.0 Reward Log Loss: 122.35136413574219 Discount Log Loss 68.37353515625 KL Loss 3.7239763736724854\n",
      "Image Log Loss: 33650184.0 Reward Log Loss: 117.84175872802734 Discount Log Loss 68.11650848388672 KL Loss 4.330898284912109\n",
      "Image Log Loss: 33607300.0 Reward Log Loss: 120.94953155517578 Discount Log Loss 68.0348129272461 KL Loss 4.690727710723877\n",
      "Image Log Loss: 33624748.0 Reward Log Loss: 124.30204772949219 Discount Log Loss 67.79742431640625 KL Loss 4.931230545043945\n",
      "Image Log Loss: 33599656.0 Reward Log Loss: 119.65599822998047 Discount Log Loss 67.18936157226562 KL Loss 5.112277030944824\n",
      "Image Log Loss: 33663168.0 Reward Log Loss: 119.44427490234375 Discount Log Loss 66.78308868408203 KL Loss 5.259124755859375\n",
      "Image Log Loss: 33592924.0 Reward Log Loss: 118.7265853881836 Discount Log Loss 66.77315521240234 KL Loss 5.389959335327148\n",
      "Image Log Loss: 33613544.0 Reward Log Loss: 118.71643829345703 Discount Log Loss 66.24317932128906 KL Loss 5.388617038726807\n",
      "Image Log Loss: 33618632.0 Reward Log Loss: 119.86825561523438 Discount Log Loss 66.29107666015625 KL Loss 5.478832244873047\n",
      "Image Log Loss: 33593684.0 Reward Log Loss: 114.19055938720703 Discount Log Loss 65.00482177734375 KL Loss 5.5458784103393555\n",
      "Image Log Loss: 33643284.0 Reward Log Loss: 119.20030975341797 Discount Log Loss 64.8718490600586 KL Loss 5.4723639488220215\n",
      "Image Log Loss: 33598352.0 Reward Log Loss: 114.66930389404297 Discount Log Loss 65.28028869628906 KL Loss 5.563727378845215\n",
      "Image Log Loss: 33657524.0 Reward Log Loss: 118.63885498046875 Discount Log Loss 64.96456909179688 KL Loss 5.55945348739624\n",
      "Image Log Loss: 33635856.0 Reward Log Loss: 116.73081970214844 Discount Log Loss 63.45246124267578 KL Loss 5.532786846160889\n",
      "Image Log Loss: 33654352.0 Reward Log Loss: 115.61851501464844 Discount Log Loss 63.682579040527344 KL Loss 5.549480438232422\n",
      "Image Log Loss: 33610664.0 Reward Log Loss: 113.14463806152344 Discount Log Loss 64.30335235595703 KL Loss 5.6004638671875\n",
      "Image Log Loss: 33624020.0 Reward Log Loss: 111.33879089355469 Discount Log Loss 62.98301315307617 KL Loss 5.598225116729736\n",
      "Image Log Loss: 33618832.0 Reward Log Loss: 109.84493255615234 Discount Log Loss 62.206722259521484 KL Loss 5.696771621704102\n",
      "Image Log Loss: 33609840.0 Reward Log Loss: 111.98304748535156 Discount Log Loss 62.599918365478516 KL Loss 5.644533634185791\n",
      "Image Log Loss: 33654396.0 Reward Log Loss: 109.43140411376953 Discount Log Loss 61.387046813964844 KL Loss 5.729727745056152\n",
      "Image Log Loss: 33657580.0 Reward Log Loss: 112.13606262207031 Discount Log Loss 61.37580108642578 KL Loss 5.743091583251953\n",
      "Image Log Loss: 33571936.0 Reward Log Loss: 108.33544921875 Discount Log Loss 61.847572326660156 KL Loss 5.79305362701416\n",
      "Image Log Loss: 33643204.0 Reward Log Loss: 106.14102172851562 Discount Log Loss 59.5143928527832 KL Loss 5.8043389320373535\n",
      "Image Log Loss: 33583364.0 Reward Log Loss: 109.8871078491211 Discount Log Loss 61.033966064453125 KL Loss 5.78984260559082\n",
      "Image Log Loss: 33592024.0 Reward Log Loss: 109.71672821044922 Discount Log Loss 60.60745620727539 KL Loss 5.829295635223389\n",
      "Image Log Loss: 33588188.0 Reward Log Loss: 108.64579772949219 Discount Log Loss 60.724700927734375 KL Loss 5.8175811767578125\n",
      "Image Log Loss: 33596692.0 Reward Log Loss: 107.00289916992188 Discount Log Loss 58.51945877075195 KL Loss 5.810647487640381\n",
      "Image Log Loss: 33601804.0 Reward Log Loss: 105.55945587158203 Discount Log Loss 59.2310905456543 KL Loss 5.836544036865234\n",
      "Image Log Loss: 33623092.0 Reward Log Loss: 103.25370025634766 Discount Log Loss 58.73154830932617 KL Loss 5.8570427894592285\n",
      "Image Log Loss: 33576012.0 Reward Log Loss: 102.22525024414062 Discount Log Loss 55.45406723022461 KL Loss 5.844124794006348\n",
      "Image Log Loss: 33599512.0 Reward Log Loss: 104.70303344726562 Discount Log Loss 56.218544006347656 KL Loss 5.8002610206604\n",
      "Image Log Loss: 33573564.0 Reward Log Loss: 103.63068389892578 Discount Log Loss 56.31389617919922 KL Loss 5.769270896911621\n",
      "Image Log Loss: 33553856.0 Reward Log Loss: 100.91724395751953 Discount Log Loss 53.203712463378906 KL Loss 5.749639987945557\n",
      "Image Log Loss: 33574616.0 Reward Log Loss: 102.21432495117188 Discount Log Loss 54.0705451965332 KL Loss 5.695314407348633\n",
      "Image Log Loss: 33633544.0 Reward Log Loss: 102.47813415527344 Discount Log Loss 52.3276252746582 KL Loss 5.643374443054199\n",
      "Image Log Loss: 33560204.0 Reward Log Loss: 101.76919555664062 Discount Log Loss 56.200435638427734 KL Loss 5.600554466247559\n",
      "Image Log Loss: 33590264.0 Reward Log Loss: 100.45281982421875 Discount Log Loss 52.321266174316406 KL Loss 5.510773181915283\n",
      "Image Log Loss: 33605608.0 Reward Log Loss: 101.73013305664062 Discount Log Loss 48.98747634887695 KL Loss 5.369356155395508\n",
      "Image Log Loss: 33622736.0 Reward Log Loss: 101.0855941772461 Discount Log Loss 48.564151763916016 KL Loss 5.186295032501221\n",
      "Image Log Loss: 33525560.0 Reward Log Loss: 99.56658172607422 Discount Log Loss 46.351009368896484 KL Loss 4.869156360626221\n",
      "Image Log Loss: 33574696.0 Reward Log Loss: 104.19804382324219 Discount Log Loss 38.75736618041992 KL Loss 3.6564745903015137\n",
      "Image Log Loss: 33554836.0 Reward Log Loss: 109.53082275390625 Discount Log Loss 34.59569549560547 KL Loss 2.883730173110962\n",
      "Image Log Loss: 33635792.0 Reward Log Loss: 109.89250183105469 Discount Log Loss 36.0218620300293 KL Loss 2.4510674476623535\n",
      "Image Log Loss: 33584992.0 Reward Log Loss: 112.43588256835938 Discount Log Loss 44.84615707397461 KL Loss 2.11576771736145\n",
      "Image Log Loss: 33583552.0 Reward Log Loss: 108.85950469970703 Discount Log Loss 34.197147369384766 KL Loss 1.8110191822052002\n",
      "Image Log Loss: 33570208.0 Reward Log Loss: 108.32612609863281 Discount Log Loss 37.102455139160156 KL Loss 1.5453697443008423\n",
      "Image Log Loss: 33581136.0 Reward Log Loss: 106.72855377197266 Discount Log Loss 44.30543518066406 KL Loss 1.349759578704834\n",
      "Image Log Loss: 33595320.0 Reward Log Loss: 104.08134460449219 Discount Log Loss 35.98561096191406 KL Loss 1.1795238256454468\n",
      "Image Log Loss: 33568988.0 Reward Log Loss: 102.130859375 Discount Log Loss 35.449974060058594 KL Loss 0.9865173101425171\n",
      "Image Log Loss: 33574024.0 Reward Log Loss: 98.64393615722656 Discount Log Loss 22.048828125 KL Loss 0.8271269798278809\n",
      "Image Log Loss: 33588748.0 Reward Log Loss: 100.02197265625 Discount Log Loss 38.856651306152344 KL Loss 0.703422486782074\n",
      "Image Log Loss: 33591144.0 Reward Log Loss: 98.6336898803711 Discount Log Loss 33.96648025512695 KL Loss 0.6101030707359314\n",
      "Image Log Loss: 33569880.0 Reward Log Loss: 98.63887023925781 Discount Log Loss 38.11802673339844 KL Loss 0.5173697471618652\n",
      "Image Log Loss: 33555040.0 Reward Log Loss: 97.25090789794922 Discount Log Loss 33.08917236328125 KL Loss 0.4526190161705017\n",
      "Image Log Loss: 33595488.0 Reward Log Loss: 97.49993133544922 Discount Log Loss 32.68575668334961 KL Loss 0.3881703317165375\n",
      "Image Log Loss: 33543384.0 Reward Log Loss: 97.17961883544922 Discount Log Loss 37.08171844482422 KL Loss 0.30771854519844055\n",
      "Image Log Loss: 33585464.0 Reward Log Loss: 98.40621185302734 Discount Log Loss 36.758506774902344 KL Loss 0.2843274176120758\n",
      "Image Log Loss: 33545580.0 Reward Log Loss: 95.9140396118164 Discount Log Loss 21.9649658203125 KL Loss 0.2188439518213272\n",
      "Image Log Loss: 33595924.0 Reward Log Loss: 96.86034393310547 Discount Log Loss 36.161766052246094 KL Loss 0.17627036571502686\n",
      "Image Log Loss: 33526408.0 Reward Log Loss: 96.7074966430664 Discount Log Loss 31.0190486907959 KL Loss 0.15147016942501068\n",
      "Image Log Loss: 33542326.0 Reward Log Loss: 97.1011734008789 Discount Log Loss 35.617374420166016 KL Loss 0.1309780776500702\n",
      "Image Log Loss: 33575948.0 Reward Log Loss: 97.5240707397461 Discount Log Loss 35.35417556762695 KL Loss 0.11666716635227203\n",
      "Image Log Loss: 33539722.0 Reward Log Loss: 95.93928527832031 Discount Log Loss 30.239765167236328 KL Loss 0.08979432284832001\n",
      "Image Log Loss: 33510502.0 Reward Log Loss: 96.38455963134766 Discount Log Loss 34.86787033081055 KL Loss 0.07586592435836792\n",
      "Image Log Loss: 33490392.0 Reward Log Loss: 94.4879379272461 Discount Log Loss 29.821002960205078 KL Loss 0.06186518445611\n",
      "Image Log Loss: 33498040.0 Reward Log Loss: 95.34184265136719 Discount Log Loss 34.45329284667969 KL Loss 0.0548577643930912\n",
      "Image Log Loss: 33550720.0 Reward Log Loss: 96.31866455078125 Discount Log Loss 39.05463790893555 KL Loss 0.05079667642712593\n",
      "Image Log Loss: 33538060.0 Reward Log Loss: 96.53996276855469 Discount Log Loss 38.844722747802734 KL Loss 0.04253070801496506\n",
      "Image Log Loss: 33585480.0 Reward Log Loss: 94.84888458251953 Discount Log Loss 24.526832580566406 KL Loss 0.044928744435310364\n",
      "Image Log Loss: 33561948.0 Reward Log Loss: 93.78561401367188 Discount Log Loss 19.78403663635254 KL Loss 0.04148766025900841\n",
      "Image Log Loss: 33512374.0 Reward Log Loss: 96.48605346679688 Discount Log Loss 47.667293548583984 KL Loss 0.03987643122673035\n",
      "Image Log Loss: 33516498.0 Reward Log Loss: 94.84535217285156 Discount Log Loss 28.954669952392578 KL Loss 0.03684142231941223\n",
      "Image Log Loss: 33558524.0 Reward Log Loss: 96.29777526855469 Discount Log Loss 38.0795783996582 KL Loss 0.03683283179998398\n",
      "Image Log Loss: 33540698.0 Reward Log Loss: 94.30206298828125 Discount Log Loss 28.82953453063965 KL Loss 0.03597564995288849\n",
      "Image Log Loss: 33529350.0 Reward Log Loss: 95.23717498779297 Discount Log Loss 33.31224060058594 KL Loss 0.03333737328648567\n",
      "Image Log Loss: 33549044.0 Reward Log Loss: 94.70590209960938 Discount Log Loss 28.73041343688965 KL Loss 0.03182632103562355\n",
      "Image Log Loss: 33554396.0 Reward Log Loss: 93.66548156738281 Discount Log Loss 24.19251251220703 KL Loss 0.029510226100683212\n",
      "Image Log Loss: 33515188.0 Reward Log Loss: 95.69725799560547 Discount Log Loss 37.6031379699707 KL Loss 0.0290111992508173\n",
      "Image Log Loss: 33536072.0 Reward Log Loss: 94.18519592285156 Discount Log Loss 24.109813690185547 KL Loss 0.02858751453459263\n",
      "Image Log Loss: 33481124.0 Reward Log Loss: 92.57762908935547 Discount Log Loss 19.57727813720703 KL Loss 0.02469288557767868\n",
      "Image Log Loss: 33471276.0 Reward Log Loss: 94.19395446777344 Discount Log Loss 28.47016143798828 KL Loss 0.02248692512512207\n",
      "Image Log Loss: 33519088.0 Reward Log Loss: 94.46334075927734 Discount Log Loss 23.884780883789062 KL Loss 0.020853351801633835\n",
      "Image Log Loss: 33548162.0 Reward Log Loss: 94.87256622314453 Discount Log Loss 28.34786605834961 KL Loss 0.019577806815505028\n",
      "Image Log Loss: 33510364.0 Reward Log Loss: 95.42096710205078 Discount Log Loss 37.479583740234375 KL Loss 0.016482258215546608\n",
      "Image Log Loss: 33552712.0 Reward Log Loss: 94.67606353759766 Discount Log Loss 32.86455154418945 KL Loss 0.01474450808018446\n",
      "Image Log Loss: 33537252.0 Reward Log Loss: 95.55377960205078 Discount Log Loss 32.84328079223633 KL Loss 0.015268460847437382\n",
      "Image Log Loss: 33525332.0 Reward Log Loss: 94.49686431884766 Discount Log Loss 28.179697036743164 KL Loss 0.012429616414010525\n",
      "Image Log Loss: 33530060.0 Reward Log Loss: 93.99368286132812 Discount Log Loss 23.495241165161133 KL Loss 0.011471583507955074\n",
      "Image Log Loss: 33549148.0 Reward Log Loss: 93.67072296142578 Discount Log Loss 23.444047927856445 KL Loss 0.011699707247316837\n",
      "Image Log Loss: 33503852.0 Reward Log Loss: 94.09984588623047 Discount Log Loss 23.386432647705078 KL Loss 0.01166903879493475\n",
      "Image Log Loss: 33482156.0 Reward Log Loss: 95.50918579101562 Discount Log Loss 42.28053665161133 KL Loss 0.010521387681365013\n",
      "Image Log Loss: 33514434.0 Reward Log Loss: 94.8504867553711 Discount Log Loss 32.794166564941406 KL Loss 0.011301628313958645\n",
      "Image Log Loss: 33494988.0 Reward Log Loss: 97.2412338256836 Discount Log Loss 51.79335021972656 KL Loss 0.009741462767124176\n",
      "Image Log Loss: 33542476.0 Reward Log Loss: 94.43513488769531 Discount Log Loss 32.741004943847656 KL Loss 0.009138626046478748\n",
      "Image Log Loss: 33502604.0 Reward Log Loss: 94.4444808959961 Discount Log Loss 28.014013290405273 KL Loss 0.009470016695559025\n",
      "Image Log Loss: 33508634.0 Reward Log Loss: 94.47481536865234 Discount Log Loss 32.67448806762695 KL Loss 0.00854353979229927\n",
      "Image Log Loss: 33519056.0 Reward Log Loss: 93.79576110839844 Discount Log Loss 28.012514114379883 KL Loss 0.010226165875792503\n",
      "Image Log Loss: 33476252.0 Reward Log Loss: 94.83380126953125 Discount Log Loss 32.6250114440918 KL Loss 0.010603368282318115\n",
      "Image Log Loss: 33530416.0 Reward Log Loss: 94.2795639038086 Discount Log Loss 28.018465042114258 KL Loss 0.007654003798961639\n",
      "Image Log Loss: 33542538.0 Reward Log Loss: 94.65718841552734 Discount Log Loss 28.021385192871094 KL Loss 0.009376471862196922\n",
      "Image Log Loss: 33477018.0 Reward Log Loss: 97.2190170288086 Discount Log Loss 50.805973052978516 KL Loss 0.008376059122383595\n",
      "Image Log Loss: 33463662.0 Reward Log Loss: 94.4361801147461 Discount Log Loss 32.559993743896484 KL Loss 0.0072571467608213425\n",
      "Image Log Loss: 33493908.0 Reward Log Loss: 93.51294708251953 Discount Log Loss 23.587783813476562 KL Loss 0.00670621870085597\n",
      "Image Log Loss: 33495508.0 Reward Log Loss: 94.77033996582031 Discount Log Loss 36.99872970581055 KL Loss 0.008470963686704636\n",
      "Image Log Loss: 33510036.0 Reward Log Loss: 94.85096740722656 Discount Log Loss 28.100486755371094 KL Loss 0.007691632024943829\n",
      "Image Log Loss: 33467752.0 Reward Log Loss: 95.05716705322266 Discount Log Loss 32.528343200683594 KL Loss 0.0075969817116856575\n",
      "Image Log Loss: 33480614.0 Reward Log Loss: 96.67103576660156 Discount Log Loss 45.72635269165039 KL Loss 0.00977722741663456\n",
      "Image Log Loss: 33464094.0 Reward Log Loss: 94.01245880126953 Discount Log Loss 28.15581703186035 KL Loss 0.007349203806370497\n",
      "Image Log Loss: 33508070.0 Reward Log Loss: 95.13751983642578 Discount Log Loss 32.523128509521484 KL Loss 0.008016087114810944\n",
      "Image Log Loss: 33515892.0 Reward Log Loss: 93.94949340820312 Discount Log Loss 28.195964813232422 KL Loss 0.006490249652415514\n",
      "Image Log Loss: 33441702.0 Reward Log Loss: 95.0293960571289 Discount Log Loss 32.521217346191406 KL Loss 0.00892223697155714\n",
      "Image Log Loss: 33481722.0 Reward Log Loss: 95.09180450439453 Discount Log Loss 36.832828521728516 KL Loss 0.007764179725199938\n",
      "Image Log Loss: 33435474.0 Reward Log Loss: 94.77438354492188 Discount Log Loss 32.518245697021484 KL Loss 0.0064963544718921185\n",
      "Image Log Loss: 33430338.0 Reward Log Loss: 95.2194595336914 Discount Log Loss 36.81471633911133 KL Loss 0.00614533107727766\n",
      "Image Log Loss: 33437428.0 Reward Log Loss: 94.41470336914062 Discount Log Loss 28.2304630279541 KL Loss 0.0068928832188248634\n",
      "Image Log Loss: 33456468.0 Reward Log Loss: 95.391845703125 Discount Log Loss 32.51496887207031 KL Loss 0.006734825670719147\n",
      "Image Log Loss: 33443096.0 Reward Log Loss: 95.73391723632812 Discount Log Loss 41.089481353759766 KL Loss 0.006440198980271816\n",
      "Image Log Loss: 33460102.0 Reward Log Loss: 96.4618148803711 Discount Log Loss 49.62836837768555 KL Loss 0.006678585894405842\n",
      "Image Log Loss: 33424102.0 Reward Log Loss: 92.64665985107422 Discount Log Loss 19.78644561767578 KL Loss 0.00590120954439044\n",
      "Image Log Loss: 33491422.0 Reward Log Loss: 92.5826187133789 Discount Log Loss 19.8076114654541 KL Loss 0.005925576668232679\n",
      "Image Log Loss: 33436820.0 Reward Log Loss: 95.84701538085938 Discount Log Loss 36.77190017700195 KL Loss 0.006464297883212566\n",
      "Image Log Loss: 33473944.0 Reward Log Loss: 94.45421600341797 Discount Log Loss 28.244596481323242 KL Loss 0.006074583623558283\n",
      "Image Log Loss: 33448794.0 Reward Log Loss: 94.3186264038086 Discount Log Loss 28.218944549560547 KL Loss 0.006069957744330168\n",
      "Image Log Loss: 33470820.0 Reward Log Loss: 94.42607116699219 Discount Log Loss 32.496761322021484 KL Loss 0.006166477221995592\n",
      "Image Log Loss: 33406666.0 Reward Log Loss: 94.65927124023438 Discount Log Loss 28.157434463500977 KL Loss 0.005767905153334141\n",
      "Image Log Loss: 33448212.0 Reward Log Loss: 95.07763671875 Discount Log Loss 36.852848052978516 KL Loss 0.00565954577177763\n",
      "Image Log Loss: 33480400.0 Reward Log Loss: 94.27363586425781 Discount Log Loss 28.1035099029541 KL Loss 0.005625017918646336\n",
      "Image Log Loss: 33449880.0 Reward Log Loss: 94.565185546875 Discount Log Loss 32.48847579956055 KL Loss 0.0054371412843465805\n",
      "Image Log Loss: 33453066.0 Reward Log Loss: 95.98472595214844 Discount Log Loss 36.92144012451172 KL Loss 0.005398534703999758\n",
      "Image Log Loss: 33445748.0 Reward Log Loss: 95.07638549804688 Discount Log Loss 32.49073028564453 KL Loss 0.006591740995645523\n",
      "Image Log Loss: 33451024.0 Reward Log Loss: 94.53083801269531 Discount Log Loss 32.492347717285156 KL Loss 0.005475135985761881\n",
      "Image Log Loss: 33478968.0 Reward Log Loss: 94.10812377929688 Discount Log Loss 32.49297332763672 KL Loss 0.005318549927324057\n",
      "Image Log Loss: 33453668.0 Reward Log Loss: 95.994873046875 Discount Log Loss 41.42424774169922 KL Loss 0.005884125828742981\n",
      "Image Log Loss: 33417072.0 Reward Log Loss: 94.18383026123047 Discount Log Loss 28.035362243652344 KL Loss 0.004534048959612846\n",
      "Image Log Loss: 33446694.0 Reward Log Loss: 94.35836029052734 Discount Log Loss 28.037981033325195 KL Loss 0.0053714849054813385\n",
      "Image Log Loss: 33439808.0 Reward Log Loss: 95.58147430419922 Discount Log Loss 41.39369201660156 KL Loss 0.004920504055917263\n",
      "Image Log Loss: 33478046.0 Reward Log Loss: 94.55245971679688 Discount Log Loss 28.045848846435547 KL Loss 0.0048247878439724445\n",
      "Image Log Loss: 33451162.0 Reward Log Loss: 95.19515991210938 Discount Log Loss 36.91801834106445 KL Loss 0.004552289377897978\n",
      "Image Log Loss: 33419856.0 Reward Log Loss: 93.79969024658203 Discount Log Loss 28.063072204589844 KL Loss 0.006000358611345291\n",
      "Image Log Loss: 33411458.0 Reward Log Loss: 95.0699691772461 Discount Log Loss 32.48273468017578 KL Loss 0.005833174102008343\n",
      "Image Log Loss: 33436698.0 Reward Log Loss: 94.16710662841797 Discount Log Loss 28.073209762573242 KL Loss 0.005792290437966585\n",
      "Image Log Loss: 33384040.0 Reward Log Loss: 93.92052459716797 Discount Log Loss 28.07134437561035 KL Loss 0.006088040769100189\n",
      "Image Log Loss: 33397122.0 Reward Log Loss: 96.00035858154297 Discount Log Loss 50.160526275634766 KL Loss 0.005759936757385731\n",
      "Image Log Loss: 33399232.0 Reward Log Loss: 96.2267074584961 Discount Log Loss 45.678321838378906 KL Loss 0.005046178586781025\n",
      "Image Log Loss: 33443070.0 Reward Log Loss: 94.2590560913086 Discount Log Loss 23.763952255249023 KL Loss 0.005247978959232569\n",
      "Image Log Loss: 33344368.0 Reward Log Loss: 95.426025390625 Discount Log Loss 41.165096282958984 KL Loss 0.004643190652132034\n",
      "Image Log Loss: 33424360.0 Reward Log Loss: 95.65116119384766 Discount Log Loss 36.801185607910156 KL Loss 0.0050389645621180534\n",
      "Image Log Loss: 33450896.0 Reward Log Loss: 94.35704040527344 Discount Log Loss 28.227069854736328 KL Loss 0.004165284335613251\n",
      "Image Log Loss: 33402476.0 Reward Log Loss: 94.453369140625 Discount Log Loss 28.254844665527344 KL Loss 0.004702101927250624\n",
      "Image Log Loss: 33366564.0 Reward Log Loss: 94.15373992919922 Discount Log Loss 32.5128059387207 KL Loss 0.00426382664591074\n",
      "Image Log Loss: 33409124.0 Reward Log Loss: 95.20674896240234 Discount Log Loss 32.51441955566406 KL Loss 0.004904972855001688\n",
      "Image Log Loss: 33435282.0 Reward Log Loss: 95.61454772949219 Discount Log Loss 32.514495849609375 KL Loss 0.004231235012412071\n",
      "Image Log Loss: 33383174.0 Reward Log Loss: 93.13971710205078 Discount Log Loss 24.03197479248047 KL Loss 0.004438331816345453\n",
      "Image Log Loss: 33389872.0 Reward Log Loss: 94.1429672241211 Discount Log Loss 23.988500595092773 KL Loss 0.004611126147210598\n",
      "Image Log Loss: 33387582.0 Reward Log Loss: 94.64904022216797 Discount Log Loss 28.203956604003906 KL Loss 0.003984309267252684\n",
      "Image Log Loss: 33374104.0 Reward Log Loss: 93.19902038574219 Discount Log Loss 19.501550674438477 KL Loss 0.0048280865885317326\n",
      "Image Log Loss: 33394252.0 Reward Log Loss: 95.15387725830078 Discount Log Loss 32.48215103149414 KL Loss 0.00454148231074214\n",
      "Image Log Loss: 33413506.0 Reward Log Loss: 95.10629272460938 Discount Log Loss 32.48603057861328 KL Loss 0.003954324405640364\n",
      "Image Log Loss: 33400162.0 Reward Log Loss: 94.3912124633789 Discount Log Loss 28.017822265625 KL Loss 0.004708985798060894\n",
      "Image Log Loss: 33385600.0 Reward Log Loss: 93.46612548828125 Discount Log Loss 27.987260818481445 KL Loss 0.004139019176363945\n",
      "Image Log Loss: 33379720.0 Reward Log Loss: 94.67023468017578 Discount Log Loss 27.961530685424805 KL Loss 0.004274630919098854\n",
      "Image Log Loss: 33388408.0 Reward Log Loss: 94.64961242675781 Discount Log Loss 32.553375244140625 KL Loss 0.0036631599068641663\n",
      "Image Log Loss: 33337694.0 Reward Log Loss: 95.69110107421875 Discount Log Loss 46.520809173583984 KL Loss 0.0047071282751858234\n",
      "Image Log Loss: 33389648.0 Reward Log Loss: 95.00764465332031 Discount Log Loss 32.57786178588867 KL Loss 0.004740098025649786\n",
      "Image Log Loss: 33374576.0 Reward Log Loss: 94.05318450927734 Discount Log Loss 23.266164779663086 KL Loss 0.0037516651209443808\n",
      "Image Log Loss: 33367452.0 Reward Log Loss: 93.97111511230469 Discount Log Loss 27.916330337524414 KL Loss 0.0038023488596081734\n",
      "Image Log Loss: 33412948.0 Reward Log Loss: 92.51821899414062 Discount Log Loss 13.883600234985352 KL Loss 0.0038528910372406244\n",
      "Image Log Loss: 33392066.0 Reward Log Loss: 93.4278335571289 Discount Log Loss 27.90103530883789 KL Loss 0.004039593506604433\n",
      "Image Log Loss: 33320102.0 Reward Log Loss: 94.19005584716797 Discount Log Loss 23.152864456176758 KL Loss 0.0037889587692916393\n",
      "Image Log Loss: 33362054.0 Reward Log Loss: 94.642333984375 Discount Log Loss 32.6638069152832 KL Loss 0.0036625205539166927\n",
      "Image Log Loss: 33393418.0 Reward Log Loss: 95.05262756347656 Discount Log Loss 32.68647766113281 KL Loss 0.003686015261337161\n",
      "Image Log Loss: 33389110.0 Reward Log Loss: 95.2312240600586 Discount Log Loss 37.5202751159668 KL Loss 0.004897110629826784\n",
      "Image Log Loss: 33343104.0 Reward Log Loss: 92.89422607421875 Discount Log Loss 18.237916946411133 KL Loss 0.0033129947260022163\n",
      "Image Log Loss: 33333598.0 Reward Log Loss: 94.40276336669922 Discount Log Loss 32.710655212402344 KL Loss 0.003915918059647083\n",
      "Image Log Loss: 33307558.0 Reward Log Loss: 96.02607727050781 Discount Log Loss 47.237735748291016 KL Loss 0.004106150474399328\n",
      "Image Log Loss: 33351214.0 Reward Log Loss: 92.93114471435547 Discount Log Loss 18.2385196685791 KL Loss 0.0035298080183565617\n",
      "Image Log Loss: 33336018.0 Reward Log Loss: 95.93051147460938 Discount Log Loss 42.30189514160156 KL Loss 0.004869191441684961\n",
      "Image Log Loss: 33367204.0 Reward Log Loss: 92.86495208740234 Discount Log Loss 18.306961059570312 KL Loss 0.004014753270894289\n",
      "Image Log Loss: 33311076.0 Reward Log Loss: 95.4841537475586 Discount Log Loss 37.42070007324219 KL Loss 0.004599182400852442\n",
      "Image Log Loss: 33334882.0 Reward Log Loss: 95.51043701171875 Discount Log Loss 42.133262634277344 KL Loss 0.0038598044775426388\n",
      "Image Log Loss: 33326718.0 Reward Log Loss: 95.81433868408203 Discount Log Loss 37.31291198730469 KL Loss 0.005158646497875452\n",
      "Image Log Loss: 33336158.0 Reward Log Loss: 95.85026550292969 Discount Log Loss 37.23629379272461 KL Loss 0.005409538745880127\n",
      "Image Log Loss: 33346408.0 Reward Log Loss: 93.54271697998047 Discount Log Loss 23.317102432250977 KL Loss 0.0033188597299158573\n",
      "Image Log Loss: 33344446.0 Reward Log Loss: 93.95159912109375 Discount Log Loss 27.952714920043945 KL Loss 0.0032994875218719244\n",
      "Image Log Loss: 33313838.0 Reward Log Loss: 94.27874755859375 Discount Log Loss 27.97256851196289 KL Loss 0.003384619252756238\n",
      "Image Log Loss: 33317048.0 Reward Log Loss: 95.25247955322266 Discount Log Loss 37.04145812988281 KL Loss 0.0036235281731933355\n",
      "Image Log Loss: 33320446.0 Reward Log Loss: 97.08052062988281 Discount Log Loss 46.007904052734375 KL Loss 0.0037259443197399378\n",
      "Image Log Loss: 33302394.0 Reward Log Loss: 95.07067108154297 Discount Log Loss 32.50491714477539 KL Loss 0.0034976317547261715\n",
      "Image Log Loss: 33276672.0 Reward Log Loss: 96.35786437988281 Discount Log Loss 50.153987884521484 KL Loss 0.003792152041569352\n",
      "Image Log Loss: 33287276.0 Reward Log Loss: 94.24451446533203 Discount Log Loss 28.16553497314453 KL Loss 0.003943255636841059\n",
      "Image Log Loss: 33319152.0 Reward Log Loss: 93.51568603515625 Discount Log Loss 23.92658042907715 KL Loss 0.0033368882723152637\n",
      "Image Log Loss: 33331758.0 Reward Log Loss: 93.90593719482422 Discount Log Loss 23.989931106567383 KL Loss 0.0031024611089378595\n",
      "Image Log Loss: 33338426.0 Reward Log Loss: 94.55789184570312 Discount Log Loss 32.536869049072266 KL Loss 0.004078236874192953\n",
      "Image Log Loss: 33261604.0 Reward Log Loss: 95.7127456665039 Discount Log Loss 41.05852508544922 KL Loss 0.004183518700301647\n",
      "Image Log Loss: 33296546.0 Reward Log Loss: 93.40167236328125 Discount Log Loss 28.291715621948242 KL Loss 0.0030585816130042076\n",
      "Image Log Loss: 33288780.0 Reward Log Loss: 94.79015350341797 Discount Log Loss 28.291790008544922 KL Loss 0.003173337783664465\n",
      "Image Log Loss: 33296088.0 Reward Log Loss: 93.71989440917969 Discount Log Loss 28.27725601196289 KL Loss 0.004433427471667528\n",
      "Image Log Loss: 33299974.0 Reward Log Loss: 95.31371307373047 Discount Log Loss 36.789798736572266 KL Loss 0.003065211232751608\n",
      "Image Log Loss: 33237196.0 Reward Log Loss: 96.79572296142578 Discount Log Loss 49.63664245605469 KL Loss 0.0034219147637486458\n",
      "Image Log Loss: 33286846.0 Reward Log Loss: 93.51321411132812 Discount Log Loss 23.984651565551758 KL Loss 0.0034528649412095547\n",
      "Image Log Loss: 33242316.0 Reward Log Loss: 95.3442153930664 Discount Log Loss 36.78459167480469 KL Loss 0.0035520694218575954\n",
      "Image Log Loss: 33285232.0 Reward Log Loss: 93.86019897460938 Discount Log Loss 23.97835922241211 KL Loss 0.003417791798710823\n",
      "Image Log Loss: 33275438.0 Reward Log Loss: 94.70023345947266 Discount Log Loss 32.50862503051758 KL Loss 0.003314565634354949\n",
      "Image Log Loss: 33250324.0 Reward Log Loss: 95.3125 Discount Log Loss 32.503780364990234 KL Loss 0.0038370974361896515\n",
      "Image Log Loss: 33236212.0 Reward Log Loss: 94.07198333740234 Discount Log Loss 28.187780380249023 KL Loss 0.0033935890533030033\n",
      "Image Log Loss: 33318658.0 Reward Log Loss: 94.49861145019531 Discount Log Loss 28.162151336669922 KL Loss 0.00400422653183341\n",
      "Image Log Loss: 33257792.0 Reward Log Loss: 92.69942474365234 Discount Log Loss 19.411243438720703 KL Loss 0.0028651023749262094\n",
      "Image Log Loss: 33222362.0 Reward Log Loss: 94.00035095214844 Discount Log Loss 28.087535858154297 KL Loss 0.0036015615332871675\n",
      "Image Log Loss: 33234348.0 Reward Log Loss: 94.58394622802734 Discount Log Loss 28.04674530029297 KL Loss 0.004112253896892071\n",
      "Image Log Loss: 33211730.0 Reward Log Loss: 94.01806640625 Discount Log Loss 28.011117935180664 KL Loss 0.003124137409031391\n",
      "Image Log Loss: 33228682.0 Reward Log Loss: 94.66592407226562 Discount Log Loss 37.074546813964844 KL Loss 0.003172321943566203\n",
      "Image Log Loss: 33177972.0 Reward Log Loss: 93.46686553955078 Discount Log Loss 23.382532119750977 KL Loss 0.0027404362335801125\n",
      "Image Log Loss: 33260922.0 Reward Log Loss: 94.65394592285156 Discount Log Loss 27.94375991821289 KL Loss 0.00285134045407176\n",
      "Image Log Loss: 33269358.0 Reward Log Loss: 95.50190734863281 Discount Log Loss 37.246952056884766 KL Loss 0.0042234379798173904\n",
      "Image Log Loss: 33227084.0 Reward Log Loss: 94.06758117675781 Discount Log Loss 27.920034408569336 KL Loss 0.0025684544816613197\n",
      "Image Log Loss: 33217680.0 Reward Log Loss: 94.41480255126953 Discount Log Loss 27.912574768066406 KL Loss 0.00412815110757947\n",
      "Image Log Loss: 33259922.0 Reward Log Loss: 94.65782928466797 Discount Log Loss 32.633670806884766 KL Loss 0.002683686325326562\n",
      "Image Log Loss: 33202200.0 Reward Log Loss: 94.45755767822266 Discount Log Loss 32.642738342285156 KL Loss 0.003548806067556143\n",
      "Image Log Loss: 33201928.0 Reward Log Loss: 94.63383483886719 Discount Log Loss 32.646400451660156 KL Loss 0.0031000683084130287\n",
      "Image Log Loss: 33171640.0 Reward Log Loss: 93.99459838867188 Discount Log Loss 27.896099090576172 KL Loss 0.002926912624388933\n",
      "Image Log Loss: 33199656.0 Reward Log Loss: 94.21208953857422 Discount Log Loss 27.893949508666992 KL Loss 0.0030685681849718094\n",
      "Image Log Loss: 33198690.0 Reward Log Loss: 93.86560821533203 Discount Log Loss 23.14148712158203 KL Loss 0.0030418667010962963\n",
      "Image Log Loss: 33218644.0 Reward Log Loss: 95.62317657470703 Discount Log Loss 37.40923309326172 KL Loss 0.0027058881241828203\n",
      "Image Log Loss: 33205698.0 Reward Log Loss: 93.6529312133789 Discount Log Loss 23.13208770751953 KL Loss 0.002552154939621687\n",
      "Image Log Loss: 33216070.0 Reward Log Loss: 92.70158386230469 Discount Log Loss 18.365558624267578 KL Loss 0.0029143246356397867\n",
      "Image Log Loss: 33200472.0 Reward Log Loss: 94.06852722167969 Discount Log Loss 27.881797790527344 KL Loss 0.002863862318918109\n",
      "Image Log Loss: 33188102.0 Reward Log Loss: 94.24020385742188 Discount Log Loss 27.878620147705078 KL Loss 0.0027806300204247236\n",
      "Image Log Loss: 33202246.0 Reward Log Loss: 93.06626892089844 Discount Log Loss 23.06033706665039 KL Loss 0.0030342184472829103\n",
      "Image Log Loss: 33228782.0 Reward Log Loss: 93.56848907470703 Discount Log Loss 27.873977661132812 KL Loss 0.0023373954463750124\n",
      "Image Log Loss: 33148200.0 Reward Log Loss: 93.50106811523438 Discount Log Loss 18.15161895751953 KL Loss 0.002847600495442748\n",
      "Image Log Loss: 33166044.0 Reward Log Loss: 93.9810562133789 Discount Log Loss 27.873682022094727 KL Loss 0.0024137923028320074\n",
      "Image Log Loss: 33193298.0 Reward Log Loss: 93.25855255126953 Discount Log Loss 18.027008056640625 KL Loss 0.002790963975712657\n",
      "Image Log Loss: 33175764.0 Reward Log Loss: 94.23481750488281 Discount Log Loss 27.882352828979492 KL Loss 0.003483434906229377\n",
      "Image Log Loss: 33183398.0 Reward Log Loss: 94.91326141357422 Discount Log Loss 32.89076232910156 KL Loss 0.00262822350487113\n",
      "Image Log Loss: 33134430.0 Reward Log Loss: 93.53385925292969 Discount Log Loss 27.89544105529785 KL Loss 0.003281639888882637\n",
      "Image Log Loss: 33184012.0 Reward Log Loss: 93.70193481445312 Discount Log Loss 22.860092163085938 KL Loss 0.0030077537521719933\n",
      "Image Log Loss: 33143388.0 Reward Log Loss: 93.72714233398438 Discount Log Loss 32.96614456176758 KL Loss 0.00282088341191411\n",
      "Image Log Loss: 33163906.0 Reward Log Loss: 95.48567199707031 Discount Log Loss 32.97521209716797 KL Loss 0.002566044218838215\n",
      "Image Log Loss: 33152344.0 Reward Log Loss: 95.85725402832031 Discount Log Loss 43.095428466796875 KL Loss 0.003191745840013027\n",
      "Image Log Loss: 33096404.0 Reward Log Loss: 93.1117935180664 Discount Log Loss 17.8239688873291 KL Loss 0.0026924835983663797\n",
      "Image Log Loss: 33110426.0 Reward Log Loss: 93.17507934570312 Discount Log Loss 22.86648941040039 KL Loss 0.0026521426625549793\n",
      "Image Log Loss: 33121880.0 Reward Log Loss: 94.65614318847656 Discount Log Loss 27.883642196655273 KL Loss 0.002443457255139947\n",
      "Image Log Loss: 33125888.0 Reward Log Loss: 95.00566101074219 Discount Log Loss 27.880599975585938 KL Loss 0.002386104315519333\n",
      "Image Log Loss: 33119526.0 Reward Log Loss: 94.85493469238281 Discount Log Loss 27.877643585205078 KL Loss 0.0036352071911096573\n",
      "Image Log Loss: 33136744.0 Reward Log Loss: 94.89253997802734 Discount Log Loss 32.858882904052734 KL Loss 0.003692129161208868\n",
      "Image Log Loss: 33086078.0 Reward Log Loss: 94.576171875 Discount Log Loss 32.83633041381836 KL Loss 0.0024209364783018827\n",
      "Image Log Loss: 33134150.0 Reward Log Loss: 96.53398132324219 Discount Log Loss 47.62470245361328 KL Loss 0.0033198806922882795\n",
      "Image Log Loss: 33098440.0 Reward Log Loss: 94.62422943115234 Discount Log Loss 32.75041580200195 KL Loss 0.0022686515003442764\n",
      "Image Log Loss: 33117382.0 Reward Log Loss: 93.04701232910156 Discount Log Loss 18.210254669189453 KL Loss 0.00273435958661139\n",
      "Image Log Loss: 33127824.0 Reward Log Loss: 94.80024719238281 Discount Log Loss 32.67106246948242 KL Loss 0.0033319476060569286\n",
      "Image Log Loss: 33103406.0 Reward Log Loss: 94.65190124511719 Discount Log Loss 27.885107040405273 KL Loss 0.00265954015776515\n",
      "Image Log Loss: 33116928.0 Reward Log Loss: 94.91336059570312 Discount Log Loss 27.895227432250977 KL Loss 0.002778214169666171\n",
      "Image Log Loss: 33113488.0 Reward Log Loss: 95.77752685546875 Discount Log Loss 37.31198501586914 KL Loss 0.002676934003829956\n",
      "Image Log Loss: 33074764.0 Reward Log Loss: 95.37430572509766 Discount Log Loss 37.259796142578125 KL Loss 0.0025161446537822485\n",
      "Image Log Loss: 33097612.0 Reward Log Loss: 96.22628784179688 Discount Log Loss 41.828392028808594 KL Loss 0.003333386266604066\n",
      "Image Log Loss: 33089518.0 Reward Log Loss: 93.52144622802734 Discount Log Loss 23.402650833129883 KL Loss 0.0023030347656458616\n",
      "Image Log Loss: 33077726.0 Reward Log Loss: 96.27559661865234 Discount Log Loss 41.619667053222656 KL Loss 0.0025728875771164894\n",
      "Image Log Loss: 33086224.0 Reward Log Loss: 94.48957061767578 Discount Log Loss 23.553028106689453 KL Loss 0.0036151378881186247\n",
      "Image Log Loss: 33078710.0 Reward Log Loss: 95.13485717773438 Discount Log Loss 36.99310302734375 KL Loss 0.00229754950851202\n",
      "Image Log Loss: 33070996.0 Reward Log Loss: 95.91387939453125 Discount Log Loss 45.816226959228516 KL Loss 0.0030388906598091125\n",
      "Image Log Loss: 33035418.0 Reward Log Loss: 95.45536804199219 Discount Log Loss 41.292144775390625 KL Loss 0.0022710084449499846\n",
      "Image Log Loss: 33087544.0 Reward Log Loss: 93.7656021118164 Discount Log Loss 28.22478485107422 KL Loss 0.0021485285833477974\n",
      "Image Log Loss: 33084534.0 Reward Log Loss: 94.5309066772461 Discount Log Loss 32.5594596862793 KL Loss 0.0028548401314765215\n",
      "Image Log Loss: 33037058.0 Reward Log Loss: 95.73271179199219 Discount Log Loss 36.81806945800781 KL Loss 0.0032793462742120028\n",
      "Image Log Loss: 33033362.0 Reward Log Loss: 96.52554321289062 Discount Log Loss 49.438350677490234 KL Loss 0.003045250428840518\n",
      "Image Log Loss: 33031916.0 Reward Log Loss: 95.072021484375 Discount Log Loss 36.77797317504883 KL Loss 0.002571101998910308\n",
      "Image Log Loss: 33073862.0 Reward Log Loss: 95.80776977539062 Discount Log Loss 40.87617874145508 KL Loss 0.0034267050214111805\n",
      "Image Log Loss: 33084922.0 Reward Log Loss: 93.49075317382812 Discount Log Loss 24.5726261138916 KL Loss 0.0018339632079005241\n",
      "Image Log Loss: 33057556.0 Reward Log Loss: 95.01427459716797 Discount Log Loss 32.71373748779297 KL Loss 0.0029473365284502506\n",
      "Image Log Loss: 33040912.0 Reward Log Loss: 95.16991424560547 Discount Log Loss 36.743900299072266 KL Loss 0.002530634868890047\n",
      "Image Log Loss: 33038488.0 Reward Log Loss: 94.15394592285156 Discount Log Loss 24.70636749267578 KL Loss 0.002094969851896167\n",
      "Image Log Loss: 32999588.0 Reward Log Loss: 96.40660858154297 Discount Log Loss 40.749454498291016 KL Loss 0.002805171301588416\n",
      "Image Log Loss: 33028620.0 Reward Log Loss: 93.27047729492188 Discount Log Loss 20.623306274414062 KL Loss 0.001951948506757617\n",
      "Image Log Loss: 33040286.0 Reward Log Loss: 94.80529022216797 Discount Log Loss 28.606313705444336 KL Loss 0.0023144236765801907\n",
      "Image Log Loss: 33033478.0 Reward Log Loss: 94.00171661376953 Discount Log Loss 28.526905059814453 KL Loss 0.0027331241872161627\n",
      "Image Log Loss: 32976198.0 Reward Log Loss: 96.27001190185547 Discount Log Loss 45.013641357421875 KL Loss 0.002401107456535101\n",
      "Image Log Loss: 33002188.0 Reward Log Loss: 95.68453216552734 Discount Log Loss 36.737491607666016 KL Loss 0.0024343072436749935\n",
      "Image Log Loss: 33052888.0 Reward Log Loss: 94.20580291748047 Discount Log Loss 24.1691951751709 KL Loss 0.003269120119512081\n",
      "Image Log Loss: 32995650.0 Reward Log Loss: 94.93368530273438 Discount Log Loss 32.53949737548828 KL Loss 0.0023444700054824352\n",
      "Image Log Loss: 32977098.0 Reward Log Loss: 96.0439682006836 Discount Log Loss 41.056949615478516 KL Loss 0.0028512608259916306\n",
      "Image Log Loss: 32984018.0 Reward Log Loss: 95.4725112915039 Discount Log Loss 41.09170913696289 KL Loss 0.0024978220462799072\n",
      "Image Log Loss: 32977862.0 Reward Log Loss: 94.73262023925781 Discount Log Loss 32.52186965942383 KL Loss 0.002021589782088995\n",
      "Image Log Loss: 32970588.0 Reward Log Loss: 93.7048568725586 Discount Log Loss 28.224163055419922 KL Loss 0.002329933922737837\n",
      "Image Log Loss: 33011678.0 Reward Log Loss: 94.58560180664062 Discount Log Loss 28.207796096801758 KL Loss 0.0022200471721589565\n",
      "Image Log Loss: 32990628.0 Reward Log Loss: 95.4017105102539 Discount Log Loss 36.84196090698242 KL Loss 0.0020564652513712645\n",
      "Image Log Loss: 32969972.0 Reward Log Loss: 93.67867279052734 Discount Log Loss 23.832172393798828 KL Loss 0.0020542768761515617\n",
      "Image Log Loss: 32966498.0 Reward Log Loss: 92.92420196533203 Discount Log Loss 19.418827056884766 KL Loss 0.0020996015518903732\n",
      "Image Log Loss: 32975348.0 Reward Log Loss: 94.09353637695312 Discount Log Loss 32.511600494384766 KL Loss 0.00252842390909791\n",
      "Image Log Loss: 32964698.0 Reward Log Loss: 95.84359741210938 Discount Log Loss 36.959049224853516 KL Loss 0.002793611027300358\n",
      "Image Log Loss: 32994924.0 Reward Log Loss: 94.76753997802734 Discount Log Loss 28.052692413330078 KL Loss 0.0023606447502970695\n",
      "Image Log Loss: 32932214.0 Reward Log Loss: 95.25357055664062 Discount Log Loss 37.022098541259766 KL Loss 0.001901872456073761\n",
      "Image Log Loss: 32955036.0 Reward Log Loss: 95.13618469238281 Discount Log Loss 32.530513763427734 KL Loss 0.002262037480250001\n",
      "Image Log Loss: 32954322.0 Reward Log Loss: 94.00859069824219 Discount Log Loss 23.4883975982666 KL Loss 0.0024092718958854675\n",
      "Image Log Loss: 32913282.0 Reward Log Loss: 93.36293029785156 Discount Log Loss 23.45201873779297 KL Loss 0.0021160158794373274\n",
      "Image Log Loss: 32893684.0 Reward Log Loss: 94.19012451171875 Discount Log Loss 27.97734832763672 KL Loss 0.002039615297690034\n",
      "Image Log Loss: 32916708.0 Reward Log Loss: 94.23994445800781 Discount Log Loss 27.9605655670166 KL Loss 0.001998999621719122\n",
      "Image Log Loss: 32903016.0 Reward Log Loss: 94.69296264648438 Discount Log Loss 32.582427978515625 KL Loss 0.00229657837189734\n",
      "Image Log Loss: 32920730.0 Reward Log Loss: 94.67118835449219 Discount Log Loss 32.59545135498047 KL Loss 0.001976940082386136\n",
      "Image Log Loss: 32917980.0 Reward Log Loss: 93.4018783569336 Discount Log Loss 18.573999404907227 KL Loss 0.001692592748440802\n",
      "Image Log Loss: 32902748.0 Reward Log Loss: 94.25555419921875 Discount Log Loss 27.916885375976562 KL Loss 0.001701138447970152\n",
      "Image Log Loss: 32873624.0 Reward Log Loss: 96.05944061279297 Discount Log Loss 42.123008728027344 KL Loss 0.002907656831666827\n",
      "Image Log Loss: 32835894.0 Reward Log Loss: 93.12645721435547 Discount Log Loss 18.415828704833984 KL Loss 0.001791678136214614\n",
      "Image Log Loss: 32889146.0 Reward Log Loss: 94.22076416015625 Discount Log Loss 27.899141311645508 KL Loss 0.00255524180829525\n",
      "Image Log Loss: 32862062.0 Reward Log Loss: 93.99134063720703 Discount Log Loss 32.68068313598633 KL Loss 0.001544607919640839\n",
      "Image Log Loss: 32876062.0 Reward Log Loss: 94.89274597167969 Discount Log Loss 37.4864501953125 KL Loss 0.0021776235662400723\n",
      "Image Log Loss: 32858718.0 Reward Log Loss: 93.73938751220703 Discount Log Loss 23.095664978027344 KL Loss 0.0020969193428754807\n",
      "Image Log Loss: 32912838.0 Reward Log Loss: 95.06989288330078 Discount Log Loss 32.68488693237305 KL Loss 0.002362197730690241\n",
      "Image Log Loss: 32906950.0 Reward Log Loss: 93.59256744384766 Discount Log Loss 27.885272979736328 KL Loss 0.0026993718929588795\n",
      "Image Log Loss: 32856710.0 Reward Log Loss: 94.92223358154297 Discount Log Loss 27.883832931518555 KL Loss 0.0023066233843564987\n",
      "Image Log Loss: 32858598.0 Reward Log Loss: 94.79917907714844 Discount Log Loss 32.674983978271484 KL Loss 0.002192872576415539\n",
      "Image Log Loss: 32853748.0 Reward Log Loss: 95.78966522216797 Discount Log Loss 42.237022399902344 KL Loss 0.0021856743842363358\n",
      "Image Log Loss: 32873116.0 Reward Log Loss: 94.95763397216797 Discount Log Loss 32.643218994140625 KL Loss 0.002246392425149679\n",
      "Image Log Loss: 32830108.0 Reward Log Loss: 94.77688598632812 Discount Log Loss 27.89316177368164 KL Loss 0.0017067301087081432\n",
      "Image Log Loss: 32800088.0 Reward Log Loss: 95.06085968017578 Discount Log Loss 32.60112380981445 KL Loss 0.00165274937171489\n",
      "Image Log Loss: 32838684.0 Reward Log Loss: 96.26437377929688 Discount Log Loss 41.927879333496094 KL Loss 0.0028011957183480263\n",
      "Image Log Loss: 32869324.0 Reward Log Loss: 94.0434799194336 Discount Log Loss 27.932327270507812 KL Loss 0.0019573597237467766\n",
      "Image Log Loss: 32804696.0 Reward Log Loss: 92.903076171875 Discount Log Loss 18.7681941986084 KL Loss 0.0019643830601125956\n",
      "Image Log Loss: 32795118.0 Reward Log Loss: 93.93893432617188 Discount Log Loss 23.381513595581055 KL Loss 0.0019405244383960962\n",
      "Image Log Loss: 32808218.0 Reward Log Loss: 94.68701171875 Discount Log Loss 32.539981842041016 KL Loss 0.0020306066144257784\n",
      "Image Log Loss: 32828436.0 Reward Log Loss: 95.59821319580078 Discount Log Loss 37.1163215637207 KL Loss 0.0019553792662918568\n",
      "Image Log Loss: 32826470.0 Reward Log Loss: 95.16664123535156 Discount Log Loss 32.534400939941406 KL Loss 0.0023644748143851757\n",
      "Image Log Loss: 32754490.0 Reward Log Loss: 92.76307678222656 Discount Log Loss 18.870895385742188 KL Loss 0.0017605775501579046\n",
      "Image Log Loss: 32788946.0 Reward Log Loss: 94.80907440185547 Discount Log Loss 32.53172302246094 KL Loss 0.00188273040112108\n",
      "Image Log Loss: 32772084.0 Reward Log Loss: 94.71334075927734 Discount Log Loss 32.53274917602539 KL Loss 0.0018127296352759004\n",
      "Image Log Loss: 32837528.0 Reward Log Loss: 93.9141616821289 Discount Log Loss 27.96422576904297 KL Loss 0.001593967666849494\n",
      "Image Log Loss: 32776120.0 Reward Log Loss: 95.12191772460938 Discount Log Loss 37.110557556152344 KL Loss 0.002871190896257758\n",
      "Image Log Loss: 32793112.0 Reward Log Loss: 94.5875473022461 Discount Log Loss 37.105247497558594 KL Loss 0.0014751139096915722\n",
      "Image Log Loss: 32790786.0 Reward Log Loss: 95.53699493408203 Discount Log Loss 37.087364196777344 KL Loss 0.0020365575328469276\n",
      "Image Log Loss: 32762696.0 Reward Log Loss: 93.04045867919922 Discount Log Loss 18.90912628173828 KL Loss 0.0021196904126554728\n",
      "Image Log Loss: 32797914.0 Reward Log Loss: 93.47904205322266 Discount Log Loss 23.444921493530273 KL Loss 0.002467888407409191\n",
      "Image Log Loss: 32754228.0 Reward Log Loss: 93.65562438964844 Discount Log Loss 23.422340393066406 KL Loss 0.0016048788093030453\n",
      "Image Log Loss: 32723256.0 Reward Log Loss: 93.71492004394531 Discount Log Loss 23.382654190063477 KL Loss 0.0015676167095080018\n",
      "Image Log Loss: 32743306.0 Reward Log Loss: 93.6163101196289 Discount Log Loss 23.329641342163086 KL Loss 0.001791693619452417\n",
      "Image Log Loss: 32780030.0 Reward Log Loss: 94.19422912597656 Discount Log Loss 23.268028259277344 KL Loss 0.0015999311581254005\n",
      "Image Log Loss: 32709104.0 Reward Log Loss: 95.37252044677734 Discount Log Loss 37.29689025878906 KL Loss 0.002077818615362048\n",
      "Image Log Loss: 32725488.0 Reward Log Loss: 93.91971588134766 Discount Log Loss 23.162534713745117 KL Loss 0.001790274865925312\n",
      "Image Log Loss: 32725916.0 Reward Log Loss: 95.40422821044922 Discount Log Loss 32.65070724487305 KL Loss 0.0027148055378347635\n",
      "Image Log Loss: 32740456.0 Reward Log Loss: 95.30868530273438 Discount Log Loss 32.67420959472656 KL Loss 0.0022524918895214796\n",
      "Image Log Loss: 32729474.0 Reward Log Loss: 95.10000610351562 Discount Log Loss 32.69013595581055 KL Loss 0.0020256994757801294\n",
      "Image Log Loss: 32773420.0 Reward Log Loss: 92.66049194335938 Discount Log Loss 18.236452102661133 KL Loss 0.0022015899885445833\n",
      "Image Log Loss: 32707270.0 Reward Log Loss: 94.17125701904297 Discount Log Loss 27.876232147216797 KL Loss 0.0019172324100509286\n",
      "Image Log Loss: 32690788.0 Reward Log Loss: 94.20512390136719 Discount Log Loss 27.875560760498047 KL Loss 0.0015702411765232682\n",
      "Image Log Loss: 32772362.0 Reward Log Loss: 93.52347564697266 Discount Log Loss 18.107473373413086 KL Loss 0.0016956960316747427\n",
      "Image Log Loss: 32774502.0 Reward Log Loss: 94.86270141601562 Discount Log Loss 27.878345489501953 KL Loss 0.0023150937631726265\n",
      "Image Log Loss: 32730470.0 Reward Log Loss: 94.98110961914062 Discount Log Loss 37.7688102722168 KL Loss 0.002434101654216647\n",
      "Image Log Loss: 32705310.0 Reward Log Loss: 95.50592041015625 Discount Log Loss 37.784664154052734 KL Loss 0.002938090357929468\n",
      "Image Log Loss: 32643210.0 Reward Log Loss: 95.71495056152344 Discount Log Loss 42.70277786254883 KL Loss 0.0022832783870399\n",
      "Image Log Loss: 32730348.0 Reward Log Loss: 94.20440673828125 Discount Log Loss 22.964740753173828 KL Loss 0.0017832225421443582\n",
      "Image Log Loss: 32704870.0 Reward Log Loss: 94.53112030029297 Discount Log Loss 27.870176315307617 KL Loss 0.0016650608740746975\n",
      "Image Log Loss: 32685164.0 Reward Log Loss: 94.4103012084961 Discount Log Loss 32.73275375366211 KL Loss 0.002253670245409012\n",
      "Image Log Loss: 32704728.0 Reward Log Loss: 96.010498046875 Discount Log Loss 37.5435676574707 KL Loss 0.002696697134524584\n",
      "Image Log Loss: 32656186.0 Reward Log Loss: 94.4732437133789 Discount Log Loss 27.873981475830078 KL Loss 0.002283327979966998\n",
      "Image Log Loss: 32669562.0 Reward Log Loss: 94.27208709716797 Discount Log Loss 23.116100311279297 KL Loss 0.0018551244866102934\n",
      "Image Log Loss: 32625360.0 Reward Log Loss: 94.91836547851562 Discount Log Loss 32.630191802978516 KL Loss 0.0020757063757628202\n",
      "Image Log Loss: 32701224.0 Reward Log Loss: 94.85599517822266 Discount Log Loss 27.894210815429688 KL Loss 0.0018427068134769797\n",
      "Image Log Loss: 32651520.0 Reward Log Loss: 95.129638671875 Discount Log Loss 42.00307083129883 KL Loss 0.0022447011433541775\n",
      "Image Log Loss: 32600410.0 Reward Log Loss: 93.98114013671875 Discount Log Loss 27.917333602905273 KL Loss 0.001945459982380271\n",
      "Image Log Loss: 32629092.0 Reward Log Loss: 93.81591796875 Discount Log Loss 27.93231201171875 KL Loss 0.0021280869841575623\n",
      "Image Log Loss: 32651812.0 Reward Log Loss: 96.45417785644531 Discount Log Loss 41.7818489074707 KL Loss 0.00224290881305933\n",
      "Image Log Loss: 32610230.0 Reward Log Loss: 95.53302764892578 Discount Log Loss 37.11775588989258 KL Loss 0.0020031605381518602\n",
      "Image Log Loss: 32578942.0 Reward Log Loss: 95.10529327392578 Discount Log Loss 37.06288146972656 KL Loss 0.0022620109375566244\n",
      "Image Log Loss: 32614132.0 Reward Log Loss: 94.45541381835938 Discount Log Loss 28.040239334106445 KL Loss 0.0019416362047195435\n",
      "Image Log Loss: 32614072.0 Reward Log Loss: 94.4757308959961 Discount Log Loss 32.520957946777344 KL Loss 0.0020413959864526987\n",
      "Image Log Loss: 32581520.0 Reward Log Loss: 94.37014770507812 Discount Log Loss 28.105323791503906 KL Loss 0.001489945687353611\n",
      "Image Log Loss: 32568776.0 Reward Log Loss: 95.6322250366211 Discount Log Loss 41.30876922607422 KL Loss 0.0018845268059521914\n",
      "Image Log Loss: 32587154.0 Reward Log Loss: 96.00801849365234 Discount Log Loss 41.24740219116211 KL Loss 0.0019313603406772017\n",
      "Image Log Loss: 32583626.0 Reward Log Loss: 93.39923095703125 Discount Log Loss 23.891691207885742 KL Loss 0.0013917770702391863\n",
      "Image Log Loss: 32564928.0 Reward Log Loss: 94.03802490234375 Discount Log Loss 23.9368953704834 KL Loss 0.001625689910724759\n",
      "Image Log Loss: 32567378.0 Reward Log Loss: 96.00546264648438 Discount Log Loss 36.8309211730957 KL Loss 0.001462007756344974\n",
      "Image Log Loss: 32539624.0 Reward Log Loss: 95.16500091552734 Discount Log Loss 36.82389450073242 KL Loss 0.0026095076464116573\n",
      "Image Log Loss: 32594504.0 Reward Log Loss: 95.23715209960938 Discount Log Loss 32.532535552978516 KL Loss 0.002070882823318243\n",
      "Image Log Loss: 32584430.0 Reward Log Loss: 95.60714721679688 Discount Log Loss 36.80830383300781 KL Loss 0.0026597478426992893\n",
      "Image Log Loss: 32565424.0 Reward Log Loss: 95.2203140258789 Discount Log Loss 32.53295135498047 KL Loss 0.001502225873991847\n",
      "Image Log Loss: 32555334.0 Reward Log Loss: 93.18925476074219 Discount Log Loss 24.0111083984375 KL Loss 0.0020505294669419527\n",
      "Image Log Loss: 32522558.0 Reward Log Loss: 94.02145385742188 Discount Log Loss 23.980825424194336 KL Loss 0.0017313696444034576\n",
      "Image Log Loss: 32524094.0 Reward Log Loss: 94.98591613769531 Discount Log Loss 32.51618576049805 KL Loss 0.0027931141667068005\n",
      "Image Log Loss: 32498028.0 Reward Log Loss: 93.56371307373047 Discount Log Loss 23.85470962524414 KL Loss 0.0016748585039749742\n",
      "Image Log Loss: 32543974.0 Reward Log Loss: 94.26698303222656 Discount Log Loss 32.50334548950195 KL Loss 0.0019810416270047426\n",
      "Image Log Loss: 32504046.0 Reward Log Loss: 94.07441711425781 Discount Log Loss 23.69585418701172 KL Loss 0.0018554932903498411\n",
      "Image Log Loss: 32464888.0 Reward Log Loss: 93.12517547607422 Discount Log Loss 23.60655975341797 KL Loss 0.001394449733197689\n",
      "Image Log Loss: 32527096.0 Reward Log Loss: 94.58007049560547 Discount Log Loss 32.52082824707031 KL Loss 0.0019788830541074276\n",
      "Image Log Loss: 32481012.0 Reward Log Loss: 95.8459701538086 Discount Log Loss 37.093116760253906 KL Loss 0.002337758196517825\n",
      "Image Log Loss: 32479416.0 Reward Log Loss: 94.2701187133789 Discount Log Loss 27.966615676879883 KL Loss 0.0015132208354771137\n",
      "Image Log Loss: 32508434.0 Reward Log Loss: 92.56707000732422 Discount Log Loss 18.70977210998535 KL Loss 0.001680406741797924\n",
      "Image Log Loss: 32508908.0 Reward Log Loss: 94.09056854248047 Discount Log Loss 27.932580947875977 KL Loss 0.0021148668602108955\n",
      "Image Log Loss: 32533420.0 Reward Log Loss: 93.18974304199219 Discount Log Loss 23.20535659790039 KL Loss 0.0012667744886130095\n",
      "Image Log Loss: 32481772.0 Reward Log Loss: 95.10551452636719 Discount Log Loss 37.438262939453125 KL Loss 0.0023450562730431557\n",
      "Image Log Loss: 32508532.0 Reward Log Loss: 94.3085708618164 Discount Log Loss 32.69995880126953 KL Loss 0.001740751788020134\n",
      "Image Log Loss: 32416566.0 Reward Log Loss: 93.13562774658203 Discount Log Loss 23.085920333862305 KL Loss 0.0014822555240243673\n",
      "Image Log Loss: 32431260.0 Reward Log Loss: 96.35702514648438 Discount Log Loss 42.42318344116211 KL Loss 0.0017937174998223782\n",
      "Image Log Loss: 32453520.0 Reward Log Loss: 94.73429870605469 Discount Log Loss 32.736732482910156 KL Loss 0.0014453257899731398\n",
      "Image Log Loss: 32482120.0 Reward Log Loss: 96.32976531982422 Discount Log Loss 37.55864715576172 KL Loss 0.001995515776798129\n",
      "Image Log Loss: 32419318.0 Reward Log Loss: 93.74149322509766 Discount Log Loss 27.889196395874023 KL Loss 0.0016804265324026346\n",
      "Image Log Loss: 32426078.0 Reward Log Loss: 93.82392883300781 Discount Log Loss 27.888412475585938 KL Loss 0.0024402544368058443\n",
      "Image Log Loss: 32450412.0 Reward Log Loss: 95.23360443115234 Discount Log Loss 32.662296295166016 KL Loss 0.0017991072963923216\n",
      "Image Log Loss: 32437416.0 Reward Log Loss: 94.61840057373047 Discount Log Loss 27.8917236328125 KL Loss 0.0020055228378623724\n",
      "Image Log Loss: 32422308.0 Reward Log Loss: 94.4400405883789 Discount Log Loss 27.89511489868164 KL Loss 0.0018876877147704363\n",
      "Image Log Loss: 32342290.0 Reward Log Loss: 93.57520294189453 Discount Log Loss 23.180622100830078 KL Loss 0.0016121214721351862\n",
      "Image Log Loss: 32411614.0 Reward Log Loss: 94.36449432373047 Discount Log Loss 27.899240493774414 KL Loss 0.0015884182648733258\n",
      "Image Log Loss: 32353944.0 Reward Log Loss: 93.82742309570312 Discount Log Loss 27.898847579956055 KL Loss 0.0014766644453629851\n",
      "Image Log Loss: 32383014.0 Reward Log Loss: 95.6058578491211 Discount Log Loss 37.329864501953125 KL Loss 0.0015289504081010818\n",
      "Image Log Loss: 32439206.0 Reward Log Loss: 95.18705749511719 Discount Log Loss 27.901451110839844 KL Loss 0.0021321375388652086\n",
      "Image Log Loss: 32366738.0 Reward Log Loss: 94.15338134765625 Discount Log Loss 23.2065372467041 KL Loss 0.0014535286463797092\n",
      "Image Log Loss: 32393810.0 Reward Log Loss: 94.70429992675781 Discount Log Loss 27.901954650878906 KL Loss 0.0019126746337860823\n",
      "Image Log Loss: 32377460.0 Reward Log Loss: 95.48532104492188 Discount Log Loss 42.0205192565918 KL Loss 0.0018544972408562899\n",
      "Image Log Loss: 32364342.0 Reward Log Loss: 93.13018798828125 Discount Log Loss 18.5197811126709 KL Loss 0.0015155848814174533\n",
      "Image Log Loss: 32338406.0 Reward Log Loss: 94.83015441894531 Discount Log Loss 32.59928894042969 KL Loss 0.00220809830352664\n",
      "Image Log Loss: 32344604.0 Reward Log Loss: 93.94625091552734 Discount Log Loss 27.901884078979492 KL Loss 0.0015132692642509937\n",
      "Image Log Loss: 32369024.0 Reward Log Loss: 96.0571517944336 Discount Log Loss 46.69770431518555 KL Loss 0.0017437387723475695\n",
      "Image Log Loss: 32325588.0 Reward Log Loss: 94.42359161376953 Discount Log Loss 32.584136962890625 KL Loss 0.0012532533146440983\n",
      "Image Log Loss: 32328570.0 Reward Log Loss: 94.43688201904297 Discount Log Loss 27.922632217407227 KL Loss 0.0014929719036445022\n",
      "Image Log Loss: 32340048.0 Reward Log Loss: 95.3783950805664 Discount Log Loss 37.184837341308594 KL Loss 0.002528062555938959\n",
      "Image Log Loss: 32349772.0 Reward Log Loss: 94.8914566040039 Discount Log Loss 32.5461311340332 KL Loss 0.0014473323244601488\n",
      "Image Log Loss: 32370828.0 Reward Log Loss: 94.17559814453125 Discount Log Loss 23.401742935180664 KL Loss 0.001124081201851368\n",
      "Image Log Loss: 32291556.0 Reward Log Loss: 94.82850646972656 Discount Log Loss 32.53054428100586 KL Loss 0.0012513231486082077\n",
      "Image Log Loss: 32299722.0 Reward Log Loss: 93.35277557373047 Discount Log Loss 23.44997787475586 KL Loss 0.0016166095156222582\n",
      "Image Log Loss: 32323886.0 Reward Log Loss: 94.67224884033203 Discount Log Loss 27.987581253051758 KL Loss 0.0018392257625237107\n",
      "Image Log Loss: 32326654.0 Reward Log Loss: 93.8193359375 Discount Log Loss 18.896486282348633 KL Loss 0.0014406798873096704\n",
      "Image Log Loss: 32315914.0 Reward Log Loss: 95.68666076660156 Discount Log Loss 41.6707763671875 KL Loss 0.0012686151312664151\n",
      "Image Log Loss: 32283038.0 Reward Log Loss: 94.64209747314453 Discount Log Loss 27.961170196533203 KL Loss 0.0013379204319790006\n",
      "Image Log Loss: 32237990.0 Reward Log Loss: 94.1908950805664 Discount Log Loss 32.53846740722656 KL Loss 0.0011545303277671337\n",
      "Image Log Loss: 32276030.0 Reward Log Loss: 95.86283874511719 Discount Log Loss 37.129432678222656 KL Loss 0.0011447154683992267\n",
      "Image Log Loss: 32315830.0 Reward Log Loss: 94.02554321289062 Discount Log Loss 23.371299743652344 KL Loss 0.0016637821681797504\n",
      "Image Log Loss: 32272768.0 Reward Log Loss: 96.31371307373047 Discount Log Loss 41.72194290161133 KL Loss 0.0026145020965486765\n",
      "Image Log Loss: 32275476.0 Reward Log Loss: 95.0146255493164 Discount Log Loss 32.536136627197266 KL Loss 0.0013050325214862823\n",
      "Image Log Loss: 32257676.0 Reward Log Loss: 93.19966125488281 Discount Log Loss 18.834369659423828 KL Loss 0.0010204283753409982\n",
      "Image Log Loss: 32286366.0 Reward Log Loss: 95.80681610107422 Discount Log Loss 37.106849670410156 KL Loss 0.002147276420146227\n",
      "Image Log Loss: 32184812.0 Reward Log Loss: 93.69518280029297 Discount Log Loss 27.962535858154297 KL Loss 0.0017066779546439648\n",
      "Image Log Loss: 32208190.0 Reward Log Loss: 94.60292053222656 Discount Log Loss 27.96139907836914 KL Loss 0.002035698853433132\n",
      "Image Log Loss: 32219486.0 Reward Log Loss: 93.32344055175781 Discount Log Loss 23.379966735839844 KL Loss 0.0016216288786381483\n",
      "Image Log Loss: 32186552.0 Reward Log Loss: 93.84745025634766 Discount Log Loss 23.35199737548828 KL Loss 0.0012901098234578967\n",
      "Image Log Loss: 32211958.0 Reward Log Loss: 93.79015350341797 Discount Log Loss 23.309730529785156 KL Loss 0.0015246017137542367\n",
      "Image Log Loss: 32118466.0 Reward Log Loss: 93.74523162841797 Discount Log Loss 27.916311264038086 KL Loss 0.0014931012410670519\n",
      "Image Log Loss: 32234874.0 Reward Log Loss: 93.49353790283203 Discount Log Loss 23.208162307739258 KL Loss 0.0016770712099969387\n",
      "Image Log Loss: 32169764.0 Reward Log Loss: 95.13482666015625 Discount Log Loss 32.629051208496094 KL Loss 0.0016903318464756012\n",
      "Image Log Loss: 32196396.0 Reward Log Loss: 93.77381896972656 Discount Log Loss 18.34495735168457 KL Loss 0.0011871855240315199\n",
      "Image Log Loss: 32164952.0 Reward Log Loss: 94.71588134765625 Discount Log Loss 32.69590759277344 KL Loss 0.001597464899532497\n",
      "Image Log Loss: 32217256.0 Reward Log Loss: 97.47358703613281 Discount Log Loss 52.12937927246094 KL Loss 0.0019101868383586407\n",
      "Image Log Loss: 32187484.0 Reward Log Loss: 94.1476821899414 Discount Log Loss 27.877714157104492 KL Loss 0.0013224955182522535\n",
      "Image Log Loss: 32181338.0 Reward Log Loss: 94.43582153320312 Discount Log Loss 27.876991271972656 KL Loss 0.0011344446102157235\n",
      "Image Log Loss: 32165872.0 Reward Log Loss: 94.15589904785156 Discount Log Loss 23.04892349243164 KL Loss 0.001392061822116375\n",
      "Image Log Loss: 32163128.0 Reward Log Loss: 95.75959014892578 Discount Log Loss 37.53472137451172 KL Loss 0.001720185624435544\n",
      "Image Log Loss: 32118434.0 Reward Log Loss: 93.41197967529297 Discount Log Loss 23.059307098388672 KL Loss 0.0013703275471925735\n",
      "Image Log Loss: 32109164.0 Reward Log Loss: 94.95227813720703 Discount Log Loss 27.875350952148438 KL Loss 0.0013685461599379778\n",
      "Image Log Loss: 32145218.0 Reward Log Loss: 95.85730743408203 Discount Log Loss 37.49534606933594 KL Loss 0.0017084622522816062\n",
      "Image Log Loss: 32116250.0 Reward Log Loss: 94.54894256591797 Discount Log Loss 32.67003631591797 KL Loss 0.0021150310058146715\n",
      "Image Log Loss: 32082142.0 Reward Log Loss: 93.86882781982422 Discount Log Loss 27.880332946777344 KL Loss 0.0017470751190558076\n",
      "Image Log Loss: 32093192.0 Reward Log Loss: 93.84883117675781 Discount Log Loss 23.131189346313477 KL Loss 0.001134488615207374\n",
      "Image Log Loss: 32095178.0 Reward Log Loss: 96.47594451904297 Discount Log Loss 46.87074661254883 KL Loss 0.0014746871311217546\n",
      "Image Log Loss: 32052024.0 Reward Log Loss: 95.217041015625 Discount Log Loss 37.319271087646484 KL Loss 0.0024509397335350513\n",
      "Image Log Loss: 32098178.0 Reward Log Loss: 94.69395446777344 Discount Log Loss 27.913860321044922 KL Loss 0.0014848306309431791\n",
      "Image Log Loss: 32068780.0 Reward Log Loss: 94.58360290527344 Discount Log Loss 27.9316463470459 KL Loss 0.0019349709618836641\n",
      "Image Log Loss: 32088902.0 Reward Log Loss: 95.0097885131836 Discount Log Loss 32.55078125 KL Loss 0.002076145028695464\n",
      "Image Log Loss: 32087852.0 Reward Log Loss: 92.8736343383789 Discount Log Loss 14.24133586883545 KL Loss 0.0012351255863904953\n",
      "Image Log Loss: 32113290.0 Reward Log Loss: 94.23194122314453 Discount Log Loss 32.54085159301758 KL Loss 0.0022850565146654844\n",
      "Image Log Loss: 32031436.0 Reward Log Loss: 94.62924194335938 Discount Log Loss 32.54011535644531 KL Loss 0.001653998508118093\n",
      "Image Log Loss: 32077478.0 Reward Log Loss: 94.75345611572266 Discount Log Loss 32.53827667236328 KL Loss 0.0013451306149363518\n",
      "Image Log Loss: 32046082.0 Reward Log Loss: 94.21078491210938 Discount Log Loss 27.968103408813477 KL Loss 0.0012238729977980256\n",
      "Image Log Loss: 32038292.0 Reward Log Loss: 95.31794738769531 Discount Log Loss 32.53569412231445 KL Loss 0.0017325597582384944\n",
      "Image Log Loss: 32025850.0 Reward Log Loss: 97.78893280029297 Discount Log Loss 55.367034912109375 KL Loss 0.001561306999064982\n",
      "Image Log Loss: 32060442.0 Reward Log Loss: 94.46524047851562 Discount Log Loss 27.997270584106445 KL Loss 0.0012173998402431607\n",
      "Image Log Loss: 32013312.0 Reward Log Loss: 95.66337585449219 Discount Log Loss 41.50369644165039 KL Loss 0.0013649738393723965\n",
      "Image Log Loss: 32057768.0 Reward Log Loss: 94.30225372314453 Discount Log Loss 28.06084442138672 KL Loss 0.0012457590783014894\n",
      "Image Log Loss: 32024166.0 Reward Log Loss: 96.55675506591797 Discount Log Loss 41.344993591308594 KL Loss 0.0014362664660438895\n",
      "Image Log Loss: 32002944.0 Reward Log Loss: 96.63839721679688 Discount Log Loss 45.6356201171875 KL Loss 0.0018081358866766095\n",
      "Image Log Loss: 32044056.0 Reward Log Loss: 94.3564224243164 Discount Log Loss 28.209253311157227 KL Loss 0.0011022959370166063\n",
      "Image Log Loss: 32044754.0 Reward Log Loss: 92.32073211669922 Discount Log Loss 19.72389793395996 KL Loss 0.0014437841018661857\n",
      "Image Log Loss: 31968908.0 Reward Log Loss: 93.87073516845703 Discount Log Loss 28.285634994506836 KL Loss 0.001300295116379857\n",
      "Image Log Loss: 32006392.0 Reward Log Loss: 94.65733337402344 Discount Log Loss 32.54094314575195 KL Loss 0.0019127909326925874\n",
      "Image Log Loss: 31998708.0 Reward Log Loss: 94.58348846435547 Discount Log Loss 28.284074783325195 KL Loss 0.0012202659854665399\n",
      "Image Log Loss: 31968880.0 Reward Log Loss: 95.37584686279297 Discount Log Loss 32.531333923339844 KL Loss 0.0021256175823509693\n",
      "Image Log Loss: 31989058.0 Reward Log Loss: 94.51110076904297 Discount Log Loss 28.24740982055664 KL Loss 0.0017426906852051616\n",
      "Image Log Loss: 31920448.0 Reward Log Loss: 95.24414825439453 Discount Log Loss 36.81439971923828 KL Loss 0.001614105305634439\n",
      "Image Log Loss: 31953142.0 Reward Log Loss: 94.53763580322266 Discount Log Loss 28.199949264526367 KL Loss 0.0015779758105054498\n",
      "Image Log Loss: 31921638.0 Reward Log Loss: 95.90625762939453 Discount Log Loss 41.1730842590332 KL Loss 0.0013923889491707087\n"
     ]
    }
   ],
   "source": [
    "# TODO move hyperparams\n",
    "epochs = 32\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002)\n",
    "\n",
    "buffer = Buffer(batch_size=1)\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 32),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "\n",
    "environment_interactor = EnvironmentInteractor(config, buffer)\n",
    "\n",
    "world_model = WorldModel()\n",
    "rssm = RSSM()\n",
    "\n",
    "models = (\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "wandb.tensorflow.log(tf.summary)\n",
    "\n",
    "for episode in range(100):\n",
    "    environment_interactor.create_trajectories(1000)\n",
    "    data = buffer.sample(batch_size=50, prefetch_size=70)\n",
    "    # Sample from buffer\n",
    "    for sequence in data:\n",
    "        state, next_state, action, reward, non_terminal = sequence[0]\n",
    "\n",
    "            # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "        combined_trainable_variables = models[0].trainable_variables\n",
    "        for i in range(len(models)):\n",
    "            if i+1 >= len(models):\n",
    "                break\n",
    "            combined_trainable_variables += models[i+1].trainable_variables\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            encoded_state = world_model.encoder(state)\n",
    "            initial_rssm_state = RSSMState()\n",
    "            prior_rssm_states, posterior_rssm_states = rssm.observing_rollout(encoded_state, action, non_terminal, initial_rssm_state)\n",
    "            hidden_state_h_and_stochastic_state_z = tf.concat([posterior_rssm_states.stochastic_state_z, posterior_rssm_states.hidden_rnn_state], axis=-1)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            hidden_state_h_and_stochastic_state_z = tf.reshape(hidden_state_h_and_stochastic_state_z, (-1,stochastic_state_size + hidden_unit_size))\n",
    "\n",
    "            decoder_logits = world_model.decoder(hidden_state_h_and_stochastic_state_z)\n",
    "\n",
    "            # TODO ÄNDERN\n",
    "            decoder_logits = tf.reshape(decoder_logits, (-1, image_shape[0], image_shape[1], image_shape[2]))\n",
    "\n",
    "            decoder_distribution = tfp.distributions.Independent(tfp.distributions.Normal(decoder_logits, 1))\n",
    "            reward_logits = world_model.reward_model(hidden_state_h_and_stochastic_state_z)\n",
    "            reward_distribution = tfp.distributions.Independent(tfp.distributions.Normal(reward_logits, 1))\n",
    "            discount_logits = world_model.discount_model(hidden_state_h_and_stochastic_state_z)\n",
    "            discount_distribution = tfp.distributions.Independent(tfp.distributions.Bernoulli(logits=discount_logits))\n",
    "\n",
    "            image_log_loss = compute_log_loss(decoder_distribution, state)\n",
    "            reward_log_loss = compute_log_loss(reward_distribution, reward)\n",
    "            discount_log_loss = compute_log_loss(discount_distribution, non_terminal)\n",
    "            kl_loss = compute_kl_loss(prior_rssm_states, posterior_rssm_states)\n",
    "\n",
    "\n",
    "\n",
    "            loss = image_log_loss + reward_log_loss + discount_log_loss + kl_loss\n",
    "            \n",
    "            print(f\"Image Log Loss: {image_log_loss} Reward Log Loss: {reward_log_loss} Discount Log Loss {discount_log_loss} KL Loss {kl_loss}\")\n",
    "            wandb.log({\"Image Log Loss\": image_log_loss, \"Reward Log Loss\": reward_log_loss, \"Discount Log Loss\": discount_log_loss, \"KL Loss\": kl_loss, \"Loss\": loss})\n",
    "           \n",
    "            # TODO maybe in Gradienttape??\n",
    "            gradients = tape.gradient(loss, combined_trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, combined_trainable_variables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "  t, _ = next(iterator)\n",
    "  trajectories.append(t)\n",
    "\n",
    "\n",
    "    train_step((\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "    \n",
    "    )\n",
    "\n",
    "#print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "  t, _ = next(iterator)\n",
    "  trajectories.append(t)\n",
    "\n",
    "\n",
    "    train_step((\n",
    "        world_model.encoder,\n",
    "        world_model.decoder,\n",
    "        world_model.reward_model,\n",
    "        world_model.discount_model,\n",
    "        rssm.state_action_embedder,\n",
    "        rssm.rnn,\n",
    "        rssm.prior_model,\n",
    "        rssm.posterior_model)\n",
    "    \n",
    "    )\n",
    "\n",
    "#print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World ModelTraining Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# World model & agent training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyperparam inits\n",
    "Agent Data collection in environment + adding data to ERB (+ measure at which reward loop stops?) \n",
    "World model loop on data sampled from ERB\n",
    "Agent training loop with world model feedback\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate environment and network objects\n",
    "# Loop:\n",
    "# Pass respective inputs to networks\n",
    "# Collect outputs\n",
    "# Compute individuall losses\n",
    "# Add together to 1 big loss\n",
    "# Propagate with gradient Tape through network\n",
    "\n",
    "\n",
    "# compute the loss of an input for the model and optimize/tweak according the parameters\n",
    "def train_step(model, input, target, loss_function, optimizer):\n",
    "    # use tf.gradientTape to compute loss, then gradients and apply these to the model to modify the parameters\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(input)\n",
    "        loss = loss_function(target, prediction)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# TODO move hyperparams to the rest\n",
    "epochs = 32\n",
    "\n",
    "# define loss-function and optimizer\n",
    "cross_entropy_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "for epoch in range(epochs): \n",
    "\n",
    "\n",
    "    for world_model_input in tqdm(data):\n",
    "        train_loss = train_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0487b339874bb139c9db03da99ed477e0b6feb8f9d8ba304ff5b5492c4ae8bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
