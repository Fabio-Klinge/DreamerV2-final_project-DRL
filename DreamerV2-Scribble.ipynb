{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "# Layer\n",
    "from tensorflow.keras.layers import Dense, Layer, Conv2DTranspose, Conv2D, GlobalAveragePooling2D, Reshape, BatchNormalization, GRUCell, MaxPooling2D\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "\n",
    "# Buffer \n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# Further support\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "\n",
    "# Environment\n",
    "import gym\n",
    "import highway_env\n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has to save (Observation, action, reward, terminal state)\n",
    "from numpy import float32\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=50,\n",
    "        buffer_length=1000, \n",
    "        observation_size=(128,32),\n",
    "        action_size=1\n",
    "    ):\n",
    "        '''\n",
    "        Create replay buffer\n",
    "\n",
    "        Buffer size = batch_size * buffer_length\n",
    "\n",
    "        '''\n",
    "        # Save batch size for other functions of buffer\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Tell buffer what data & which size to expect\n",
    "        self.data_spec = (\n",
    "            tf.TensorSpec(\n",
    "                shape= observation_size,\n",
    "                dtype=tf.dtypes.float64,\n",
    "                name=\"Observation\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=observation_size,\n",
    "                dtype=tf.dtypes.float64,\n",
    "                name=\"Next state\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[action_size],\n",
    "                dtype=tf.dtypes.int32,\n",
    "                name=\"Action\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                # Reward size\n",
    "                shape=[1, 1],\n",
    "                dtype=tf.dtypes.float64,\n",
    "                name=\"Reward\"\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[1, 1],\n",
    "                # Either 0 or 1 \n",
    "                dtype=tf.dtypes.bool,\n",
    "                name=\"Terminal State\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the buffer \n",
    "        self.buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "            self.data_spec, batch_size, buffer_length\n",
    "        )\n",
    "\n",
    "    def obtain_buffer_specs(self):\n",
    "        return self.data_spec\n",
    "\n",
    "    def add(self, items):\n",
    "        '''\n",
    "        length of items must be equal to batch size\n",
    "\n",
    "        items: list or tuple of batched data from (50, 4)\n",
    "\n",
    "\n",
    "        '''\n",
    "        # Combine all values from \"items\" in tensor\n",
    "        # Not sure wether we need tf.nest.map_structure\n",
    "        self.batched_values = tf.nest.map_structure(\n",
    "            lambda t: tf.stack([t] * self.batch_size),\n",
    "            items\n",
    "        )\n",
    "\n",
    "        # Add to batch\n",
    "        self.buffer.add_batch(self.batched_values)\n",
    "\n",
    "    def sample(self):\n",
    "        data = self.buffer.as_dataset(single_deterministic_pass=True).take(50)\n",
    "        data = \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentInteractor:\n",
    "\n",
    "  def __init__(self, config, buffer, environment_name = \"highway-fast-v0\"):\n",
    "    self.config = config\n",
    "\n",
    "    self.env = gym.make(environment_name)    \n",
    "    self.env.configure(config)\n",
    "\n",
    "    self.buffer = buffer\n",
    "    # Save sizes of the stupid tensors\n",
    "    self.data_spec = self.buffer.obtain_buffer_specs()\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  def create_trajectories(self, iterations):\n",
    "    state = self.env.reset()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        action = self.env.action_space.sample()\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "        self.buffer.add((\n",
    "            tf.constant(state, shape=self.data_spec[0].shape.as_list()),\n",
    "            tf.constant(next_state, shape=self.data_spec[1].shape.as_list()),\n",
    "            tf.constant(action, shape=self.data_spec[2].shape.as_list()),\n",
    "            tf.constant(reward, shape=self.data_spec[3].shape.as_list()),\n",
    "            tf.constant(done, shape=self.data_spec[4].shape.as_list())\n",
    "        ))\n",
    "        #print(state.shape)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "          state = self.env.reset()\n",
    "\n",
    "\n",
    "  def __del__(self):\n",
    "    self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\highway_env\\vehicle\\objects.py:33: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.position = np.array(position, dtype=np.float)\n",
      "c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\highway_env\\vehicle\\controller.py:273: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.int(np.clip(np.round(x * (self.target_speeds.size - 1)), 0, self.target_speeds.size - 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User1\\anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "Iterator trajectories:\n",
      "[(<tf.Tensor: shape=(128, 32), dtype=float64, numpy=\n",
      "array([[100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       ...,\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.]])>, <tf.Tensor: shape=(128, 32), dtype=float64, numpy=\n",
      "array([[100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       ...,\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>, <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.79799228]])>, <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[False]])>), (<tf.Tensor: shape=(128, 32), dtype=float64, numpy=\n",
      "array([[100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       ...,\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.]])>, <tf.Tensor: shape=(128, 32), dtype=float64, numpy=\n",
      "array([[100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       ...,\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1])>, <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.79799228]])>, <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[False]])>), (<tf.Tensor: shape=(128, 32), dtype=float64, numpy=\n",
      "array([[100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       ...,\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.]])>, <tf.Tensor: shape=(128, 32), dtype=float64, numpy=\n",
      "array([[100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       ...,\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.],\n",
      "       [100., 100., 100., ..., 100., 255., 100.]])>, <tf.Tensor: shape=(1,), dtype=int32, numpy=array([4])>, <tf.Tensor: shape=(1, 1), dtype=float64, numpy=array([[0.66897827]])>, <tf.Tensor: shape=(1, 1), dtype=bool, numpy=array([[False]])>)]\n",
      "[(TensorShape([128, 32]), TensorShape([128, 32]), TensorShape([1]), TensorShape([1, 1]), TensorShape([1, 1])), (TensorShape([128, 32]), TensorShape([128, 32]), TensorShape([1]), TensorShape([1, 1]), TensorShape([1, 1])), (TensorShape([128, 32]), TensorShape([128, 32]), TensorShape([1]), TensorShape([1, 1]), TensorShape([1, 1]))]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size\n",
    "image_shape = (128,32)\n",
    "\n",
    "# Long term memory of GRU\n",
    "hidden_unit_size = 200\n",
    "\n",
    "# Z in paper\n",
    "stochastic_state_shape = (32,32)\n",
    "stochastic_state_size = stochastic_state_shape[0] * stochastic_state_shape[1]\n",
    "\n",
    "#\n",
    "action_size = 1\n",
    "#\n",
    "\n",
    "#\n",
    "mlp_hidden_layer_size = 100\n",
    "\n",
    "# TODO different variable names for network inp/outp sizes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib_metadata import distribution\n",
    "\n",
    "\n",
    "class WorldModel:\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def create_encoder(self, input_size=image_shape, output_size=hidden_unit_size):\n",
    "        # Third dimension might be obsolete\n",
    "        encoder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Conv2D(16, (3, 3), activation=\"elu\", padding=\"same\")(encoder_input) # 16 layers of filtered 192x48 features\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(32, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 96x24\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 96x24\n",
    "        x = Conv2D(64, (3, 3), activation=\"elu\", padding=\"same\")(x) # 64 / 48x12\n",
    "        x = MaxPooling2D((2, 2), padding=\"same\")(x) # 64 / 48x12\n",
    "        x = GlobalAveragePooling2D()(x) # 64\n",
    "        encoder_output = Dense(output_size, activation = \"elu\")(x)\n",
    "\n",
    "        encoder = tf.keras.Model(encoder_input, encoder_output, name=\"Encoder\")\n",
    "\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    # Input size = 1024(z:32x32) + 200(size of hidden state)\n",
    "    # Output size = game frame\n",
    "    def create_decoder(\n",
    "        self, \n",
    "        input_size=stochastic_state_size + hidden_unit_size, \n",
    "        output_size=image_shape\n",
    "    ):\n",
    "        # Third dimension might be obsolete\n",
    "        decoder_input = tf.keras.Input(shape=input_size)\n",
    "        # TODO WIE SCHLIMM IST EIN MLP HIER?\n",
    "        x = Dense(256, activation= \"elu\")(decoder_input)\n",
    "        x = Reshape((32, 8, 1))(x) \n",
    "        x = Conv2DTranspose(16, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2DTranspose(1, (3, 3), strides=2, activation=\"elu\", padding=\"same\")(x)\n",
    "        # Might needs shape as Tensor\n",
    "        decoder_output = tfp.layers.IndependentNormal(event_shape=output_size)(x)\n",
    "\n",
    "\n",
    "        decoder = tf.keras.Model(decoder_input, decoder_output, name=\"Decoder\")\n",
    "\n",
    "        return decoder\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_reward_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        reward_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(reward_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        x = Dense(mlp_hidden_layer_size)(x)\n",
    "        # Creates indipendent normal distribution\n",
    "        # Hope is that it learns to output variables over reward space [0,1]\n",
    "        reward_predictor_output = tfp.layers.IndependentNormal()(x)\n",
    "\n",
    "        create_reward_predictor = tf.keras.Model(\n",
    "            reward_predictor_input,\n",
    "            reward_predictor_output,\n",
    "            name=\"create_reward_predictor\"\n",
    "        )\n",
    "\n",
    "        return create_reward_predictor\n",
    "    \n",
    "\n",
    "        # Input: concatination of h and z\n",
    "    # Output: float predicting the obtained reward\n",
    "    def create_discount_predictor(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+stochastic_state_size,\n",
    "        output_size=1\n",
    "    ):\n",
    "        discount_predictor_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(discount_predictor_input)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(x)\n",
    "        # Create 1 output sampled from bernoulli distribution\n",
    "        discount_predictor_output = tfp.layers.IndependentBernoulli()(x)\n",
    "\n",
    "        create_discount_predictor = tf.keras.Model(\n",
    "            discount_predictor_input,\n",
    "            discount_predictor_output,\n",
    "            name=\"create_discount_predictor\"\n",
    "        )\n",
    "\n",
    "        return create_discount_predictor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class RSSM:\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    # \n",
    "    def create_stochastic_state_action_embedder(\n",
    "        self, \n",
    "        input_size=stochastic_state_size + action_size,\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        state_action_input = tf.keras.Input(shape=input_size)\n",
    "        state_action_output = Dense(output_size, activation = \"elu\")(state_action_input)\n",
    "\n",
    "        stochastic_state_action_embedder = tf.keras.Model(\n",
    "            state_action_input,\n",
    "            state_action_output,\n",
    "            name=\"stochastic_state_action_embedder\"\n",
    "        )\n",
    "\n",
    "        return stochastic_state_action_embedder\n",
    "\n",
    "\n",
    "    # Contains GRU cell\n",
    "    def create_rnn(\n",
    "        self, \n",
    "        input_size=hidden_unit_size,\n",
    "        output_size=hidden_unit_size\n",
    "    ):\n",
    "        rnn_input = tf.keras.Input(shape=input_size)\n",
    "        rnn_output = GRUCell(output_size)(rnn_input)\n",
    "\n",
    "        rnn = tf.keras.Model(\n",
    "            rnn_input,\n",
    "            rnn_output,\n",
    "            name=\"rnn\"\n",
    "        )\n",
    "\n",
    "        return rnn\n",
    "\n",
    "\n",
    "    # Gets probabilities for each element of class in each category\n",
    "    # Turns these (32x32) probabilities into categoricals (either 0 or 1)\n",
    "    def sample_stochastic_state(self, logits):\n",
    "        # logits Output from MLP\n",
    "        #Onehot logits and create distribution from it (tfp.distrib)\n",
    "        # sample from distribution = sample (32x32)\n",
    "        # Apply softmax on sample to get probabillities = probs\n",
    "        # Do sample = + probs - stop_grad(probs) (gradients)\n",
    "        #return sample, gradients\n",
    "        logits = tf.reshape(logits, shape=(*logits.shape[:-1], *stochastic_state_shape))\n",
    "        logits_distribution = tfp.distributions.OneHotCategorical(logits)\n",
    "        sample = logits_distribution.sample() \n",
    "        sample += logits_distribution.prob(sample) - tf.stop_gradient(logits_distribution.prob(sample))\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "    # Z^ in paper\n",
    "    def create_prior_stochastic_state_embedder(\n",
    "        self, \n",
    "        input_size=hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(state_embedder_output)\n",
    "\n",
    "        create_prior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_prior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_prior_stochastic_state_embedder\n",
    "\n",
    "\n",
    "    # Z in paper\n",
    "    # Input size = concatenated output of RNN with output of CNN\n",
    "    def create_posterior_stochastic_state_embedder(\n",
    "        self, \n",
    "        input_size=hidden_unit_size+hidden_unit_size,\n",
    "        output_size=stochastic_state_size\n",
    "    ):\n",
    "        state_embedder_input = tf.keras.Input(shape=input_size)\n",
    "        x = Dense(mlp_hidden_layer_size, activation=\"elu\")(state_embedder_input)\n",
    "        # Activation function removed\n",
    "        state_embedder_output = Dense(output_size)(state_embedder_output)\n",
    "\n",
    "        create_posterior_stochastic_state_embedder = tf.keras.Model(\n",
    "            state_embedder_input,\n",
    "            state_embedder_output,\n",
    "            name=\"create_posterior_stochastic_state_embedder\"\n",
    "        )\n",
    "\n",
    "        return create_posterior_stochastic_state_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = Buffer(batch_size=1)\n",
    "config = {\n",
    "        \"observation\": {\n",
    "            \"type\": \"GrayscaleObservation\",\n",
    "            \"observation_shape\": (128, 32),\n",
    "            \"stack_size\": 1,\n",
    "            # weights for RGB conversion\n",
    "            \"weights\": [0.01, 0.01, 0.98],  \n",
    "            \"scaling\": 1.5,\n",
    "        },\n",
    "        # was at 2\n",
    "        \"policy_frequency\": 1 \n",
    "    }\n",
    "\n",
    "environment_interactor = EnvironmentInteractor(config, buffer)\n",
    "environment_interactor.create_trajectories(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = buffer.sample()\n",
    "\n",
    "iterator = iter(dataset)\n",
    "print(\"Iterator trajectories:\")\n",
    "trajectories = []\n",
    "for _ in range(3):\n",
    "  t, _ = next(iterator)\n",
    "  trajectories.append(t)\n",
    "\n",
    "print(trajectories)\n",
    "\n",
    "print(tf.nest.map_structure(lambda t: t.shape, trajectories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World model & agent training loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam inits\n",
    "Agent Data collection in environment + adding data to ERB (+ measure at which reward loop stops?) \n",
    "World model loop on data sampled from ERB\n",
    "Agent training loop with world model feedback\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7b373b46a3ad398ff0fbe06950ea2901f8c9a237be6643544c903d3b2478057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
